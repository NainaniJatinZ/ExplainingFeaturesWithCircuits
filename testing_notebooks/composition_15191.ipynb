{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6be84eb-0c56-44c4-84c8-00e92cd90ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 21 06:23:58 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              64W / 400W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              64W / 400W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d419f8-952d-49a6-a8b3-8203cf19e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62c80046-2b0f-446e-b95d-a80d72d86801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1950c0b-8566-4ba9-bea0-ae4d2f6557af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "# Imports for displaying vis in Colab / notebook\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# For the most part I'll try to import functions and classes near where they are used\n",
    "# to make it clear where they come from.\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f989131-016a-4972-a017-8811b3e6bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "def get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"8-gemmascope-res-16k\", feature_idx=0):\n",
    "    return html_template.format(sae_release, sae_id, feature_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de8f1e1a-cf33-46a9-8997-e920ad74d6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc2cc68dbeb489c875d7919b9d08da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "model = HookedSAETransformer.from_pretrained(\"google/gemma-2-2b\", device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "103fdb42-65a1-440e-919e-cf83745676f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>model</th>\n",
       "      <th>saes_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemma-scope-27b-pt-res</th>\n",
       "      <td>gemma-scope-27b-pt-res</td>\n",
       "      <td>google/gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_10/width_131k/average_l0_106': 'layer_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-att</th>\n",
       "      <td>gemma-scope-2b-pt-att</td>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_104': 'layer_0/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-att-canonical</th>\n",
       "      <td>gemma-scope-2b-pt-att-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'layer_0/width...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-mlp</th>\n",
       "      <td>gemma-scope-2b-pt-mlp</td>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_119': 'layer_0/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical</th>\n",
       "      <td>gemma-scope-2b-pt-mlp-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'layer_0/width...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-res</th>\n",
       "      <td>gemma-scope-2b-pt-res</td>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_105': 'layer_0/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-res-canonical</th>\n",
       "      <td>gemma-scope-2b-pt-res-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'layer_0/width...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-9b-pt-att</th>\n",
       "      <td>gemma-scope-9b-pt-att</td>\n",
       "      <td>google/gemma-scope-9b-pt-att</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_131k/average_l0_55': 'layer_0/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-9b-pt-mlp</th>\n",
       "      <td>gemma-scope-9b-pt-mlp</td>\n",
       "      <td>google/gemma-scope-9b-pt-mlp</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_131k/average_l0_11': 'layer_0/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         release  \\\n",
       "gemma-scope-27b-pt-res                    gemma-scope-27b-pt-res   \n",
       "gemma-scope-2b-pt-att                      gemma-scope-2b-pt-att   \n",
       "gemma-scope-2b-pt-att-canonical  gemma-scope-2b-pt-att-canonical   \n",
       "gemma-scope-2b-pt-mlp                      gemma-scope-2b-pt-mlp   \n",
       "gemma-scope-2b-pt-mlp-canonical  gemma-scope-2b-pt-mlp-canonical   \n",
       "gemma-scope-2b-pt-res                      gemma-scope-2b-pt-res   \n",
       "gemma-scope-2b-pt-res-canonical  gemma-scope-2b-pt-res-canonical   \n",
       "gemma-scope-9b-pt-att                      gemma-scope-9b-pt-att   \n",
       "gemma-scope-9b-pt-mlp                      gemma-scope-9b-pt-mlp   \n",
       "\n",
       "                                                       repo_id       model  \\\n",
       "gemma-scope-27b-pt-res           google/gemma-scope-27b-pt-res  gemma-2-2b   \n",
       "gemma-scope-2b-pt-att             google/gemma-scope-2b-pt-att  gemma-2-2b   \n",
       "gemma-scope-2b-pt-att-canonical   google/gemma-scope-2b-pt-att  gemma-2-2b   \n",
       "gemma-scope-2b-pt-mlp             google/gemma-scope-2b-pt-mlp  gemma-2-2b   \n",
       "gemma-scope-2b-pt-mlp-canonical   google/gemma-scope-2b-pt-mlp  gemma-2-2b   \n",
       "gemma-scope-2b-pt-res             google/gemma-scope-2b-pt-res  gemma-2-2b   \n",
       "gemma-scope-2b-pt-res-canonical   google/gemma-scope-2b-pt-res  gemma-2-2b   \n",
       "gemma-scope-9b-pt-att             google/gemma-scope-9b-pt-att  gemma-2-2b   \n",
       "gemma-scope-9b-pt-mlp             google/gemma-scope-9b-pt-mlp  gemma-2-2b   \n",
       "\n",
       "                                                                          saes_map  \n",
       "gemma-scope-27b-pt-res           {'layer_10/width_131k/average_l0_106': 'layer_...  \n",
       "gemma-scope-2b-pt-att            {'layer_0/width_16k/average_l0_104': 'layer_0/...  \n",
       "gemma-scope-2b-pt-att-canonical  {'layer_0/width_16k/canonical': 'layer_0/width...  \n",
       "gemma-scope-2b-pt-mlp            {'layer_0/width_16k/average_l0_119': 'layer_0/...  \n",
       "gemma-scope-2b-pt-mlp-canonical  {'layer_0/width_16k/canonical': 'layer_0/width...  \n",
       "gemma-scope-2b-pt-res            {'layer_0/width_16k/average_l0_105': 'layer_0/...  \n",
       "gemma-scope-2b-pt-res-canonical  {'layer_0/width_16k/canonical': 'layer_0/width...  \n",
       "gemma-scope-9b-pt-att            {'layer_0/width_131k/average_l0_55': 'layer_0/...  \n",
       "gemma-scope-9b-pt-mlp            {'layer_0/width_131k/average_l0_11': 'layer_0/...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "\n",
    "# TODO: Make this nicer.\n",
    "df = pd.DataFrame.from_records({k:v.__dict__ for k,v in get_pretrained_saes_directory().items()}).T\n",
    "df.drop(columns=[\"expected_var_explained\", \"expected_l0\", \"config_overrides\", \"conversion_func\"], inplace=True)\n",
    "df[df['model']=='gemma-2-2b'] # Each row is a \"release\" which has multiple SAEs which may have different configs / match different hook points in a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9250a94-492e-4a55-9d22-82d43d94ba62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: layer_0/width_16k/average_l0_105\n",
      "Layer 1: layer_1/width_16k/average_l0_102\n",
      "Layer 2: layer_2/width_16k/average_l0_141\n",
      "Layer 3: layer_3/width_16k/average_l0_59\n",
      "Layer 4: layer_4/width_16k/average_l0_124\n",
      "Layer 5: layer_5/width_16k/average_l0_68\n",
      "Layer 6: layer_6/width_16k/average_l0_70\n",
      "Layer 7: layer_7/width_16k/average_l0_69\n",
      "Layer 8: layer_8/width_16k/average_l0_71\n",
      "Layer 9: layer_9/width_16k/average_l0_73\n",
      "Layer 10: layer_10/width_16k/average_l0_77\n",
      "Layer 11: layer_11/width_16k/average_l0_80\n",
      "Layer 12: layer_12/width_16k/average_l0_82\n",
      "Layer 13: layer_13/width_16k/average_l0_84\n",
      "Layer 14: layer_14/width_16k/average_l0_84\n",
      "Layer 15: layer_15/width_16k/average_l0_78\n",
      "Layer 16: layer_16/width_16k/average_l0_78\n",
      "Layer 17: layer_17/width_16k/average_l0_77\n",
      "Layer 18: layer_18/width_16k/average_l0_74\n",
      "Layer 19: layer_19/width_16k/average_l0_73\n",
      "Layer 20: layer_20/width_16k/average_l0_71\n",
      "Layer 21: layer_21/width_16k/average_l0_70\n",
      "Layer 22: layer_22/width_16k/average_l0_72\n",
      "Layer 23: layer_23/width_16k/average_l0_75\n",
      "Layer 24: layer_24/width_16k/average_l0_73\n",
      "Layer 25: layer_25/width_16k/average_l0_116\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "sae_keys = list(df.loc['gemma-scope-2b-pt-res']['saes_map'].keys())\n",
    "# Dictionary to store the closest string for each layer\n",
    "closest_strings = {}\n",
    "\n",
    "# Regular expression to extract the layer number and l0 value\n",
    "pattern = re.compile(r'layer_(\\d+)/width_16k/average_l0_(\\d+)')\n",
    "\n",
    "# Organize strings by layer\n",
    "layer_dict = defaultdict(list)\n",
    "\n",
    "for s in sae_keys:\n",
    "    match = pattern.search(s)\n",
    "    if match:\n",
    "        layer = int(match.group(1))\n",
    "        l0_value = int(match.group(2))\n",
    "        layer_dict[layer].append((s, l0_value))\n",
    "\n",
    "# Find the string with l0 value closest to 100 for each layer\n",
    "for layer, items in layer_dict.items():\n",
    "    closest_string = min(items, key=lambda x: abs(x[1] - 100))\n",
    "    closest_strings[layer] = closest_string[0]\n",
    "\n",
    "# Output the closest string for each layer\n",
    "for layer in sorted(closest_strings):\n",
    "    print(f\"Layer {layer}: {closest_strings[layer]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b9925eb-dd52-4b97-8de1-b2dd5cab8a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer = 0\n",
    "\n",
    "prompt = 'What is the output of 53 plus 32 ? A: '\n",
    "\n",
    "# operator\n",
    "ind = 9\n",
    "\n",
    "operator_features = {}\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    operator_features[layer] = []\n",
    "    # Load the SAE for the current layer\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=\"gemma-scope-2b-pt-res\", \n",
    "        sae_id=closest_strings[layer],  \n",
    "        device=device\n",
    "    )\n",
    "    _, cache = model.run_with_cache_with_saes(prompt, saes=[sae])\n",
    "    x = cache[f'blocks.{layer}.hook_resid_post.hook_sae_acts_post']\n",
    "    topk_values, topk_indices = torch.topk(x[0, ind, :], 10)\n",
    "    gathered_values = x[0, :, topk_indices]\n",
    "    \n",
    "    softmaxed_x = F.softmax(gathered_values, dim=0)\n",
    "    top3_indices_after_softmax = torch.topk(softmaxed_x, 1, dim=0).indices\n",
    "    for i in range(topk_indices.size(0)):\n",
    "        if ind in top3_indices_after_softmax[:, i]:\n",
    "            operator_features[layer].append(topk_indices[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b9a38-2908-45fd-a8cb-31d4063dc033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "operator_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4348f4-f0c3-4f2a-a602-24b91b6e31b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c48a0-6f06-431d-b8d3-c6ee4a5906e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d63f35a-9fce-4dc2-9d2c-2b1861c85386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 50])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res\", # <- Release name \n",
    "    sae_id = \"layer_8/width_16k/average_l0_71\", # <- SAE id (not always a hook point!)\n",
    "    device = device\n",
    ")\n",
    "pr = 'What is the output of 53 plus 32 ? A: '\n",
    "\n",
    "# for ind in range(1, 17):\n",
    "ind = 13\n",
    "_, cache = model.run_with_cache_with_saes(pr, saes=[sae])\n",
    "x = cache['blocks.8.hook_resid_post.hook_sae_acts_post']\n",
    "topk_values, topk_indices = torch.topk(x[0, ind, :], 50)\n",
    "gathered_values = x[0, :, topk_indices]\n",
    "print(gathered_values.shape)\n",
    "# softmaxed_values = F.softmax(gathered_values, dim=0)\n",
    "# print(softmaxed_values)\n",
    "# vals, inds = torch.topk(cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, 13, :], 50)\n",
    "# inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b94a413c-b284-42c9-8ac3-e781ad275fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmaxed_x = F.softmax(gathered_values, dim=0)\n",
    "top3_indices_after_softmax = torch.topk(softmaxed_x, 2, dim=0).indices\n",
    "max_indices = torch.argmax(softmaxed_x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2db7a53b-a91d-4ecb-8517-910ebbd5b6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13, 13, 13,  0,  9, 13, 16, 13,  9,  3, 13, 13, 13,  0,  4,  3, 13,  0,\n",
       "         16,  0,  0, 13,  4,  0, 13, 14,  0, 13,  0,  0, 13,  4,  0, 13,  4,  0,\n",
       "          5,  0,  4, 13, 13, 16,  5,  0, 13, 13, 14, 13,  0, 16],\n",
       "        [ 0,  6,  0,  1, 10,  0,  0,  0, 10,  4,  4,  2,  9,  4,  3, 13, 14, 13,\n",
       "         13, 13,  9,  7, 13,  2,  6, 15, 13,  0, 13, 13,  0,  6,  4,  0, 13, 13,\n",
       "         13, 14,  5,  0,  0,  8, 13, 13, 12,  0,  9, 10, 13, 13]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top3_indices_after_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "823b6285-b970-45ca-a232-8f65b99cd810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4909, device='cuda:0')\n",
      "tensor(7759, device='cuda:0')\n",
      "tensor(2003, device='cuda:0')\n",
      "tensor(4781, device='cuda:0')\n",
      "tensor(4646, device='cuda:0')\n",
      "tensor(8109, device='cuda:0')\n",
      "tensor(2165, device='cuda:0')\n",
      "tensor(10524, device='cuda:0')\n",
      "tensor(14888, device='cuda:0')\n",
      "tensor(15075, device='cuda:0')\n",
      "tensor(778, device='cuda:0')\n",
      "tensor(15191, device='cuda:0')\n",
      "tensor(2707, device='cuda:0')\n",
      "tensor(10585, device='cuda:0')\n",
      "tensor(2121, device='cuda:0')\n",
      "tensor(9261, device='cuda:0')\n",
      "tensor(4978, device='cuda:0')\n",
      "tensor(8262, device='cuda:0')\n",
      "tensor(1083, device='cuda:0')\n",
      "tensor(571, device='cuda:0')\n",
      "tensor(9188, device='cuda:0')\n",
      "tensor(9561, device='cuda:0')\n",
      "tensor(7876, device='cuda:0')\n",
      "tensor(14993, device='cuda:0')\n",
      "tensor(2288, device='cuda:0')\n",
      "tensor(11746, device='cuda:0')\n",
      "tensor(5864, device='cuda:0')\n",
      "tensor(11973, device='cuda:0')\n",
      "tensor(498, device='cuda:0')\n",
      "tensor(4788, device='cuda:0')\n",
      "tensor(5821, device='cuda:0')\n",
      "tensor(10971, device='cuda:0')\n",
      "tensor(10825, device='cuda:0')\n",
      "tensor(5326, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(topk_indices.size(0)):\n",
    "    if 13 in top3_indices_after_softmax[:, i]:\n",
    "        print(topk_indices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23d8c633-8a17-473b-b913-0c27b1de361c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13, 13, 13,  0,  9, 13, 16, 13,  9,  3, 13, 13, 13,  0,  4,  3, 13,  0,\n",
       "        16,  0], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3533c9b3-2b2d-4299-b820-ba7a9a2553fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 6.1795, 0.0000, 0.0000, 7.0834],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, :, 15191]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4aa1cccd-e113-4a5c-bf0c-d0d6ffa9f1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <bos>\n",
      "1 What\n",
      "2  is\n",
      "3  the\n",
      "4  output\n",
      "5  of\n",
      "6  \n",
      "7 5\n",
      "8 3\n",
      "9  plus\n",
      "10  \n",
      "11 3\n",
      "12 2\n",
      "13  ?\n",
      "14  A\n",
      "15 :\n",
      "16  \n"
     ]
    }
   ],
   "source": [
    "for i, tk in enumerate(model.to_str_tokens(pr)):\n",
    "    print(i, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d76425-a6c3-42a4-b5f6-94a01f2b2333",
   "metadata": {},
   "outputs": [],
   "source": [
    "operator = 9\n",
    "operand = [7, 8, 11, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "316b452b-19c9-44fa-a61f-3f469d5696b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1:\n",
      "  Clean:     What is the output of 96 plus 43 ? \n",
      "  Corrupted: What is the output of 96 and 43 ? \n",
      "\n",
      "Pair 2:\n",
      "  Clean:     What is the output of 56 plus 77 ? \n",
      "  Corrupted: What is the output of 56 and 77 ? \n",
      "\n",
      "Pair 3:\n",
      "  Clean:     What is the output of 93 plus 56 ? \n",
      "  Corrupted: What is the output of 93 and 56 ? \n",
      "\n",
      "Pair 4:\n",
      "  Clean:     What is the output of 29 plus 37 ? \n",
      "  Corrupted: What is the output of 29 and 37 ? \n",
      "\n",
      "Pair 5:\n",
      "  Clean:     What is the output of 29 plus 75 ? \n",
      "  Corrupted: What is the output of 29 and 75 ? \n",
      "\n",
      "Pair 6:\n",
      "  Clean:     What is the output of 48 plus 18 ? \n",
      "  Corrupted: What is the output of 48 and 18 ? \n",
      "\n",
      "Pair 7:\n",
      "  Clean:     What is the output of 91 plus 12 ? \n",
      "  Corrupted: What is the output of 91 and 12 ? \n",
      "\n",
      "Pair 8:\n",
      "  Clean:     What is the output of 33 plus 76 ? \n",
      "  Corrupted: What is the output of 33 and 76 ? \n",
      "\n",
      "Pair 9:\n",
      "  Clean:     What is the output of 53 plus 83 ? \n",
      "  Corrupted: What is the output of 53 and 83 ? \n",
      "\n",
      "Pair 10:\n",
      "  Clean:     What is the output of 39 plus 16 ? \n",
      "  Corrupted: What is the output of 39 and 16 ? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_example_pair():\n",
    "    # Generate two random numbers between 1 and 3 digits\n",
    "    num1 = random.randint(10, 99)\n",
    "    num2 = random.randint(10, 99)\n",
    "    \n",
    "    # Create clean and corrupted examples\n",
    "    clean_example = f'What is the output of {num1} plus {num2} ? '\n",
    "    corrupted_example = f'What is the output of {num1} and {num2} ? '\n",
    "    \n",
    "    return clean_example, corrupted_example\n",
    "\n",
    "def generate_dataset(N):\n",
    "    dataset = []\n",
    "    for _ in range(N):\n",
    "        clean, corrupted = generate_example_pair()\n",
    "        dataset.append((clean, corrupted))\n",
    "    return dataset\n",
    "\n",
    "# Example usage\n",
    "N = 10  # Number of pairs to generate\n",
    "dataset = generate_dataset(N)\n",
    "\n",
    "# Print the dataset\n",
    "for i, (clean, corrupted) in enumerate(dataset):\n",
    "    print(f\"Pair {i+1}:\")\n",
    "    print(f\"  Clean:     {clean}\")\n",
    "    print(f\"  Corrupted: {corrupted}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32ab56c7-c247-49d6-a91e-6b4feb179081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'layer_1/width_16k/average_l0_102'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_strings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cf7f5b2-c456-4c56-ac09-6522178a451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD till 8 \n",
    "saes = []\n",
    "for layer in range(9):\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=\"gemma-scope-2b-pt-res\", \n",
    "        sae_id=closest_strings[layer],  \n",
    "        device=device\n",
    "    )\n",
    "    sae.use_error_term = True\n",
    "    saes.append(sae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "34c71c69-039e-4a5d-8f73-996299cbd988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architecture': 'jumprelu',\n",
       " 'd_in': 2304,\n",
       " 'd_sae': 16384,\n",
       " 'dtype': 'float32',\n",
       " 'model_name': 'gemma-2-2b',\n",
       " 'hook_name': 'blocks.8.hook_resid_post',\n",
       " 'hook_layer': 8,\n",
       " 'hook_head_index': None,\n",
       " 'activation_fn_str': 'relu',\n",
       " 'finetuning_scaling_factor': False,\n",
       " 'sae_lens_training_version': None,\n",
       " 'prepend_bos': True,\n",
       " 'dataset_path': 'monology/pile-uncopyrighted',\n",
       " 'context_size': 1024,\n",
       " 'dataset_trust_remote_code': True,\n",
       " 'apply_b_dec_to_input': False,\n",
       " 'normalize_activations': None,\n",
       " 'device': 'cuda'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94b39448-5a9c-4b97-934c-9353ab1c09d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('blocks.8.hook_resid_post.hook_sae_input', torch.Size([1, 17, 2304])), ('blocks.8.hook_resid_post.hook_sae_acts_pre', torch.Size([1, 17, 16384])), ('blocks.8.hook_resid_post.hook_sae_acts_post', torch.Size([1, 17, 16384])), ('blocks.8.hook_resid_post.hook_sae_recons', torch.Size([1, 17, 2304])), ('blocks.8.hook_resid_post.hook_sae_error', torch.Size([1, 17, 2304])), ('blocks.8.hook_resid_post.hook_sae_output', torch.Size([1, 17, 2304]))]\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 6.1795, 0.0000, 0.0000, 7.0834],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "_, cache = model.run_with_cache_with_saes(prompt, saes=[sae])\n",
    "\n",
    "print([(k, v.shape) for k,v in cache.items() if \"sae\" in k])\n",
    "\n",
    "print(cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, :, 15191])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d45df24d-c2d6-40e0-ab60-8a89934f90e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1:\n",
      "  Clean:     What is the output of 96 plus 43 ? \n",
      "tensor(7.0159, device='cuda:0')\n",
      "  Corrupted: What is the output of 96 and 43 ? \n",
      "tensor(0., device='cuda:0')\n",
      "Pair 2:\n",
      "  Clean:     What is the output of 56 plus 77 ? \n",
      "tensor(5.5384, device='cuda:0')\n",
      "  Corrupted: What is the output of 56 and 77 ? \n",
      "tensor(0., device='cuda:0')\n",
      "Pair 3:\n",
      "  Clean:     What is the output of 93 plus 56 ? \n",
      "tensor(6.7896, device='cuda:0')\n",
      "  Corrupted: What is the output of 93 and 56 ? \n",
      "tensor(0., device='cuda:0')\n",
      "Pair 4:\n",
      "  Clean:     What is the output of 29 plus 37 ? \n",
      "tensor(5.6018, device='cuda:0')\n",
      "  Corrupted: What is the output of 29 and 37 ? \n",
      "tensor(0., device='cuda:0')\n",
      "Pair 5:\n",
      "  Clean:     What is the output of 29 plus 75 ? \n",
      "tensor(6.8103, device='cuda:0')\n",
      "  Corrupted: What is the output of 29 and 75 ? \n",
      "tensor(0., device='cuda:0')\n",
      "Pair 6:\n",
      "  Clean:     What is the output of 48 plus 18 ? \n",
      "tensor(8.9317, device='cuda:0')\n",
      "  Corrupted: What is the output of 48 and 18 ? \n",
      "tensor(2.8513, device='cuda:0')\n",
      "Pair 7:\n",
      "  Clean:     What is the output of 91 plus 12 ? \n",
      "tensor(6.7175, device='cuda:0')\n",
      "  Corrupted: What is the output of 91 and 12 ? \n",
      "tensor(0., device='cuda:0')\n",
      "Pair 8:\n",
      "  Clean:     What is the output of 33 plus 76 ? \n",
      "tensor(6.0717, device='cuda:0')\n",
      "  Corrupted: What is the output of 33 and 76 ? \n",
      "tensor(0., device='cuda:0')\n",
      "Pair 9:\n",
      "  Clean:     What is the output of 53 plus 83 ? \n",
      "tensor(6.7595, device='cuda:0')\n",
      "  Corrupted: What is the output of 53 and 83 ? \n",
      "tensor(0., device='cuda:0')\n",
      "Pair 10:\n",
      "  Clean:     What is the output of 39 plus 16 ? \n",
      "tensor(7.5431, device='cuda:0')\n",
      "  Corrupted: What is the output of 39 and 16 ? \n",
      "tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i, (clean, corrupted) in enumerate(dataset):\n",
    "    _, clean_cache = model.run_with_cache_with_saes(clean, saes=saes)\n",
    "    print(f\"Pair {i+1}:\")\n",
    "    print(f\"  Clean:     {clean}\")\n",
    "    print(clean_cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, :, 15191][13])\n",
    "    _, corr_cache = model.run_with_cache_with_saes(corrupted, saes=saes)\n",
    "    print(f\"  Corrupted: {corrupted}\")\n",
    "    print(corr_cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, :, 15191][13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "896171ef-1a90-4cda-b2c5-479d72477e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 9\n",
    "operator_features = {}\n",
    "_, clean_cache = model.run_with_cache_with_saes(clean, saes=saes)\n",
    "for layer in range(9):\n",
    "    operator_features[layer] = []\n",
    "    x = clean_cache[f'blocks.{layer}.hook_resid_post.hook_sae_acts_post']\n",
    "    topk_values, topk_indices = torch.topk(x[0, ind, :], 50)\n",
    "    gathered_values = x[0, :, topk_indices]\n",
    "    softmaxed_x = F.softmax(gathered_values, dim=0)\n",
    "    top3_indices_after_softmax = torch.topk(softmaxed_x, 3, dim=0).indices\n",
    "    for i in range(topk_indices.size(0)):\n",
    "        if ind in top3_indices_after_softmax[:, i]:\n",
    "            operator_features[layer].append(topk_indices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8f5840df-2474-4077-9f78-1bf7adcae3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_features_under_comp = 0\n",
    "for key, val in operator_features.items():\n",
    "    total_features_under_comp+= len(val)\n",
    "total_features_under_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "629cdcb5-7bba-44d8-a2ab-8230b3aa12ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [tensor(14495, device='cuda:0'), tensor(9416, device='cuda:0'), tensor(60, device='cuda:0'), tensor(5039, device='cuda:0'), tensor(12108, device='cuda:0'), tensor(6179, device='cuda:0'), tensor(13928, device='cuda:0'), tensor(15396, device='cuda:0'), tensor(9361, device='cuda:0'), tensor(283, device='cuda:0'), tensor(8441, device='cuda:0'), tensor(5043, device='cuda:0'), tensor(3977, device='cuda:0'), tensor(10297, device='cuda:0'), tensor(1717, device='cuda:0'), tensor(15325, device='cuda:0'), tensor(2337, device='cuda:0'), tensor(4470, device='cuda:0'), tensor(7782, device='cuda:0'), tensor(9091, device='cuda:0'), tensor(9069, device='cuda:0'), tensor(750, device='cuda:0'), tensor(9908, device='cuda:0'), tensor(4275, device='cuda:0'), tensor(15871, device='cuda:0'), tensor(15842, device='cuda:0'), tensor(6772, device='cuda:0'), tensor(15706, device='cuda:0'), tensor(10646, device='cuda:0'), tensor(5874, device='cuda:0'), tensor(13170, device='cuda:0'), tensor(13651, device='cuda:0'), tensor(11093, device='cuda:0'), tensor(10368, device='cuda:0'), tensor(4453, device='cuda:0'), tensor(8735, device='cuda:0'), tensor(6300, device='cuda:0'), tensor(2759, device='cuda:0'), tensor(10696, device='cuda:0'), tensor(16147, device='cuda:0'), tensor(15729, device='cuda:0'), tensor(1950, device='cuda:0'), tensor(15850, device='cuda:0'), tensor(13353, device='cuda:0'), tensor(8647, device='cuda:0'), tensor(16315, device='cuda:0'), tensor(3604, device='cuda:0'), tensor(2241, device='cuda:0'), tensor(9958, device='cuda:0')]\n",
      "1 [tensor(15546, device='cuda:0'), tensor(5344, device='cuda:0'), tensor(6923, device='cuda:0'), tensor(5804, device='cuda:0'), tensor(15664, device='cuda:0'), tensor(5732, device='cuda:0'), tensor(8279, device='cuda:0'), tensor(3927, device='cuda:0'), tensor(11196, device='cuda:0'), tensor(13412, device='cuda:0'), tensor(6808, device='cuda:0'), tensor(12559, device='cuda:0'), tensor(392, device='cuda:0'), tensor(2904, device='cuda:0'), tensor(616, device='cuda:0'), tensor(246, device='cuda:0'), tensor(8959, device='cuda:0'), tensor(7419, device='cuda:0'), tensor(740, device='cuda:0'), tensor(15435, device='cuda:0'), tensor(12352, device='cuda:0'), tensor(11997, device='cuda:0'), tensor(1631, device='cuda:0'), tensor(3262, device='cuda:0'), tensor(12870, device='cuda:0'), tensor(12466, device='cuda:0'), tensor(7492, device='cuda:0'), tensor(674, device='cuda:0'), tensor(6394, device='cuda:0'), tensor(12360, device='cuda:0'), tensor(14738, device='cuda:0'), tensor(5868, device='cuda:0'), tensor(9559, device='cuda:0'), tensor(3786, device='cuda:0'), tensor(9115, device='cuda:0'), tensor(12585, device='cuda:0'), tensor(16322, device='cuda:0'), tensor(16356, device='cuda:0'), tensor(12703, device='cuda:0'), tensor(2991, device='cuda:0'), tensor(7103, device='cuda:0'), tensor(2766, device='cuda:0'), tensor(16258, device='cuda:0'), tensor(16136, device='cuda:0')]\n",
      "2 [tensor(15626, device='cuda:0'), tensor(10297, device='cuda:0'), tensor(1973, device='cuda:0'), tensor(9181, device='cuda:0'), tensor(2441, device='cuda:0'), tensor(15040, device='cuda:0'), tensor(5542, device='cuda:0'), tensor(2437, device='cuda:0'), tensor(1786, device='cuda:0'), tensor(572, device='cuda:0'), tensor(12669, device='cuda:0'), tensor(484, device='cuda:0'), tensor(2348, device='cuda:0'), tensor(16242, device='cuda:0'), tensor(14894, device='cuda:0'), tensor(6744, device='cuda:0'), tensor(12229, device='cuda:0'), tensor(11667, device='cuda:0'), tensor(910, device='cuda:0'), tensor(4939, device='cuda:0'), tensor(14636, device='cuda:0'), tensor(2124, device='cuda:0'), tensor(10380, device='cuda:0'), tensor(13920, device='cuda:0'), tensor(13291, device='cuda:0'), tensor(7548, device='cuda:0'), tensor(443, device='cuda:0'), tensor(13996, device='cuda:0'), tensor(11753, device='cuda:0'), tensor(5139, device='cuda:0'), tensor(3863, device='cuda:0'), tensor(12834, device='cuda:0'), tensor(7132, device='cuda:0'), tensor(9013, device='cuda:0'), tensor(11359, device='cuda:0'), tensor(15059, device='cuda:0'), tensor(12075, device='cuda:0'), tensor(4025, device='cuda:0'), tensor(5907, device='cuda:0'), tensor(23, device='cuda:0'), tensor(5209, device='cuda:0'), tensor(3441, device='cuda:0'), tensor(14644, device='cuda:0'), tensor(3668, device='cuda:0'), tensor(11795, device='cuda:0'), tensor(330, device='cuda:0'), tensor(6351, device='cuda:0'), tensor(10424, device='cuda:0')]\n",
      "3 [tensor(5140, device='cuda:0'), tensor(2441, device='cuda:0'), tensor(11970, device='cuda:0'), tensor(10532, device='cuda:0'), tensor(11621, device='cuda:0'), tensor(7915, device='cuda:0'), tensor(2285, device='cuda:0'), tensor(13764, device='cuda:0'), tensor(16172, device='cuda:0'), tensor(10638, device='cuda:0'), tensor(14683, device='cuda:0'), tensor(10933, device='cuda:0'), tensor(4461, device='cuda:0'), tensor(9115, device='cuda:0'), tensor(7567, device='cuda:0'), tensor(10721, device='cuda:0'), tensor(7265, device='cuda:0'), tensor(15582, device='cuda:0'), tensor(4167, device='cuda:0'), tensor(11389, device='cuda:0'), tensor(1574, device='cuda:0'), tensor(2476, device='cuda:0'), tensor(2612, device='cuda:0'), tensor(13225, device='cuda:0'), tensor(8884, device='cuda:0'), tensor(5779, device='cuda:0'), tensor(6698, device='cuda:0'), tensor(16000, device='cuda:0'), tensor(2227, device='cuda:0'), tensor(6020, device='cuda:0'), tensor(8794, device='cuda:0'), tensor(2810, device='cuda:0'), tensor(15746, device='cuda:0'), tensor(7244, device='cuda:0'), tensor(3695, device='cuda:0'), tensor(1845, device='cuda:0'), tensor(2791, device='cuda:0'), tensor(677, device='cuda:0'), tensor(4104, device='cuda:0'), tensor(16160, device='cuda:0'), tensor(10995, device='cuda:0'), tensor(3151, device='cuda:0'), tensor(8037, device='cuda:0')]\n",
      "4 [tensor(4790, device='cuda:0'), tensor(9849, device='cuda:0'), tensor(5492, device='cuda:0'), tensor(7897, device='cuda:0'), tensor(16146, device='cuda:0'), tensor(15594, device='cuda:0'), tensor(10701, device='cuda:0'), tensor(5301, device='cuda:0'), tensor(8189, device='cuda:0'), tensor(8499, device='cuda:0'), tensor(2946, device='cuda:0'), tensor(12162, device='cuda:0'), tensor(10287, device='cuda:0'), tensor(1199, device='cuda:0'), tensor(11245, device='cuda:0'), tensor(11687, device='cuda:0'), tensor(4753, device='cuda:0'), tensor(5464, device='cuda:0'), tensor(5274, device='cuda:0'), tensor(12841, device='cuda:0'), tensor(4620, device='cuda:0'), tensor(1225, device='cuda:0'), tensor(7105, device='cuda:0'), tensor(15626, device='cuda:0'), tensor(5522, device='cuda:0'), tensor(6432, device='cuda:0'), tensor(12393, device='cuda:0'), tensor(14480, device='cuda:0'), tensor(362, device='cuda:0'), tensor(622, device='cuda:0'), tensor(10729, device='cuda:0'), tensor(8302, device='cuda:0'), tensor(7229, device='cuda:0'), tensor(8241, device='cuda:0'), tensor(5212, device='cuda:0'), tensor(11884, device='cuda:0'), tensor(1602, device='cuda:0'), tensor(10848, device='cuda:0'), tensor(11137, device='cuda:0'), tensor(6685, device='cuda:0'), tensor(8480, device='cuda:0'), tensor(7672, device='cuda:0')]\n",
      "5 [tensor(14998, device='cuda:0'), tensor(424, device='cuda:0'), tensor(8779, device='cuda:0'), tensor(7791, device='cuda:0'), tensor(12759, device='cuda:0'), tensor(697, device='cuda:0'), tensor(5691, device='cuda:0'), tensor(4901, device='cuda:0'), tensor(13898, device='cuda:0'), tensor(15987, device='cuda:0'), tensor(3926, device='cuda:0'), tensor(2099, device='cuda:0'), tensor(3815, device='cuda:0'), tensor(7020, device='cuda:0'), tensor(9586, device='cuda:0'), tensor(4349, device='cuda:0'), tensor(14600, device='cuda:0'), tensor(7929, device='cuda:0'), tensor(12669, device='cuda:0'), tensor(16161, device='cuda:0'), tensor(11607, device='cuda:0'), tensor(1472, device='cuda:0'), tensor(13250, device='cuda:0'), tensor(15451, device='cuda:0'), tensor(9472, device='cuda:0'), tensor(10674, device='cuda:0'), tensor(8212, device='cuda:0'), tensor(201, device='cuda:0'), tensor(6595, device='cuda:0'), tensor(7595, device='cuda:0'), tensor(13752, device='cuda:0'), tensor(12484, device='cuda:0'), tensor(15185, device='cuda:0'), tensor(12370, device='cuda:0'), tensor(9386, device='cuda:0'), tensor(1220, device='cuda:0'), tensor(8333, device='cuda:0'), tensor(7437, device='cuda:0'), tensor(4989, device='cuda:0'), tensor(2055, device='cuda:0'), tensor(4778, device='cuda:0'), tensor(12842, device='cuda:0'), tensor(16186, device='cuda:0'), tensor(3080, device='cuda:0')]\n",
      "6 [tensor(15626, device='cuda:0'), tensor(5936, device='cuda:0'), tensor(9847, device='cuda:0'), tensor(15313, device='cuda:0'), tensor(8952, device='cuda:0'), tensor(7425, device='cuda:0'), tensor(3919, device='cuda:0'), tensor(11785, device='cuda:0'), tensor(100, device='cuda:0'), tensor(9603, device='cuda:0'), tensor(4064, device='cuda:0'), tensor(12005, device='cuda:0'), tensor(8691, device='cuda:0'), tensor(1072, device='cuda:0'), tensor(2350, device='cuda:0'), tensor(14559, device='cuda:0'), tensor(11227, device='cuda:0'), tensor(7519, device='cuda:0'), tensor(9642, device='cuda:0'), tensor(11440, device='cuda:0'), tensor(7221, device='cuda:0'), tensor(1664, device='cuda:0'), tensor(11877, device='cuda:0'), tensor(2351, device='cuda:0'), tensor(16002, device='cuda:0'), tensor(3422, device='cuda:0'), tensor(14526, device='cuda:0'), tensor(16137, device='cuda:0'), tensor(283, device='cuda:0'), tensor(14083, device='cuda:0'), tensor(8804, device='cuda:0'), tensor(9748, device='cuda:0'), tensor(10928, device='cuda:0'), tensor(7899, device='cuda:0'), tensor(7501, device='cuda:0'), tensor(7620, device='cuda:0'), tensor(1093, device='cuda:0')]\n",
      "7 [tensor(6192, device='cuda:0'), tensor(13838, device='cuda:0'), tensor(13766, device='cuda:0'), tensor(3704, device='cuda:0'), tensor(4334, device='cuda:0'), tensor(9028, device='cuda:0'), tensor(237, device='cuda:0'), tensor(8189, device='cuda:0'), tensor(2074, device='cuda:0'), tensor(9847, device='cuda:0'), tensor(6170, device='cuda:0'), tensor(12439, device='cuda:0'), tensor(11956, device='cuda:0'), tensor(7120, device='cuda:0'), tensor(11976, device='cuda:0'), tensor(4378, device='cuda:0'), tensor(12832, device='cuda:0'), tensor(4733, device='cuda:0'), tensor(4017, device='cuda:0'), tensor(7439, device='cuda:0'), tensor(9695, device='cuda:0'), tensor(6, device='cuda:0'), tensor(16032, device='cuda:0'), tensor(15048, device='cuda:0'), tensor(12649, device='cuda:0'), tensor(7156, device='cuda:0'), tensor(12233, device='cuda:0'), tensor(5723, device='cuda:0'), tensor(6226, device='cuda:0'), tensor(217, device='cuda:0'), tensor(2476, device='cuda:0'), tensor(11272, device='cuda:0'), tensor(3373, device='cuda:0'), tensor(14776, device='cuda:0'), tensor(7666, device='cuda:0'), tensor(6875, device='cuda:0'), tensor(10077, device='cuda:0')]\n",
      "8 [tensor(6179, device='cuda:0'), tensor(15063, device='cuda:0'), tensor(9908, device='cuda:0'), tensor(2024, device='cuda:0'), tensor(9793, device='cuda:0'), tensor(11093, device='cuda:0'), tensor(5697, device='cuda:0'), tensor(4797, device='cuda:0'), tensor(10350, device='cuda:0'), tensor(13504, device='cuda:0'), tensor(10487, device='cuda:0'), tensor(7653, device='cuda:0'), tensor(15682, device='cuda:0'), tensor(6496, device='cuda:0'), tensor(10524, device='cuda:0'), tensor(279, device='cuda:0'), tensor(8416, device='cuda:0'), tensor(11537, device='cuda:0'), tensor(11109, device='cuda:0'), tensor(95, device='cuda:0'), tensor(12376, device='cuda:0'), tensor(2986, device='cuda:0'), tensor(5685, device='cuda:0'), tensor(332, device='cuda:0'), tensor(11103, device='cuda:0'), tensor(14187, device='cuda:0'), tensor(5624, device='cuda:0'), tensor(324, device='cuda:0'), tensor(9874, device='cuda:0'), tensor(815, device='cuda:0'), tensor(11031, device='cuda:0'), tensor(10585, device='cuda:0'), tensor(4676, device='cuda:0'), tensor(12223, device='cuda:0'), tensor(4574, device='cuda:0'), tensor(15068, device='cuda:0'), tensor(9932, device='cuda:0'), tensor(1660, device='cuda:0'), tensor(896, device='cuda:0'), tensor(3821, device='cuda:0'), tensor(11602, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "for key, val in operator_features.items():\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "814338c0-62c5-47be-9294-53d9085db59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric definition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1501640e-2afe-41b4-b133-fe6e9411fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_activations(corr_activations, clean_cache, layer_name, feature_index):\n",
    "    # Replace the corrupted activations at the specified feature index with the clean one\n",
    "    patched_activations = corr_activations.clone()  # clone to avoid modifying the original tensor\n",
    "    clean_activations = clean_cache[layer_name]\n",
    "    patched_activations[..., feature_index] = clean_activations[..., feature_index]\n",
    "    return patched_activations\n",
    "\n",
    "def forward_hook(module, input, output, clean_cache, layer_name, feature_index):\n",
    "    return patch_activations(output, clean_cache, layer_name, feature_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "07dba640-cd1c-4023-92e5-3745d831d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_clean, clean_cache = model.run_with_cache_with_saes(clean, saes=saes)\n",
    "op_corr, corr_cache = model.run_with_cache_with_saes(corrupted, saes=saes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "430be75e-cda2-4ab7-a1ad-f45b89aa645e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.5431, device='cuda:0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_difference(clean_cache, corr_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a2ec3afe-518c-4e93-9791-306289389d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-24.3121,  -8.7513,  -6.9737,  ..., -18.3960, -17.4268, -24.3171],\n",
      "         [-17.6520,  -1.1834, -13.3353,  ..., -11.1017,  -4.4314, -17.6057],\n",
      "         [-15.1261,  -1.6613,  -1.7596,  ...,  -9.2118, -12.0350, -14.9411],\n",
      "         ...,\n",
      "         [-13.9915,  18.2728,   2.8354,  ...,   0.2403,   3.8752, -13.9369],\n",
      "         [-10.2113,  19.6030,   4.2905,  ...,  -1.8026,   5.7783, -10.1475],\n",
      "         [ -6.0919,  13.5185,   9.8800,  ...,   4.7390,  11.3725,  -6.1564]]],\n",
      "       device='cuda:0')\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 7.5431, 2.4472], device='cuda:0')\n",
      "tensor([[[-24.3121,  -8.7513,  -6.9737,  ..., -18.3960, -17.4268, -24.3171],\n",
      "         [-17.6520,  -1.1834, -13.3353,  ..., -11.1017,  -4.4314, -17.6057],\n",
      "         [-15.1261,  -1.6613,  -1.7596,  ...,  -9.2118, -12.0350, -14.9411],\n",
      "         ...,\n",
      "         [-11.1365,  16.4926,   6.8349,  ...,   2.2972,   5.6023, -11.1145],\n",
      "         [ -7.3857,  20.0960,   6.3642,  ...,   0.5787,   7.0400,  -7.3775],\n",
      "         [ -4.4510,  14.4597,   8.7713,  ...,   7.8364,  13.7519,  -4.5347]]],\n",
      "       device='cuda:0')\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(op_clean)\n",
    "print(feature_difference(clean_cache))\n",
    "\n",
    "print(op_corr)\n",
    "print(feature_difference(corr_cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69f01740-6c94-4951-9b2f-43f32b47e5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "4\n",
      "5\n",
      "<strong>\n",
      "\n",
      "\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "probs = op_corr[0].softmax(dim=-1)\n",
    "token_probs = probs[-1]\n",
    "sorted_token_probs, sorted_token_values = token_probs.sort(descending=True)\n",
    "        # Janky way to get the index of the token in the sorted list - I couldn't find a better way?\n",
    "for i in range(10):\n",
    "    # sorted_token_values\n",
    "    print(model.to_string(sorted_token_values[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4cdf710c-1403-407f-a73e-22533dee1513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD till 8 \n",
    "saes = []\n",
    "for layer in range(9):\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=\"gemma-scope-2b-pt-res\", \n",
    "        sae_id=closest_strings[layer],  \n",
    "        device=device\n",
    "    )\n",
    "    sae.use_error_term = True\n",
    "    saes.append(sae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "546920c5-00a9-4b8e-81b7-3bf17055d278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "og logit torch.Size([1, 15, 256000])\n",
      "Original feature diff:  tensor(7.5431, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 126\u001b[0m\n\u001b[1;32m    123\u001b[0m hook_handle \u001b[38;5;241m=\u001b[39m hook_point\u001b[38;5;241m.\u001b[39mregister_forward_hook(hook)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# print(hook_handle)\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Run the corrupted prompt with the hook applied\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m inter_op, corr_cache_intervened \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache_with_saes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrupted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msaes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# print(\"intervened logit\", inter_op[0, -1, :10])\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Remove the hook after the forward pass\u001b[39;00m\n\u001b[1;32m    129\u001b[0m hook_handle\u001b[38;5;241m.\u001b[39mremove()\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/sae_lens/analysis/hooked_sae_transformer.py:228\u001b[0m, in \u001b[0;36mHookedSAETransformer.run_with_cache_with_saes\u001b[0;34m(self, saes, reset_saes_end, use_error_term, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around 'run_with_cache' in HookedTransformer.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03mAttaches given SAEs before running the model with cache and then removes them.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    **kwargs: Keyword arguments for the model forward pass\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaes(\n\u001b[1;32m    226\u001b[0m     saes\u001b[38;5;241m=\u001b[39msaes, reset_saes_end\u001b[38;5;241m=\u001b[39mreset_saes_end, use_error_term\u001b[38;5;241m=\u001b[39muse_error_term\n\u001b[1;32m    227\u001b[0m ):\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_cache_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_cache_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:643\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    628\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    636\u001b[0m ]:\n\u001b[1;32m    637\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    647\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim)\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/transformer_lens/hook_points.py:566\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    553\u001b[0m     names_filter,\n\u001b[1;32m    554\u001b[0m     incl_bwd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m     pos_slice\u001b[38;5;241m=\u001b[39mpos_slice,\n\u001b[1;32m    558\u001b[0m )\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    561\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    562\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    563\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    564\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    565\u001b[0m ):\n\u001b[0;32m--> 566\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    568\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:532\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mLocallyOverridenDefaults(\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m, prepend_bos\u001b[38;5;241m=\u001b[39mprepend_bos, padding_side\u001b[38;5;241m=\u001b[39mpadding_side\n\u001b[1;32m    525\u001b[0m ):\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    527\u001b[0m         (\n\u001b[1;32m    528\u001b[0m             residual,\n\u001b[1;32m    529\u001b[0m             tokens,\n\u001b[1;32m    530\u001b[0m             shortformer_pos_embed,\n\u001b[1;32m    531\u001b[0m             attention_mask,\n\u001b[0;32m--> 532\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_to_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:294\u001b[0m, in \u001b[0;36mHookedTransformer.input_to_embed\u001b[0;34m(self, input, prepend_bos, padding_side, attention_mask, past_kv_cache)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m attention_mask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m tokens\u001b[38;5;241m.\u001b[39mshape, (\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention mask shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattention_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match tokens shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m     )\n\u001b[1;32m    293\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg))\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m past_kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;66;03m# If the padding side is left or we are using caching, we need to compute the attention\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# mask for the adjustment of absolute positional embeddings and attention masking so\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# that pad tokens are not attended.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prepend_bos \u001b[38;5;129;01mis\u001b[39;00m USE_DEFAULT_VALUE:\n\u001b[1;32m    302\u001b[0m         prepend_bos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdefault_prepend_bos\n",
      "File \u001b[0;32m~/.conda/envs/finetuning/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:271\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    Size of the full vocabulary with the added tokens.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwith_added_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_deep_attr(obj: Any, path: str):\n",
    "    \"\"\"Helper function to get a nested attribute from a object.\n",
    "    In practice used to access HookedTransformer HookPoints (eg model.blocks[0].attn.hook_z)\n",
    "\n",
    "    Args:\n",
    "        obj: Any object. In practice, this is a HookedTransformer (or subclass)\n",
    "        path: str. The path to the attribute you want to access. (eg \"blocks.0.attn.hook_z\")\n",
    "\n",
    "    returns:\n",
    "        Any. The attribute at the end of the path\n",
    "    \"\"\"\n",
    "    parts = path.split(\".\")\n",
    "    # Navigate to the last component in the path\n",
    "    for part in parts:\n",
    "        if part.isdigit():  # This is a list index\n",
    "            obj = obj[int(part)]\n",
    "        else:  # This is an attribute\n",
    "            obj = getattr(obj, part)\n",
    "    return obj\n",
    "\n",
    "def replace_with_zeros(layer, feature_ind, clean_cache):\n",
    "    \"\"\"\n",
    "    Forward hook function to replace activations in the corrupted run\n",
    "    with zeros instead of the clean activations.\n",
    "    \n",
    "    Args:\n",
    "        layer (int): The layer number where the intervention should take place.\n",
    "        feature_ind (int): The feature index to replace.\n",
    "        clean_cache (ActivationCache): The cache from the clean run (not used in this case).\n",
    "    \"\"\"\n",
    "    def hook_fn(module, input, output):\n",
    "        # Create a tensor of zeros with the same shape as the specific output slice\n",
    "        zero_tensor = torch.zeros_like(output[0, :, feature_ind])\n",
    "        \n",
    "        # Replace the corresponding activations in the output with zeros\n",
    "        output[0, :, feature_ind] = zero_tensor\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    return hook_fn\n",
    "\n",
    "# Define the hook function\n",
    "def replace_with_clean_activations(layer, feature_ind, clean_cache):\n",
    "    \"\"\"\n",
    "    Forward hook function to replace activations in the corrupted run\n",
    "    with those from the clean run.\n",
    "    \n",
    "    Args:\n",
    "        layer (int): The layer number where the intervention should take place.\n",
    "        feature_ind (int): The feature index to replace.\n",
    "        clean_cache (ActivationCache): The cache from the clean run.\n",
    "    \"\"\"\n",
    "    def hook_fn(module, input, output):\n",
    "        # Extract the clean activations\n",
    "        clean_acts = clean_cache[f'blocks.{layer}.hook_resid_post.hook_sae_acts_post'][0, :, feature_ind]\n",
    "        # print(output[:, :, feature_ind])\n",
    "        # print(clean_acts)\n",
    "        # Replace the corresponding activations in the output of the corrupted run\n",
    "        output[0, :, feature_ind] = clean_acts\n",
    "        # print(output[0, :, feature_ind])\n",
    "        return output\n",
    "    \n",
    "    return hook_fn\n",
    "\n",
    "def subtract_and_mean(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Subtract two tensors element-wise and return the mean of the resulting tensor.\n",
    "    \n",
    "    Args:\n",
    "        tensor1 (torch.Tensor): The first tensor.\n",
    "        tensor2 (torch.Tensor): The second tensor.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The mean of the element-wise subtraction of the two tensors.\n",
    "    \"\"\"\n",
    "    # Ensure that both tensors have the same shape\n",
    "    assert tensor1.shape == tensor2.shape, \"Tensors must have the same shape\"\n",
    "    \n",
    "    # Subtract the tensors element-wise\n",
    "    difference = tensor1 - tensor2\n",
    "    \n",
    "    # Calculate the mean of the difference\n",
    "    mean_difference = torch.mean(difference)\n",
    "    \n",
    "    return mean_difference\n",
    "\n",
    "\n",
    "\n",
    "def feature_acts(cache):\n",
    "    return cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, :, 15191][13]\n",
    "\n",
    "def overall_feature(cache):\n",
    "    return cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, :, 15191]\n",
    "\n",
    "def feature_difference(cache1, cache2):\n",
    "    return feature_acts(cache1) - feature_acts(cache2)\n",
    "\n",
    "def overall_feature_difference(cache1, cache2):\n",
    "    return subtract_and_mean(cache1['blocks.8.hook_resid_post.hook_sae_acts_post'][0, :, 15191], cache2['blocks.8.hook_resid_post.hook_sae_acts_post'][0, :, 15191])\n",
    "# # Specify the layer number and feature index\n",
    "# layer = 2  # Example: replace activations at layer 2\n",
    "# feature_ind = 5  # Example: replace activations at feature index 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "op_clean, clean_cache = model.run_with_cache_with_saes(clean, saes=saes)\n",
    "op_corr, corr_cache = model.run_with_cache_with_saes(corrupted, saes=saes)\n",
    "\n",
    "print(\"og logit\", op_clean.shape)\n",
    "print(\"Original feature diff: \", feature_difference(clean_cache, corr_cache))\n",
    "\n",
    "for key, val in operator_features.items():\n",
    "    layer = key\n",
    "    for ind in range(10000):\n",
    "        feature_ind = ind\n",
    "        hook = replace_with_clean_activations(layer, feature_ind, clean_cache)\n",
    "        # with model.saes(saes=saes, reset_saes_end=True):\n",
    "            # Now that SAEs are attached, access the hook point\n",
    "        hook_point = get_deep_attr(saes[layer], 'hook_sae_acts_post')\n",
    "        # print(hook_point)\n",
    "        # Register the hook\n",
    "        hook_handle = hook_point.register_forward_hook(hook)\n",
    "        # print(hook_handle)\n",
    "        # Run the corrupted prompt with the hook applied\n",
    "        inter_op, corr_cache_intervened = model.run_with_cache_with_saes(corrupted, saes=saes)\n",
    "        # print(\"intervened logit\", inter_op[0, -1, :10])\n",
    "        # Remove the hook after the forward pass\n",
    "        hook_handle.remove()\n",
    "        \n",
    "        # print(subtract_and_mean(op_corr, inter_op))\n",
    "        if overall_feature_difference(corr_cache_intervened, corr_cache) != 0: \n",
    "            print(\"UPPPPPPPP\")\n",
    "        # print(overall_feature_difference(corr_cache_intervened, corr_cache))\n",
    "            \n",
    "            \n",
    "        # hook_point = get_deep_attr(model, f'blocks.{layer}.hook_resid_post.hook_sae_acts_post')\n",
    "        # print(hook_point)\n",
    "        # hook_handle = hook_point.register_forward_hook(hook)\n",
    "        # print(hook_handle)\n",
    "        # _, corr_cache_intervened = model.run_with_cache_with_saes(corrupted, saes=saes)\n",
    "        # hook_handle.remove()\n",
    "        # print(\"PLESAAE CHANGE ONCE JUST ONCE :\", feature_acts(corr_cache_intervened))\n",
    "        # print(\"PLESAAE CHANGE ONCE JUST ONCE :\", feature_difference(clean_cache, corr_cache_intervened))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c23341-823f-446b-9bed-38e66593682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in operator_features.items():\n",
    "    layer = key\n",
    "    for feature_ind in val:\n",
    "        print(feature_ind.item())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "eed927b9-eb3c-48c0-979d-1b63bbf745bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature difference:  tensor(7.5431, device='cuda:0')\n",
      "layer:  0\n",
      "layer:  1\n",
      "layer:  2\n",
      "layer:  3\n",
      "layer:  4\n",
      "layer:  5\n",
      "layer:  6\n",
      "layer:  7\n",
      "layer:  8\n",
      "Top 10 features with the highest overall_feature_difference:\n",
      "Layer: 1, Feature Index: 172, Overall Feature Difference: 5.880991693629767e-07\n",
      "Layer: 1, Feature Index: 202, Overall Feature Difference: 5.722046125811175e-07\n",
      "Layer: 5, Feature Index: 697, Overall Feature Difference: 5.722046125811175e-07\n",
      "Layer: 5, Feature Index: 112, Overall Feature Difference: 5.404154990173993e-07\n",
      "Layer: 0, Feature Index: 133, Overall Feature Difference: 5.245208853921213e-07\n",
      "Layer: 2, Feature Index: 616, Overall Feature Difference: 5.245208853921213e-07\n",
      "Layer: 1, Feature Index: 534, Overall Feature Difference: 5.086263286102621e-07\n",
      "Layer: 4, Feature Index: 646, Overall Feature Difference: 5.086263286102621e-07\n",
      "Layer: 2, Feature Index: 498, Overall Feature Difference: 4.92731771828403e-07\n",
      "Layer: 4, Feature Index: 920, Overall Feature Difference: 4.92731771828403e-07\n"
     ]
    }
   ],
   "source": [
    "# Corr -> clean \n",
    "\n",
    "_, clean_cache = model.run_with_cache_with_saes(clean, saes=saes)\n",
    "_, corr_cache = model.run_with_cache_with_saes(corrupted, saes=saes)\n",
    "\n",
    "print(\"Original feature difference: \", feature_difference(clean_cache, corr_cache))\n",
    "\n",
    "# Dictionary to store the differences\n",
    "differences = []\n",
    "\n",
    "for key, val in operator_features.items():\n",
    "    layer = key\n",
    "    print(\"layer: \", layer)\n",
    "    for feature_ind in range(1000):\n",
    "        hook = replace_with_clean_activations(layer, feature_ind, corr_cache)\n",
    "        \n",
    "        # Now that SAEs are attached, access the hook point\n",
    "        hook_point = get_deep_attr(saes[layer], 'hook_sae_acts_post')\n",
    "        \n",
    "        # Register the hook\n",
    "        hook_handle = hook_point.register_forward_hook(hook)\n",
    "        \n",
    "        # Run the corrupted prompt with the hook applied\n",
    "        _, clean_cache_intervened = model.run_with_cache_with_saes(clean, saes=saes)\n",
    "        \n",
    "        # Remove the hook after the forward pass\n",
    "        hook_handle.remove()\n",
    "        \n",
    "        # Calculate the overall feature difference\n",
    "        diff = overall_feature_difference(clean_cache_intervened, clean_cache)\n",
    "        \n",
    "        # Store the difference along with the corresponding layer and feature index\n",
    "        differences.append((layer, feature_ind, diff))\n",
    "\n",
    "# Sort the differences to get the top 10\n",
    "top_differences = sorted(differences, key=lambda x: x[2], reverse=True)[:10]\n",
    "\n",
    "# Print the top 10 features with their differences\n",
    "print(\"Top 10 features with the highest overall_feature_difference:\")\n",
    "for layer, feature_ind, diff in top_differences:\n",
    "    print(f\"Layer: {layer}, Feature Index: {feature_ind}, Overall Feature Difference: {diff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3b520376-90fd-44c2-b1e2-d9bad67abadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "og logit torch.Size([1, 15, 256000])\n",
      "Original feature diff:  tensor(7.5431, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(5.8810e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.3842e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.6094e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(6.8347e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.0200e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-1.5895e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(7.9473e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.1126e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(6.1989e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.8147e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.8610e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.2252e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-3.1789e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.4505e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.9073e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(6.0399e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.2915e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(7.9473e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-1.9073e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.1126e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.9736e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.4305e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.4305e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.0200e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.0200e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.2252e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.2915e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.3379e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.3842e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(5.0863e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.4305e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.9736e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-3.4968e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.4305e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.8147e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-9.5367e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.2915e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.3842e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(7.9473e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-1.4305e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.6094e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.8610e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.2252e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.2252e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.3842e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.2252e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.7021e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.3842e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(7.9473e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.4505e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.7021e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.6558e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-3.1789e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.3842e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.3842e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.0200e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.4968e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.7684e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.4305e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.3379e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.6558e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.2915e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.4968e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.1326e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.9073e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-3.1789e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.3842e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.3842e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.7021e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(7.9473e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.1126e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.7021e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.0200e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.9073e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.9073e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-1.9073e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.3842e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.9073e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.6558e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.1126e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.9736e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.8147e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(5.7220e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.1126e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-7.9473e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.0200e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-1.4305e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-3.1789e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-9.5367e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-9.5367e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.4968e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(9.5367e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.1326e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.9736e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-1.5895e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.9736e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-1.5895e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(5.8810e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.1326e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.0200e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.0200e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-1.9073e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.7021e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.4968e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.1126e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(5.7220e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.1326e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(7.9473e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.8147e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.2252e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-7.9473e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.4305e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.7021e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.9736e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.2252e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.0200e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.1326e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.3842e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.3842e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.1126e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.2252e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.4968e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.9073e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.6558e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.0200e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.3842e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.7021e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.0200e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.1126e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.3842e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.4968e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-3.1789e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-1.5895e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(9.5367e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.7021e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.8610e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.4968e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.7021e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.0200e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-3.1789e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.9736e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.3379e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.1126e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.9073e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.8610e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(7.9473e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.2252e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(7.9473e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.0663e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.4305e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(9.5367e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.3379e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.2252e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.4968e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-1.1126e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.9073e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.3842e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.1326e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.4305e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.9073e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.3379e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(9.5367e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(9.5367e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.6558e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(7.9473e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.4305e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(7.9473e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-3.1789e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.2252e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.1126e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.4305e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.1126e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(7.9473e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.1126e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.1126e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.7484e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.9073e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(6.3578e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.1126e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(-4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(3.1789e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.9073e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(9.5367e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.2716e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(2.5431e-07, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(4.7684e-08, device='cuda:0')\n",
      "UPPPPPPPP\n",
      "tensor(1.5895e-08, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Corr -> clean \n",
    "\n",
    "op_clean, clean_cache = model.run_with_cache_with_saes(clean, saes=saes)\n",
    "op_corr, corr_cache = model.run_with_cache_with_saes(corrupted, saes=saes)\n",
    "\n",
    "print(\"og logit\", op_clean.shape)\n",
    "print(\"Original feature diff: \", feature_difference(clean_cache, corr_cache))\n",
    "\n",
    "for key, val in operator_features.items():\n",
    "    layer = key\n",
    "    for feature_ind in val:\n",
    "        feature_ind = feature_ind.item()\n",
    "        hook = replace_with_zeros(layer, feature_ind, corr_cache)\n",
    "        # with model.saes(saes=saes, reset_saes_end=True):\n",
    "            # Now that SAEs are attached, access the hook point\n",
    "        hook_point = get_deep_attr(saes[layer], 'hook_sae_acts_post')\n",
    "        # print(hook_point)\n",
    "        # Register the hook\n",
    "        hook_handle = hook_point.register_forward_hook(hook)\n",
    "        # print(hook_handle)\n",
    "        # Run the corrupted prompt with the hook applied\n",
    "        inter_op, clean_cache_intervened = model.run_with_cache_with_saes(clean, saes=saes)\n",
    "        # print(\"intervened logit\", inter_op[0, -1, :10])\n",
    "        # Remove the hook after the forward pass\n",
    "        hook_handle.remove()\n",
    "        \n",
    "        # print(subtract_and_mean(op_corr, inter_op))\n",
    "        # print(\"PLESAAE CHANGE ONCE JUST ONCE :\", feature_difference(clean_cache_intervened, corr_cache))\n",
    "        if overall_feature_difference(clean_cache_intervened, clean_cache) != 0: \n",
    "            print(\"UPPPPPPPP\")\n",
    "            print(overall_feature_difference(clean_cache_intervened, clean_cache))\n",
    "            # print(\"Og :\", overall_feature(clean_cache))\n",
    "            # print(\"Intervened :\", overall_feature(clean_cache_intervened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2fb25d-bc1a-4f63-931f-399948737ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd375a9-339d-4668-b1c5-b53b6c5524b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9412354d-cba1-4e01-8dbb-4fc655f9a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_names_to_reset = []\n",
    "prev_saes = []\n",
    "for sae in saes:\n",
    "    print(sae.cfg.hook_name)\n",
    "    act_names_to_reset.append(sae.cfg.hook_name)\n",
    "    prev_sae = model.acts_to_saes.get(sae.cfg.hook_name, None)\n",
    "    print(model.acts_to_saes)\n",
    "    prev_saes.append(prev_sae)\n",
    "    model.add_sae(sae, use_error_term=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad9707f-0680-4cbe-bfc0-38468a80c2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce55f3f0-d848-4a2a-994c-7ee8383bc8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74afb24-91b0-47ec-a1e9-2b64bffb6a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19e47e-c066-4d19-9ef1-196f49d4b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, (clean, corrupted) in enumerate(dataset):\n",
    "    _, clean_cache = model.run_with_cache_with_saes(clean, saes=saes)\n",
    "    _, corr_cache = model.run_with_cache_with_saes(corrupted, saes=saes)\n",
    "    \n",
    "    hook_handle = model.get_layer(layer_name).register_forward_hook(\n",
    "        lambda module, input, output: forward_hook(module, input, output, clean_cache, layer_name, feature_index)\n",
    "    )\n",
    "    \n",
    "    patched_output, _ = model(corrupted)\n",
    "    hook_handle.remove()\n",
    "    \n",
    "    metric_diff = feature_difference(clean_cache, corr_cache)\n",
    "    results.append(metric_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c44ea-4373-47e6-b841-c5c77635933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_activations(corr_activations, clean_cache, layer_name, feature_index):\n",
    "    # Replace the corrupted activations at the specified feature index with the clean one\n",
    "    patched_activations = corr_activations.clone()  # clone to avoid modifying the original tensor\n",
    "    clean_activations = clean_cache[layer_name]\n",
    "    patched_activations[..., feature_index] = clean_activations[..., feature_index]\n",
    "    return patched_activations\n",
    "\n",
    "def forward_hook(module, input, output, clean_cache, layer_name, feature_index):\n",
    "    return patch_activations(output, clean_cache, layer_name, feature_index)\n",
    "\n",
    "\n",
    "# Example layer name and feature index\n",
    "layer_name = 'blocks.8.hook_resid_post.hook_sae_acts_post'\n",
    "feature_index = 15191\n",
    "\n",
    "# Register the hook\n",
    "hook_handle = model.get_layer(layer_name).register_forward_hook(\n",
    "    lambda module, input, output: forward_hook(module, input, output, clean_cache, layer_name, feature_index)\n",
    ")\n",
    "\n",
    "# Run the model on the corrupted input with the hook applied\n",
    "patched_output, _ = model(corrupted)\n",
    "\n",
    "# Remove the hook after running\n",
    "hook_handle.remove()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-finetuning]",
   "language": "python",
   "name": "conda-env-.conda-finetuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
