{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c912b37-6257-409c-b3ae-f57758663b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "# Imports for displaying vis in Colab / notebook\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# For the most part I'll try to import functions and classes near where they are used\n",
    "# to make it clear where they come from.\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a0fa40-a6b3-42c4-aad2-abba41867f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "def get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"8-gemmascope-res-16k\", feature_idx=0):\n",
    "    return html_template.format(sae_release, sae_id, feature_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e4dff8-d615-4127-8348-d63fa65e035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d3b51db99d4a87954b3d6bc2477b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "model = HookedSAETransformer.from_pretrained(\"google/gemma-2-2b\", device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "10d577a5-e29b-4e4e-a277-0edfac1097d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'What', ' is', ' the', ' output', ' of', ' ', '5', '3', ' plus', ' ', '3', '2', ' ?', ' A', ':', ' ']\n",
      "Tokenized answer: [' ', '8']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">469</span><span style=\"font-weight: bold\">      Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.61</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span><span style=\"font-weight: bold\">% Token: | |</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m469\u001b[0m\u001b[1m      Logit: \u001b[0m\u001b[1;36m10.61\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.00\u001b[0m\u001b[1m% Token: | |\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 28.92 Prob: 40.73% Token: |8|\n",
      "Top 1th token. Logit: 28.03 Prob: 16.72% Token: |5|\n",
      "Top 2th token. Logit: 27.64 Prob: 11.27% Token: |1|\n",
      "Top 3th token. Logit: 27.49 Prob:  9.76% Token: |2|\n",
      "Top 4th token. Logit: 26.98 Prob:  5.83% Token: |3|\n",
      "Top 5th token. Logit: 26.85 Prob:  5.16% Token: |7|\n",
      "Top 6th token. Logit: 26.71 Prob:  4.45% Token: |4|\n",
      "Top 7th token. Logit: 26.51 Prob:  3.67% Token: |6|\n",
      "Top 8th token. Logit: 25.52 Prob:  1.36% Token: |9|\n",
      "Top 9th token. Logit: 25.19 Prob:  0.98% Token: |0|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28.33</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29.65</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m28.33\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m29.65\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 28.33 Prob: 29.65% Token: |8|\n",
      "Top 1th token. Logit: 28.08 Prob: 23.24% Token: |5|\n",
      "Top 2th token. Logit: 27.49 Prob: 12.86% Token: |1|\n",
      "Top 3th token. Logit: 27.20 Prob:  9.60% Token: |2|\n",
      "Top 4th token. Logit: 26.87 Prob:  6.89% Token: |3|\n",
      "Top 5th token. Logit: 26.57 Prob:  5.13% Token: |7|\n",
      "Top 6th token. Logit: 26.47 Prob:  4.64% Token: |6|\n",
      "Top 7th token. Logit: 26.46 Prob:  4.57% Token: |4|\n",
      "Top 8th token. Logit: 25.64 Prob:  2.02% Token: |9|\n",
      "Top 9th token. Logit: 25.09 Prob:  1.17% Token: |0|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' '</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">469</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'8'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' '\u001b[0m, \u001b[1;36m469\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'8'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "\n",
    "prompt = \"What is the output of 53 plus 32 ? A: \"\n",
    "answer = '8'\n",
    "# Show that the model can confidently predict the next token.\n",
    "test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed01ab7a-ee89-4126-aab5-7f93c915a613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"300\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/8-gemmascope-res-16k/15191?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc77be5d20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"8-gemmascope-res-16k\", feature_idx=int(15191))\n",
    "display(IFrame(html, width=1200, height=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d03349d0-77b7-4190-904a-60c2cd3f695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', 'What', ' is', ' the', ' output', ' of', ' ', '5', '3', ' plus', ' ', '3', '2', ' ?', ' A', ':', ' ']\n",
      "13  ?\n",
      "['<bos>', 'How', ' much', ' is', ' ', '1', '2', ' plus', ' ', '8', '9', ' ?', ' A', ':', ' ']\n",
      "11  ?\n",
      "['<bos>', 'What', ' is', ' ', '3', '1', ' plus', ' ', '1', '6', '?', ' It', ' is', ' ']\n",
      "10 ?\n",
      "['<bos>', 'What', ' is', ' the', ' sum', ' of', ' ', '5', '9', ' and', ' the', ' product', ' of', ' ', '8', '2', ' and', ' ', '3', '3', '?', ' It', ' is', ' ']\n",
      "20 ?\n",
      "['<bos>', 'What', ' is', ' the', ' difference', ' between', ' ', '1', '9', ' and', ' the', ' product', ' of', ' ', '5', '5', ' and', ' ', '1', '0', ' ?']\n",
      "20  ?\n",
      "['<bos>', '1', '3', ' +', ' ', '4', '5', ' *', ' ', '4', '2', ' =', ' ']\n",
      "11  =\n"
     ]
    }
   ],
   "source": [
    "equal_variation = {'What is the output of 53 plus 32 ? A: ': 13,\n",
    "                   'How much is 12 plus 89 ? A: ': 12, \n",
    "                   'What is 31 plus 16? It is ': 12, \n",
    "                   'What is the sum of 59 and the product of 82 and 33? It is ': 21,\n",
    "                   'What is the difference between 19 and the product of 55 and 10 ?' : 21, \n",
    "                   '13 + 45 * 42 = ': 12} \n",
    "for pr in equal_variation.keys():\n",
    "    tokens = model.to_str_tokens(pr)\n",
    "    print(tokens)\n",
    "    for ind, token in enumerate(tokens):\n",
    "        # print(ind, token)\n",
    "        if '?' in token or '=' in token:\n",
    "            print(ind, token)\n",
    "            equal_variation[pr] = ind\n",
    "    # print(model.tokenizer.encode(pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06bbddb8-daad-42fa-867a-b97a84d64664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'What is the output of 53 plus 32 ? A: ': 13,\n",
       " 'How much is 12 plus 89 ? A: ': 11,\n",
       " 'What is 31 plus 16? It is ': 10,\n",
       " 'What is the sum of 59 and the product of 82 and 33? It is ': 20,\n",
       " 'What is the difference between 19 and the product of 55 and n10 ?': 20,\n",
       " '13 + 45 * 42 = ': 11}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equal_variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c10efdd1-a751-47e0-b821-af5802c9cb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 16384])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache[f'blocks.8.hook_resid_post.hook_sae_acts_post'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c5232e2-2007-41fb-9ba6-eb92435735ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17, 16384])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache['blocks.8.hook_resid_post.hook_sae_acts_post'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bcbec742-9c56-48a8-bfb0-3ea4f90862b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.4373e-03, 2.9353e-04, 3.4759e-08, 1.0000e+00, 6.2925e-12, 2.7857e-07,\n",
      "         4.3877e-03, 8.1741e-07, 1.1822e-03, 5.7304e-07, 9.4502e-05, 2.8141e-04,\n",
      "         1.4745e-04, 9.9998e-01, 5.5514e-08, 7.1966e-02, 6.2936e-04, 1.0000e+00,\n",
      "         5.9179e-04, 9.9625e-01],\n",
      "        [3.3724e-10, 1.0378e-08, 3.4759e-08, 0.0000e+00, 6.2925e-12, 2.7857e-07,\n",
      "         1.2796e-09, 8.1741e-07, 1.3132e-01, 5.7304e-07, 9.4502e-05, 2.8141e-04,\n",
      "         2.4072e-03, 5.7876e-11, 5.5514e-08, 7.2224e-04, 8.7562e-02, 7.2615e-09,\n",
      "         5.9179e-04, 8.0424e-06],\n",
      "        [3.3724e-10, 1.0378e-08, 3.4759e-08, 0.0000e+00, 6.2925e-12, 2.7857e-07,\n",
      "         1.2796e-09, 8.1741e-07, 5.4638e-02, 7.7114e-05, 9.4502e-05, 1.3838e-02,\n",
      "         1.6084e-03, 5.7876e-11, 4.2306e-03, 2.1765e-02, 6.2936e-04, 7.2615e-09,\n",
      "         5.9179e-04, 8.0424e-06],\n",
      "        [3.3724e-10, 1.0378e-08, 3.4759e-08, 0.0000e+00, 6.2925e-12, 2.7857e-07,\n",
      "         1.2796e-09, 8.1741e-07, 1.5620e-02, 9.1871e-01, 1.8900e-03, 6.9146e-03,\n",
      "         3.3578e-03, 5.7876e-11, 1.1538e-01, 4.6186e-01, 1.8752e-01, 7.2615e-09,\n",
      "         5.9179e-04, 8.0424e-06],\n",
      "        [3.3724e-10, 4.9569e-04, 3.4759e-08, 0.0000e+00, 6.2925e-12, 2.7857e-07,\n",
      "         1.2796e-09, 8.1741e-07, 1.0432e-03, 4.4838e-02, 7.4748e-02, 2.8141e-04,\n",
      "         2.3284e-02, 1.9177e-05, 8.2000e-01, 7.2224e-04, 6.2936e-04, 7.2615e-09,\n",
      "         5.9179e-04, 8.0424e-06],\n",
      "        [3.3724e-10, 1.0064e-04, 3.4759e-08, 0.0000e+00, 6.2925e-12, 2.7857e-07,\n",
      "         2.4588e-08, 8.1741e-07, 4.1940e-02, 2.5596e-02, 1.0162e-02, 2.8141e-04,\n",
      "         5.7807e-02, 9.7605e-07, 5.8822e-02, 7.2224e-04, 1.2510e-02, 7.2615e-09,\n",
      "         5.9179e-04, 8.0424e-06],\n",
      "        [3.3724e-10, 2.5074e-01, 3.4759e-08, 0.0000e+00, 3.5044e-10, 2.7857e-07,\n",
      "         1.2796e-09, 8.1741e-07, 6.5786e-02, 1.0032e-03, 1.4803e-02, 2.8141e-04,\n",
      "         2.7355e-02, 2.1233e-09, 1.5209e-03, 1.3183e-02, 6.2936e-04, 7.2615e-09,\n",
      "         5.9179e-04, 8.0424e-06],\n",
      "        [3.3724e-10, 1.2990e-04, 3.4759e-08, 0.0000e+00, 7.2223e-06, 2.7857e-07,\n",
      "         1.2796e-09, 8.1741e-07, 2.8651e-02, 1.2150e-05, 9.4502e-05, 2.8141e-04,\n",
      "         7.8210e-02, 5.7876e-11, 4.1478e-06, 7.2224e-04, 6.2936e-04, 7.2615e-09,\n",
      "         5.9179e-04, 8.0424e-06],\n",
      "        [3.3724e-10, 2.8193e-06, 3.4759e-08, 0.0000e+00, 3.8438e-09, 2.7857e-07,\n",
      "         1.2796e-09, 8.1741e-07, 6.1001e-03, 5.7304e-07, 9.4502e-05, 2.8141e-04,\n",
      "         8.2446e-02, 1.1344e-09, 5.5514e-08, 7.2224e-04, 1.2062e-02, 7.2615e-09,\n",
      "         5.9179e-04, 8.0424e-06],\n",
      "        [3.3724e-10, 6.6229e-07, 3.4759e-08, 0.0000e+00, 5.6149e-01, 2.7857e-07,\n",
      "         1.2796e-09, 8.1741e-07, 2.0895e-01, 5.7304e-07, 9.4502e-05, 2.8141e-04,\n",
      "         3.3592e-01, 5.7876e-11, 5.5514e-08, 7.2224e-04, 6.2936e-04, 7.2615e-09,\n",
      "         5.9179e-04, 8.0424e-06],\n",
      "        [3.3724e-10, 1.9655e-07, 3.4759e-08, 0.0000e+00, 4.3844e-01, 2.7857e-07,\n",
      "         1.2796e-09, 8.1741e-07, 1.4696e-01, 5.7304e-07, 9.4502e-05, 2.8141e-04,\n",
      "         3.6400e-03, 5.7876e-11, 5.5514e-08, 7.2224e-04, 6.2936e-04, 7.2615e-09,\n",
      "         5.9179e-04, 8.0424e-06],\n",
      "        [3.3724e-10, 1.0378e-08, 3.4759e-08, 0.0000e+00, 2.2272e-07, 2.7857e-07,\n",
      "         1.2796e-09, 8.1741e-07, 1.3864e-01, 5.7304e-07, 9.4502e-05, 2.8141e-04,\n",
      "         1.7467e-03, 5.7876e-11, 5.5514e-08, 7.2224e-04, 6.2936e-04, 7.2615e-09,\n",
      "         5.9179e-04, 8.0424e-06],\n",
      "        [3.3724e-10, 1.0378e-08, 3.4759e-08, 0.0000e+00, 4.2802e-08, 2.7857e-07,\n",
      "         1.2796e-09, 8.1741e-07, 1.2467e-02, 5.7304e-07, 9.4502e-05, 2.8141e-04,\n",
      "         1.2747e-02, 5.7876e-11, 5.5514e-08, 7.2224e-04, 5.9231e-02, 7.2615e-09,\n",
      "         5.9179e-04, 8.0424e-06],\n",
      "        [9.9756e-01, 7.4823e-01, 1.0000e+00, 0.0000e+00, 6.4693e-05, 1.0000e+00,\n",
      "         3.4283e-03, 9.9999e-01, 7.8913e-02, 9.7586e-03, 8.5253e-01, 9.5882e-01,\n",
      "         3.6147e-01, 9.0606e-08, 4.4223e-05, 4.2256e-01, 3.5446e-01, 3.8641e-06,\n",
      "         2.8569e-01, 3.6286e-03],\n",
      "        [3.3724e-10, 1.0582e-06, 3.4759e-08, 0.0000e+00, 6.2925e-12, 2.7857e-07,\n",
      "         4.3891e-07, 8.1741e-07, 3.0839e-02, 5.7304e-07, 4.4825e-02, 1.2632e-02,\n",
      "         5.7832e-03, 5.7876e-11, 5.5514e-08, 7.2224e-04, 1.8932e-01, 7.2615e-09,\n",
      "         5.9179e-04, 8.0424e-06],\n",
      "        [3.3724e-10, 1.2458e-07, 3.4759e-08, 0.0000e+00, 6.2925e-12, 2.7857e-07,\n",
      "         3.7639e-06, 8.1741e-07, 2.6333e-02, 5.7304e-07, 9.4502e-05, 4.4224e-03,\n",
      "         1.9270e-03, 5.7876e-11, 5.5514e-08, 7.2224e-04, 9.1671e-02, 7.2615e-09,\n",
      "         5.9179e-04, 8.0424e-06],\n",
      "        [3.3724e-10, 1.0378e-08, 3.4759e-08, 0.0000e+00, 6.4645e-08, 2.7857e-07,\n",
      "         9.9218e-01, 8.1741e-07, 1.0610e-02, 5.7304e-07, 9.4502e-05, 2.8141e-04,\n",
      "         1.4745e-04, 5.7876e-11, 5.5514e-08, 7.2224e-04, 6.2936e-04, 7.2615e-09,\n",
      "         7.0543e-01, 8.0424e-06]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res\", # <- Release name \n",
    "    sae_id = \"layer_8/width_16k/average_l0_71\", # <- SAE id (not always a hook point!)\n",
    "    device = device\n",
    ")\n",
    "pr = 'What is the output of 53 plus 32 ? A: '\n",
    "\n",
    "# for ind in range(1, 17):\n",
    "ind = 13\n",
    "_, cache = model.run_with_cache_with_saes(pr, saes=[sae])\n",
    "# print*F.softmax(x, dim=1)\n",
    "x = cache['blocks.8.hook_resid_post.hook_sae_acts_post']\n",
    "topk_values, topk_indices = torch.topk(x[0, ind, :], 20)\n",
    "gathered_values = x[0, :, topk_indices]\n",
    "softmaxed_values = F.softmax(gathered_values, dim=0)\n",
    "print(softmaxed_values)\n",
    "# vals, inds = torch.topk(cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, 13, :], 50)\n",
    "# inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0aecf20c-f2ea-432d-991d-1d2e1835f4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 20])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmaxed_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ec435953-030f-4785-b282-8922455225a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 YOPPP\n",
      "16 YOPPP\n"
     ]
    }
   ],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res\", # <- Release name \n",
    "    sae_id = \"layer_8/width_16k/average_l0_71\", # <- SAE id (not always a hook point!)\n",
    "    device = device\n",
    ")\n",
    "pr = 'What is the output of 53 plus 32 ? A: '\n",
    "\n",
    "for ind in range(1, 17):\n",
    "    _, cache = model.run_with_cache_with_saes(pr, saes=[sae])\n",
    "    vals, inds = torch.topk(cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, ind, :], 50)\n",
    "    # print(inds)\n",
    "    if 15191 in inds:\n",
    "        print(ind, \"YOPPP\")\n",
    "# let's print the top 5 features and how much they fired\n",
    "# _, cache = model.run_with_cache_with_saes(pr, saes=[sae])\n",
    "# vals, inds = torch.topk(cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, -1, :], 10)\n",
    "# for val, ind in zip(vals, inds):\n",
    "#     print(f\"Feature {ind} fired {val:.2f}\")\n",
    "    # html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"8-gemmascope-res-16k\", feature_idx=int(ind))\n",
    "    # display(IFrame(html, width=1200, height=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26994e-ee9f-46ab-b65f-84b678e5e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 8\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=\"gemma-scope-2b-pt-res-canonical\",  # <- Release name \n",
    "        sae_id=f\"layer_{layer}/width_16k/canonical\",  # <- SAE id (not always a hook point!)\n",
    "        device=device\n",
    "    )\n",
    "pr = 'What is the output of 53 plus 32 ? A: '\n",
    "# let's print the top 5 features and how much they fired\n",
    "_, cache = model.run_with_cache_with_saes(pr, saes=[sae])\n",
    "vals, inds = torch.topk(cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, -1, :], 5)\n",
    "for val, ind in zip(vals, inds):\n",
    "    print(f\"Feature {ind} fired {val:.2f}\")\n",
    "    html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"8-gemmascope-res-16k\", feature_idx=int(ind))\n",
    "    display(IFrame(html, width=1200, height=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "301efa48-2b0b-42c8-b8cd-605652269edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = 'What is the output of 53 plus 32 ? A: '\n",
    "\n",
    "for ind in range(1, 17):\n",
    "    _, cache = model.run_with_cache_with_saes(pr, saes=[sae])\n",
    "    vals, inds = torch.topk(cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, ind, :], 50)\n",
    "    # print(inds)\n",
    "    if 15191 in inds:\n",
    "        print(\"YOPPP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "57115533-e359-44df-9780-91047ab71341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haja\n"
     ]
    }
   ],
   "source": [
    "if 14801 in inds:\n",
    "    print('haja')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d4dcf-cc32-483f-9a90-b1d4298b143d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14801,  5443,  8857,  7625,  7357,  3178, 15216,  1434,  6179, 14157,\n",
       "        15733,  1934, 10676, 12868,  7555, 14787, 11233,   637, 13106,  6309,\n",
       "        16021,  3604,  4487,   863,  3919, 10796,  2811, 14049, 12656, 13304,\n",
       "         9253, 10930,  7316,   929,  3975,  3187,  6969,  6488,  8842, 11295,\n",
       "         2145, 13034,  3174,  2122, 13978,  3042,  9096,  8409, 13905, 16309],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a1d3dc1-3fd9-4c80-8f8a-fd10ed790cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer = 8\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=\"gemma-scope-2b-pt-res\",  # <- Release name \n",
    "        sae_id=f\"layer_8/width_16k/average_l0_71\",  # <- SAE id (not always a hook point!)\n",
    "        device=device\n",
    "    )\n",
    "vals_dict = defaultdict(float)\n",
    "count_dict = defaultdict(int)\n",
    "for pr, ind in equal_variation.items():\n",
    "    _, cache = model.run_with_cache_with_saes(pr, saes=[sae])\n",
    "    vals, inds = torch.topk(cache[f'blocks.8.hook_resid_post.hook_sae_acts_post'][0, ind, :], 50)\n",
    "    \n",
    "    for val, ind in zip(vals, inds):\n",
    "        vals_dict[ind.item()] += val.item()\n",
    "        count_dict[ind.item()] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af18d083-c29c-4a66-a299-342bb9393b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_dict[15191]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a418f4de-36c4-4b19-b0c0-251ce0b32d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"300\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/8-gemmascope-res-16k/7759?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc766c73d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"8-gemmascope-res-16k\", feature_idx=int(7759))\n",
    "display(IFrame(html, width=1200, height=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "54a35bcc-2b0f-461b-bb22-dabc32ffa8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {4909: 5,\n",
       "             7759: 6,\n",
       "             2003: 3,\n",
       "             9213: 6,\n",
       "             6179: 6,\n",
       "             4781: 5,\n",
       "             8857: 6,\n",
       "             4646: 5,\n",
       "             2024: 6,\n",
       "             4087: 5,\n",
       "             8109: 1,\n",
       "             2165: 5,\n",
       "             10524: 6,\n",
       "             5001: 1,\n",
       "             11670: 5,\n",
       "             14888: 4,\n",
       "             15075: 3,\n",
       "             778: 3,\n",
       "             15191: 4,\n",
       "             2707: 3,\n",
       "             4797: 5,\n",
       "             10585: 2,\n",
       "             2121: 5,\n",
       "             6061: 6,\n",
       "             9261: 2,\n",
       "             14157: 3,\n",
       "             4978: 2,\n",
       "             8262: 4,\n",
       "             1083: 3,\n",
       "             571: 4,\n",
       "             9188: 5,\n",
       "             15115: 2,\n",
       "             734: 1,\n",
       "             9561: 3,\n",
       "             7876: 1,\n",
       "             14993: 1,\n",
       "             2288: 1,\n",
       "             8123: 3,\n",
       "             12684: 2,\n",
       "             11746: 3,\n",
       "             5864: 1,\n",
       "             7653: 1,\n",
       "             11973: 1,\n",
       "             498: 1,\n",
       "             4788: 1,\n",
       "             5821: 2,\n",
       "             324: 6,\n",
       "             10971: 2,\n",
       "             10825: 2,\n",
       "             5326: 2,\n",
       "             13831: 4,\n",
       "             1500: 4,\n",
       "             11351: 5,\n",
       "             8096: 2,\n",
       "             10350: 4,\n",
       "             15018: 2,\n",
       "             11010: 2,\n",
       "             14355: 1,\n",
       "             3400: 2,\n",
       "             1567: 1,\n",
       "             15290: 4,\n",
       "             7194: 1,\n",
       "             5030: 3,\n",
       "             8305: 2,\n",
       "             16050: 3,\n",
       "             13504: 3,\n",
       "             13653: 3,\n",
       "             9601: 1,\n",
       "             14541: 1,\n",
       "             4235: 2,\n",
       "             8837: 1,\n",
       "             3116: 1,\n",
       "             13702: 1,\n",
       "             7679: 2,\n",
       "             13156: 1,\n",
       "             3069: 2,\n",
       "             2576: 1,\n",
       "             10129: 1,\n",
       "             14755: 1,\n",
       "             11671: 2,\n",
       "             728: 2,\n",
       "             1003: 2,\n",
       "             7828: 1,\n",
       "             15030: 1,\n",
       "             11416: 3,\n",
       "             6426: 1,\n",
       "             824: 1,\n",
       "             5293: 2,\n",
       "             2291: 2,\n",
       "             13136: 1,\n",
       "             8515: 2,\n",
       "             5353: 2,\n",
       "             7494: 1,\n",
       "             3465: 1,\n",
       "             11297: 1,\n",
       "             12660: 1,\n",
       "             10545: 1,\n",
       "             16144: 1,\n",
       "             11846: 1,\n",
       "             4802: 3,\n",
       "             5555: 2,\n",
       "             7585: 1,\n",
       "             7166: 1,\n",
       "             2032: 1,\n",
       "             7686: 1,\n",
       "             8360: 1,\n",
       "             14086: 1,\n",
       "             4085: 1,\n",
       "             2366: 1,\n",
       "             9668: 1,\n",
       "             1127: 1,\n",
       "             4187: 1,\n",
       "             15247: 1,\n",
       "             95: 2,\n",
       "             14552: 1,\n",
       "             7672: 1,\n",
       "             9654: 1,\n",
       "             311: 1,\n",
       "             5354: 1,\n",
       "             7690: 1,\n",
       "             14396: 1,\n",
       "             9392: 1,\n",
       "             4439: 1,\n",
       "             15682: 1,\n",
       "             5601: 1,\n",
       "             9211: 1,\n",
       "             6459: 1,\n",
       "             1273: 1,\n",
       "             148: 1,\n",
       "             16197: 1,\n",
       "             12188: 1,\n",
       "             14545: 1,\n",
       "             5728: 1,\n",
       "             16301: 1,\n",
       "             13175: 1,\n",
       "             14187: 1,\n",
       "             15243: 1,\n",
       "             815: 1,\n",
       "             9655: 1,\n",
       "             487: 1})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4af8ee8-355a-481e-94ca-aae5d1f25f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f53b7d16-6258-42a0-acc1-9e3c2321a4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 feature indices for layer 0 and their average values:\n",
      "Index: 11566, Average Value: 46.200286102294925\n",
      "Index: 6210, Average Value: 34.4622917175293\n",
      "Index: 5737, Average Value: 26.257030487060547\n",
      "Index: 443, Average Value: 25.61747932434082\n",
      "Index: 8418, Average Value: 25.00115203857422\n",
      "Top 10 feature indices for layer 1 and their average values:\n",
      "Index: 14339, Average Value: 52.61330922444662\n",
      "Index: 6210, Average Value: 40.77787780761719\n",
      "Index: 9727, Average Value: 32.45850467681885\n",
      "Index: 1112, Average Value: 23.716838836669922\n",
      "Index: 1508, Average Value: 21.153118133544922\n",
      "Top 10 feature indices for layer 2 and their average values:\n",
      "Index: 9433, Average Value: 36.74750518798828\n",
      "Index: 15046, Average Value: 29.509745279947918\n",
      "Index: 1473, Average Value: 25.7916259765625\n",
      "Index: 324, Average Value: 24.814545822143554\n",
      "Index: 12653, Average Value: 22.311517906188964\n",
      "Top 10 feature indices for layer 3 and their average values:\n",
      "Index: 4793, Average Value: 38.174590301513675\n",
      "Index: 10909, Average Value: 32.39120864868164\n",
      "Index: 10538, Average Value: 25.340720494588215\n",
      "Index: 7465, Average Value: 21.02108383178711\n",
      "Index: 6210, Average Value: 17.363277435302734\n",
      "Top 10 feature indices for layer 4 and their average values:\n",
      "Index: 14260, Average Value: 41.60628128051758\n",
      "Index: 11465, Average Value: 30.917710876464845\n",
      "Index: 9433, Average Value: 29.14781951904297\n",
      "Index: 2983, Average Value: 21.2492618560791\n",
      "Index: 14307, Average Value: 17.716243743896484\n",
      "Top 10 feature indices for layer 5 and their average values:\n",
      "Index: 14260, Average Value: 32.957908630371094\n",
      "Index: 4793, Average Value: 29.707582092285158\n",
      "Index: 2841, Average Value: 23.889563242594402\n",
      "Index: 4463, Average Value: 20.049205780029297\n",
      "Index: 5971, Average Value: 19.761066436767578\n",
      "Top 10 feature indices for layer 6 and their average values:\n",
      "Index: 14525, Average Value: 28.608153978983562\n",
      "Index: 13401, Average Value: 27.663322067260744\n",
      "Index: 13506, Average Value: 21.753177642822266\n",
      "Index: 4315, Average Value: 20.728521982828777\n",
      "Index: 1227, Average Value: 17.006886800130207\n",
      "Top 10 feature indices for layer 7 and their average values:\n",
      "Index: 3186, Average Value: 38.38584518432617\n",
      "Index: 15356, Average Value: 28.452518463134766\n",
      "Index: 6751, Average Value: 27.4490966796875\n",
      "Index: 9234, Average Value: 25.660961151123047\n",
      "Index: 9182, Average Value: 23.671207427978516\n",
      "Top 10 feature indices for layer 8 and their average values:\n",
      "Index: 14801, Average Value: 41.51571083068848\n",
      "Index: 7357, Average Value: 33.20413398742676\n",
      "Index: 3178, Average Value: 31.21910858154297\n",
      "Index: 4781, Average Value: 26.748559188842773\n",
      "Index: 8217, Average Value: 23.01729393005371\n",
      "Top 10 feature indices for layer 9 and their average values:\n",
      "Index: 3517, Average Value: 36.59803263346354\n",
      "Index: 785, Average Value: 26.311880111694336\n",
      "Index: 13664, Average Value: 24.884449005126953\n",
      "Index: 13125, Average Value: 20.077545166015625\n",
      "Index: 13084, Average Value: 19.83129119873047\n",
      "Top 10 feature indices for layer 10 and their average values:\n",
      "Index: 11772, Average Value: 43.767255783081055\n",
      "Index: 13146, Average Value: 33.37391662597656\n",
      "Index: 12541, Average Value: 31.429155985514324\n",
      "Index: 8915, Average Value: 27.96062660217285\n",
      "Index: 2710, Average Value: 27.930782318115234\n",
      "Top 10 feature indices for layer 11 and their average values:\n",
      "Index: 12930, Average Value: 39.17667706807455\n",
      "Index: 892, Average Value: 39.01293754577637\n",
      "Index: 7330, Average Value: 31.008285522460938\n",
      "Index: 14117, Average Value: 30.257923762003582\n",
      "Index: 3300, Average Value: 23.18971824645996\n",
      "Top 10 feature indices for layer 12 and their average values:\n",
      "Index: 10708, Average Value: 47.934686024983726\n",
      "Index: 9467, Average Value: 29.514047622680664\n",
      "Index: 6558, Average Value: 28.683330917358397\n",
      "Index: 15437, Average Value: 28.182409286499023\n",
      "Index: 4514, Average Value: 21.887222290039062\n",
      "Top 10 feature indices for layer 13 and their average values:\n",
      "Index: 3883, Average Value: 50.04939206441244\n",
      "Index: 5472, Average Value: 45.397453943888344\n",
      "Index: 9467, Average Value: 42.304283142089844\n",
      "Index: 12931, Average Value: 38.15758209228515\n",
      "Index: 12969, Average Value: 29.625812530517578\n",
      "Top 10 feature indices for layer 14 and their average values:\n",
      "Index: 15603, Average Value: 58.318609873453774\n",
      "Index: 2884, Average Value: 42.05490417480469\n",
      "Index: 15559, Average Value: 39.046241760253906\n",
      "Index: 3211, Average Value: 32.775944519042966\n",
      "Index: 15471, Average Value: 29.37887954711914\n",
      "Top 10 feature indices for layer 15 and their average values:\n",
      "Index: 4300, Average Value: 51.74925676981608\n",
      "Index: 15892, Average Value: 42.46276702880859\n",
      "Index: 772, Average Value: 34.68086624145508\n",
      "Index: 13987, Average Value: 33.935614013671874\n",
      "Index: 14211, Average Value: 32.66618982950846\n",
      "Top 10 feature indices for layer 16 and their average values:\n",
      "Index: 7645, Average Value: 53.15576171875\n",
      "Index: 6094, Average Value: 52.22842343648275\n",
      "Index: 1281, Average Value: 51.58769098917643\n",
      "Index: 318, Average Value: 48.4905039469401\n",
      "Index: 16333, Average Value: 42.81694030761719\n",
      "Top 10 feature indices for layer 17 and their average values:\n",
      "Index: 9962, Average Value: 81.1238276163737\n",
      "Index: 6396, Average Value: 65.58076477050781\n",
      "Index: 8675, Average Value: 46.98172505696615\n",
      "Index: 14016, Average Value: 43.80245780944824\n",
      "Index: 2844, Average Value: 42.20041688283285\n",
      "Top 10 feature indices for layer 18 and their average values:\n",
      "Index: 12865, Average Value: 88.5863431294759\n",
      "Index: 11871, Average Value: 81.81711196899414\n",
      "Index: 2464, Average Value: 62.58462905883789\n",
      "Index: 623, Average Value: 48.85605239868164\n",
      "Index: 8486, Average Value: 41.510746765136716\n",
      "Top 10 feature indices for layer 19 and their average values:\n",
      "Index: 619, Average Value: 65.64395141601562\n",
      "Index: 9837, Average Value: 56.29365158081055\n",
      "Index: 15530, Average Value: 48.74580383300781\n",
      "Index: 13266, Average Value: 47.22393544514974\n",
      "Index: 6897, Average Value: 42.807013511657715\n",
      "Top 10 feature indices for layer 20 and their average values:\n",
      "Index: 3013, Average Value: 88.45135752360027\n",
      "Index: 13363, Average Value: 64.82990264892578\n",
      "Index: 13260, Average Value: 56.37356376647949\n",
      "Index: 9268, Average Value: 50.00233395894369\n",
      "Index: 10032, Average Value: 47.21528625488281\n",
      "Top 10 feature indices for layer 21 and their average values:\n",
      "Index: 13608, Average Value: 96.21867370605469\n",
      "Index: 10140, Average Value: 79.54266357421875\n",
      "Index: 3461, Average Value: 65.96608924865723\n",
      "Index: 12089, Average Value: 56.30696487426758\n",
      "Index: 5099, Average Value: 53.99234619140625\n",
      "Top 10 feature indices for layer 22 and their average values:\n",
      "Index: 8273, Average Value: 78.92124303181966\n",
      "Index: 5079, Average Value: 78.50505065917969\n",
      "Index: 8483, Average Value: 71.00369644165039\n",
      "Index: 261, Average Value: 69.07780456542969\n",
      "Index: 1072, Average Value: 68.29635492960612\n",
      "Top 10 feature indices for layer 23 and their average values:\n",
      "Index: 1541, Average Value: 181.09188588460287\n",
      "Index: 5148, Average Value: 88.39195251464844\n",
      "Index: 6895, Average Value: 81.28666687011719\n",
      "Index: 9039, Average Value: 72.08507792154948\n",
      "Index: 12660, Average Value: 65.19308471679688\n",
      "Top 10 feature indices for layer 24 and their average values:\n",
      "Index: 13752, Average Value: 127.97515869140625\n",
      "Index: 8648, Average Value: 87.73385620117188\n",
      "Index: 11687, Average Value: 80.15138626098633\n",
      "Index: 3074, Average Value: 75.44028854370117\n",
      "Index: 15615, Average Value: 75.26011505126954\n",
      "Top 10 feature indices for layer 25 and their average values:\n",
      "Index: 13749, Average Value: 202.93736012776694\n",
      "Index: 1689, Average Value: 182.919921875\n",
      "Index: 3017, Average Value: 147.8154312133789\n",
      "Index: 5553, Average Value: 135.17332458496094\n",
      "Index: 10550, Average Value: 127.17289733886719\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store top 10 features for each layer\n",
    "top_features_per_layer = {}\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    # Load the SAE for the current layer\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=\"gemma-scope-2b-pt-res-canonical\",  # <- Release name \n",
    "        sae_id=f\"layer_{layer}/width_16k/canonical\",  # <- SAE id (not always a hook point!)\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Dictionary to accumulate the sums of values for each index\n",
    "    vals_dict = defaultdict(float)\n",
    "    # Dictionary to count occurrences of each index\n",
    "    count_dict = defaultdict(int)\n",
    "\n",
    "    # Run through each prompt and accumulate the values for each index\n",
    "    for pr, ind in equal_variation.items():\n",
    "        _, cache = model.run_with_cache_with_saes(pr, saes=[sae])\n",
    "        vals, inds = torch.topk(cache[f'blocks.{layer}.hook_resid_post.hook_sae_acts_post'][0, ind, :], 15)\n",
    "\n",
    "        for val, ind in zip(vals, inds):\n",
    "            vals_dict[ind.item()] += val.item()\n",
    "            count_dict[ind.item()] += 1\n",
    "\n",
    "    # Calculate the average value for each index\n",
    "    avg_vals_dict = {ind: vals_dict[ind] / count_dict[ind] for ind in vals_dict}\n",
    "\n",
    "    # Sort the indices by their average values in descending order\n",
    "    sorted_avg_vals = sorted(avg_vals_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top 10 indices with the highest average values\n",
    "    top_10_inds = sorted_avg_vals[:5]\n",
    "\n",
    "    # Store the top 10 features for this layer\n",
    "    top_features_per_layer[layer] = top_10_inds\n",
    "\n",
    "    # Optionally, print the top 10 indices and their corresponding average values\n",
    "    print(f\"Top 10 feature indices for layer {layer} and their average values:\")\n",
    "    for ind, avg_val in top_10_inds:\n",
    "        print(f\"Index: {ind}, Average Value: {avg_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bbd64578-bc6b-43a2-9ebf-b0062b348681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Index: 14801, Average Value: 41.51571083068848\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"300\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/25-gemmascope-res-16k/14801?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc64244a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Index: 7357, Average Value: 33.20413398742676\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"300\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/25-gemmascope-res-16k/7357?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc64222b30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Index: 3178, Average Value: 31.21910858154297\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"300\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/25-gemmascope-res-16k/3178?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc64220100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Index: 4781, Average Value: 26.748559188842773\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"300\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/25-gemmascope-res-16k/4781?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc64220100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Index: 8217, Average Value: 23.01729393005371\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"300\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/25-gemmascope-res-16k/8217?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc64220100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for ind, avg_val in top_features_per_layer[8]:\n",
    "    print(f\"  Index: {ind}, Average Value: {avg_val}\")\n",
    "    html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=f\"{layer}-gemmascope-res-16k\", feature_idx=ind)\n",
    "    display(IFrame(html, width=1200, height=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "461a4c66-b898-4d8e-bf8a-06dd21902bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "Layer 1:\n",
      "Layer 2:\n",
      "Layer 3:\n",
      "Layer 4:\n",
      "Layer 5:\n",
      "Layer 6:\n",
      "Layer 7:\n",
      "Layer 8:\n",
      "  Index: 14801, Average Value: 41.51571083068848\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"300\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/8-gemmascope-res-16k/14801?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc64246350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Index: 7357, Average Value: 33.20413398742676\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"300\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/8-gemmascope-res-16k/7357?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc77a304c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Index: 3178, Average Value: 31.21910858154297\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"300\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/8-gemmascope-res-16k/3178?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc77a304c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Index: 4781, Average Value: 26.748559188842773\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"300\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/8-gemmascope-res-16k/4781?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc77a304c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Index: 8217, Average Value: 23.01729393005371\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"300\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/8-gemmascope-res-16k/8217?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc77a304c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 9:\n",
      "Layer 10:\n",
      "Layer 11:\n",
      "Layer 12:\n",
      "Layer 13:\n",
      "Layer 14:\n",
      "Layer 15:\n",
      "Layer 16:\n",
      "Layer 17:\n",
      "Layer 18:\n",
      "Layer 19:\n",
      "Layer 20:\n",
      "Layer 21:\n",
      "Layer 22:\n",
      "Layer 23:\n",
      "Layer 24:\n",
      "Layer 25:\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the dictionary and print the indices and corresponding layer\n",
    "for layer, top_features in top_features_per_layer.items():\n",
    "    print(f\"Layer {layer}:\")\n",
    "    if layer!=8:\n",
    "        continue\n",
    "    for ind, avg_val in top_features:\n",
    "        print(f\"  Index: {ind}, Average Value: {avg_val}\")\n",
    "        html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=f\"{layer}-gemmascope-res-16k\", feature_idx=ind)\n",
    "        display(IFrame(html, width=1200, height=300))\n",
    "    # if layer>20:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf256e-d5e4-4089-904f-e5f11aac0b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6740199d-32ab-4591-bcb2-e6d18a1d831a",
   "metadata": {},
   "source": [
    "# Find other equal to features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b5aa9-a7ca-4351-8d02-d66ffe5a6ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7775608-1159-429c-a411-93b299c196c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store top 10 features for each layer\n",
    "top_features_per_layer = {}\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    # Load the SAE for the current layer\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=\"gemma-scope-2b-pt-res-canonical\",  # <- Release name \n",
    "        sae_id=f\"layer_{layer}/width_16k/canonical\",  # <- SAE id (not always a hook point!)\n",
    "        device=device\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d86e205-0495-456e-b9a6-f6b18ffae27e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc49de6-4611-4cb8-99ba-a60b2f9f8d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26bad8-ff53-4e48-92be-c08c12d59e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8764e3-9144-4abf-8f01-7dbb61e8603c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "97444a57-04e8-4926-a266-fcc4042ad479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from typing import Any, Literal, NamedTuple, Callable\n",
    "\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Use the functional API with inplace=False\n",
    "# feature_acts = self.hook_sae_acts_post(F.relu(hidden_pre, inplace=False))\n",
    "\n",
    "class SaeReconstructionCache(NamedTuple):\n",
    "    sae_in: torch.Tensor\n",
    "    feature_acts: torch.Tensor\n",
    "    sae_out: torch.Tensor\n",
    "    sae_error: torch.Tensor\n",
    "\n",
    "\n",
    "def track_grad(tensor: torch.Tensor) -> None:\n",
    "    \"\"\"wrapper around requires_grad and retain_grad\"\"\"\n",
    "    tensor.requires_grad_(True)\n",
    "    tensor.retain_grad()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ApplySaesAndRunOutput:\n",
    "    model_output: torch.Tensor\n",
    "    model_activations: dict[str, torch.Tensor]\n",
    "    sae_activations: dict[str, SaeReconstructionCache]\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Helper to zero grad all tensors in this object.\"\"\"\n",
    "        self.model_output.grad = None\n",
    "        for act in self.model_activations.values():\n",
    "            act.grad = None\n",
    "        for cache in self.sae_activations.values():\n",
    "            cache.sae_in.grad = None\n",
    "            cache.feature_acts.grad = None\n",
    "            cache.sae_out.grad = None\n",
    "            cache.sae_error.grad = None\n",
    "\n",
    "\n",
    "def apply_saes_and_run(\n",
    "    model: HookedTransformer,\n",
    "    saes: dict[str, SAE],\n",
    "    input: Any,\n",
    "    include_error_term: bool = True,\n",
    "    track_model_hooks: list[str] | None = None,\n",
    "    return_type: Literal[\"logits\", \"loss\"] = \"logits\",\n",
    "    track_grads: bool = False,\n",
    ") -> ApplySaesAndRunOutput:\n",
    "    \"\"\"\n",
    "    Apply the SAEs to the model at the specific hook points, and run the model.\n",
    "    By default, this will include a SAE error term which guarantees that the SAE\n",
    "    will not affect model output. This function is designed to work correctly with\n",
    "    backprop as well, so it can be used for gradient-based feature attribution.\n",
    "\n",
    "    Args:\n",
    "        model: the model to run\n",
    "        saes: the SAEs to apply\n",
    "        input: the input to the model\n",
    "        include_error_term: whether to include the SAE error term to ensure the SAE doesn't affect model output. Default True\n",
    "        track_model_hooks: a list of hook points to record the activations and gradients. Default None\n",
    "        return_type: this is passed to the model.run_with_hooks function. Default \"logits\"\n",
    "        track_grads: whether to track gradients. Default False\n",
    "    \"\"\"\n",
    "\n",
    "    fwd_hooks = []\n",
    "    bwd_hooks = []\n",
    "\n",
    "    sae_activations: dict[str, SaeReconstructionCache] = {}\n",
    "    model_activations: dict[str, torch.Tensor] = {}\n",
    "\n",
    "    # this hook just track the SAE input, output, features, and error. If `track_grads=True`, it also ensures\n",
    "    # that requires_grad is set to True and retain_grad is called for intermediate values.\n",
    "    def reconstruction_hook(sae_in: torch.Tensor, hook: HookPoint, hook_point: str):  # noqa: ARG001\n",
    "        sae = saes[hook_point]\n",
    "#         x = sae_in.to(sae.dtype)\n",
    "#         x = sae.reshape_fn_in(x)\n",
    "#         x = sae.hook_sae_input(x)\n",
    "#         x = sae.run_time_activation_norm_fn_in(x)\n",
    "\n",
    "#         sae_in = x - (sae.b_dec * sae.cfg.apply_b_dec_to_input)\n",
    "\n",
    "#         # # \"... d_in, d_in d_sae -> ... d_sae\",\n",
    "#         hidden_pre = sae.hook_sae_acts_pre(sae_in @ sae.W_enc + sae.b_enc)\n",
    "#         feature_acts = sae.hook_sae_acts_post(F.relu(hidden_pre, inplace=False))\n",
    "        feature_acts = sae.encode(sae_in)\n",
    "        \n",
    "        \n",
    "        sae_out = sae.decode(feature_acts)\n",
    "        sae_error = (sae_in - sae_out).detach().clone()\n",
    "        if track_grads:\n",
    "            track_grad(sae_error)\n",
    "            track_grad(sae_out)\n",
    "            track_grad(feature_acts)\n",
    "            track_grad(sae_in)\n",
    "        sae_activations[hook_point] = SaeReconstructionCache(\n",
    "            sae_in=sae_in,\n",
    "            feature_acts=feature_acts,\n",
    "            sae_out=sae_out,\n",
    "            sae_error=sae_error,\n",
    "        )\n",
    "\n",
    "        if include_error_term:\n",
    "            return sae_out + sae_error\n",
    "        return sae_out\n",
    "\n",
    "    def sae_bwd_hook(output_grads: torch.Tensor, hook: HookPoint):  # noqa: ARG001\n",
    "        # this just passes the output grads to the input, so the SAE gets the same grads despite the error term hackery\n",
    "        return (output_grads,)\n",
    "\n",
    "    # this hook just records model activations, and ensures that intermediate activations have gradient tracking turned on if needed\n",
    "    def tracking_hook(hook_input: torch.Tensor, hook: HookPoint, hook_point: str):  # noqa: ARG001\n",
    "        model_activations[hook_point] = hook_input\n",
    "        if track_grads:\n",
    "            track_grad(hook_input)\n",
    "        return hook_input\n",
    "\n",
    "    for hook_point in saes.keys():\n",
    "        fwd_hooks.append(\n",
    "            (hook_point, partial(reconstruction_hook, hook_point=hook_point))\n",
    "        )\n",
    "        bwd_hooks.append((hook_point, sae_bwd_hook))\n",
    "    for hook_point in track_model_hooks or []:\n",
    "        fwd_hooks.append((hook_point, partial(tracking_hook, hook_point=hook_point)))\n",
    "\n",
    "    # now, just run the model while applying the hooks\n",
    "    with model.hooks(fwd_hooks=fwd_hooks, bwd_hooks=bwd_hooks):\n",
    "        model_output = model(input, return_type=return_type)\n",
    "\n",
    "    return ApplySaesAndRunOutput(\n",
    "        model_output=model_output,\n",
    "        model_activations=model_activations,\n",
    "        sae_activations=sae_activations,\n",
    "    )\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from typing import Any, Literal, NamedTuple\n",
    "\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "EPS = 1e-8\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "@dataclass\n",
    "class AttributionGrads:\n",
    "    metric: torch.Tensor\n",
    "    model_output: torch.Tensor\n",
    "    model_activations: dict[str, torch.Tensor]\n",
    "    sae_activations: dict[str, SaeReconstructionCache]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Attribution:\n",
    "    model_attributions: dict[str, torch.Tensor]\n",
    "    model_activations: dict[str, torch.Tensor]\n",
    "    model_grads: dict[str, torch.Tensor]\n",
    "    sae_feature_attributions: dict[str, torch.Tensor]\n",
    "    sae_feature_activations: dict[str, torch.Tensor]\n",
    "    sae_feature_grads: dict[str, torch.Tensor]\n",
    "    sae_errors_attribution_proportion: dict[str, float]\n",
    "\n",
    "\n",
    "def calculate_attribution_grads(\n",
    "    model: HookedSAETransformer,\n",
    "    prompt: str,\n",
    "    metric_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "    track_hook_points: list[str] | None = None,\n",
    "    include_saes: dict[str, SAE] | None = None,\n",
    "    return_logits: bool = True,\n",
    "    include_error_term: bool = True,\n",
    ") -> AttributionGrads:\n",
    "    \"\"\"\n",
    "    Wrapper around apply_saes_and_run that calculates gradients wrt to the metric_fn.\n",
    "    Tracks grads for both SAE feature and model neurons, and returns them in a structured format.\n",
    "    \"\"\"\n",
    "    output = apply_saes_and_run(\n",
    "        model,\n",
    "        saes=include_saes or {},\n",
    "        input=prompt,\n",
    "        return_type=\"logits\" if return_logits else \"loss\",\n",
    "        track_model_hooks=track_hook_points,\n",
    "        include_error_term=include_error_term,\n",
    "        track_grads=True,\n",
    "    )\n",
    "    metric = metric_fn(output.model_output)\n",
    "    output.zero_grad()\n",
    "    metric.backward()\n",
    "    return AttributionGrads(\n",
    "        metric=metric,\n",
    "        model_output=output.model_output,\n",
    "        model_activations=output.model_activations,\n",
    "        sae_activations=output.sae_activations,\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_feature_attribution(\n",
    "    model: HookedSAETransformer,\n",
    "    input: Any,\n",
    "    metric_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "    track_hook_points: list[str] | None = None,\n",
    "    include_saes: dict[str, SAE] | None = None,\n",
    "    return_logits: bool = True,\n",
    "    include_error_term: bool = True,\n",
    ") -> Attribution:\n",
    "    \"\"\"\n",
    "    Calculate feature attribution for SAE features and model neurons following\n",
    "    the procedure in https://transformer-circuits.pub/2024/march-update/index.html#feature-heads.\n",
    "    This include the SAE error term by default, so inserting the SAE into the calculation is\n",
    "    guaranteed to not affect the model output. This can be disabled by setting `include_error_term=False`.\n",
    "\n",
    "    Args:\n",
    "        model: The model to calculate feature attribution for.\n",
    "        input: The input to the model.\n",
    "        metric_fn: A function that takes the model output and returns a scalar metric.\n",
    "        track_hook_points: A list of model hook points to track activations for, if desired\n",
    "        include_saes: A dictionary of SAEs to include in the calculation. The key is the hook point to apply the SAE to.\n",
    "        return_logits: Whether to return the model logits or loss. This is passed to TLens, so should match whatever the metric_fn expects (probably logits)\n",
    "        include_error_term: Whether to include the SAE error term in the calculation. This is recommended, as it ensures that the SAE will not affecting the model output.\n",
    "    \"\"\"\n",
    "    # first, calculate gradients wrt to the metric_fn.\n",
    "    # these will be multiplied with the activation values to get the attributions\n",
    "    outputs_with_grads = calculate_attribution_grads(\n",
    "        model,\n",
    "        input,\n",
    "        metric_fn,\n",
    "        track_hook_points,\n",
    "        include_saes=include_saes,\n",
    "        return_logits=return_logits,\n",
    "        include_error_term=include_error_term,\n",
    "    )\n",
    "    model_attributions = {}\n",
    "    model_activations = {}\n",
    "    model_grads = {}\n",
    "    sae_feature_attributions = {}\n",
    "    sae_feature_activations = {}\n",
    "    sae_feature_grads = {}\n",
    "    sae_error_proportions = {}\n",
    "    # this code is long, but all it's doing is multiplying the grads by the activations\n",
    "    # and recording grads, acts, and attributions in dictionaries to return to the user\n",
    "    with torch.no_grad():\n",
    "        for name, act in outputs_with_grads.model_activations.items():\n",
    "            assert act.grad is not None\n",
    "            raw_activation = act.detach().clone()\n",
    "            model_attributions[name] = (act.grad * raw_activation).detach().clone()\n",
    "            model_activations[name] = raw_activation\n",
    "            model_grads[name] = act.grad.detach().clone()\n",
    "        for name, act in outputs_with_grads.sae_activations.items():\n",
    "            assert act.feature_acts.grad is not None\n",
    "            assert act.sae_out.grad is not None\n",
    "            raw_activation = act.feature_acts.detach().clone()\n",
    "            sae_feature_attributions[name] = (\n",
    "                (act.feature_acts.grad * raw_activation).detach().clone()\n",
    "            )\n",
    "            sae_feature_activations[name] = raw_activation\n",
    "            sae_feature_grads[name] = act.feature_acts.grad.detach().clone()\n",
    "            if include_error_term:\n",
    "                assert act.sae_error.grad is not None\n",
    "                error_grad_norm = act.sae_error.grad.norm().item()\n",
    "            else:\n",
    "                error_grad_norm = 0\n",
    "            sae_out_norm = act.sae_out.grad.norm().item()\n",
    "            sae_error_proportions[name] = error_grad_norm / (\n",
    "                sae_out_norm + error_grad_norm + EPS\n",
    "            )\n",
    "        return Attribution(\n",
    "            model_attributions=model_attributions,\n",
    "            model_activations=model_activations,\n",
    "            model_grads=model_grads,\n",
    "            sae_feature_attributions=sae_feature_attributions,\n",
    "            sae_feature_activations=sae_feature_activations,\n",
    "            sae_feature_grads=sae_feature_grads,\n",
    "            sae_errors_attribution_proportion=sae_error_proportions,\n",
    "        )\n",
    "        \n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=\"gemma-scope-2b-pt-res\",  # <- Release name \n",
    "        sae_id=f\"layer_8/width_16k/average_l0_71\",  # <- SAE id (not always a hook point!)\n",
    "        device=device\n",
    "    )    \n",
    "# prompt = \" Tiger Woods plays the sport of\"\n",
    "# pos_token = model.tokenizer.encode(\" golf\")[0]\n",
    "prompt = \"What is the output of 53 plus 32 ? A: \"\n",
    "pos_token = [model.tokenizer.encode(\"8\")[1]]\n",
    "neg_token = [model.tokenizer.encode(\"2\")[1]]\n",
    "# def metric_fn(logits: torch.tensor, pos_token: torch.tensor =pos_token, neg_token: torch.Tensor=neg_token) -> torch.Tensor:\n",
    "#     return logits[0,-1,pos_token] - logits[0,-1,neg_token]\n",
    "\n",
    "def metric_fn(logits: torch.Tensor, pos_token: torch.Tensor = pos_token, neg_token: torch.Tensor = neg_token) -> torch.Tensor:\n",
    "    return (logits[0, -1, pos_token] - logits[0, -1, neg_token]).sum()\n",
    "\n",
    "\n",
    "feature_attribution_df = calculate_feature_attribution(\n",
    "    input = prompt,\n",
    "    model = model,\n",
    "    metric_fn = metric_fn,\n",
    "    include_saes={sae.cfg.hook_name: sae},\n",
    "    include_error_term=True,\n",
    "    return_logits=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "62ed446e-a0fd-4c2c-9d92-a1908173bbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "x=%{x}<br>y=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "0/<bos>",
          "1/What",
          "2/ is",
          "3/ the",
          "4/ output",
          "5/ of",
          "6/ ",
          "7/5",
          "8/3",
          "9/ plus",
          "10/ ",
          "11/3",
          "12/2",
          "13/ ?",
          "14/ A",
          "15/:",
          "16/ "
         ],
         "xaxis": "x",
         "y": [
          11.2443495,
          0.046196982,
          0.029003462,
          0.026792755,
          -0.083179384,
          0.06043318,
          -0.08096224,
          -0.09092079,
          0.4119156,
          0.086970076,
          0.06739654,
          0.20100303,
          0.01926277,
          0.033405986,
          0.5578953,
          0.07969533,
          0.6484184
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"3dd6ac60-d639-41f0-a92d-28b462f6ad08\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3dd6ac60-d639-41f0-a92d-28b462f6ad08\")) {                    Plotly.newPlot(                        \"3dd6ac60-d639-41f0-a92d-28b462f6ad08\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"0\\u002f\\u003cbos\\u003e\",\"1\\u002fWhat\",\"2\\u002f is\",\"3\\u002f the\",\"4\\u002f output\",\"5\\u002f of\",\"6\\u002f \",\"7\\u002f5\",\"8\\u002f3\",\"9\\u002f plus\",\"10\\u002f \",\"11\\u002f3\",\"12\\u002f2\",\"13\\u002f ?\",\"14\\u002f A\",\"15\\u002f:\",\"16\\u002f \"],\"xaxis\":\"x\",\"y\":[11.2443495,0.046196982,0.029003462,0.026792755,-0.083179384,0.06043318,-0.08096224,-0.09092079,0.4119156,0.086970076,0.06739654,0.20100303,0.01926277,0.033405986,0.5578953,0.07969533,0.6484184],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('3dd6ac60-d639-41f0-a92d-28b462f6ad08');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the plot\n",
    "tokens = model.to_str_tokens(prompt)\n",
    "unique_tokens = [f\"{i}/{t}\" for i, t in enumerate(tokens)]\n",
    "\n",
    "fig = px.bar(x=unique_tokens,\n",
    "             y=feature_attribution_df.sae_feature_attributions[sae.cfg.hook_name][0].sum(-1).detach().cpu().numpy())\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "fig.write_image(\"feature_attribution_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "48028304-bec4-4ed8-af22-93c912c6ddf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>feature</th>\n",
       "      <th>attribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7135</th>\n",
       "      <td>16</td>\n",
       "      <td>16100</td>\n",
       "      <td>0.892943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4059</th>\n",
       "      <td>0</td>\n",
       "      <td>9213</td>\n",
       "      <td>0.333110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6267</th>\n",
       "      <td>14</td>\n",
       "      <td>14157</td>\n",
       "      <td>0.276517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4356</th>\n",
       "      <td>8</td>\n",
       "      <td>9854</td>\n",
       "      <td>0.254655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>11</td>\n",
       "      <td>6981</td>\n",
       "      <td>0.202731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5841</th>\n",
       "      <td>0</td>\n",
       "      <td>13188</td>\n",
       "      <td>0.170647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3863</th>\n",
       "      <td>12</td>\n",
       "      <td>8764</td>\n",
       "      <td>0.154779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5529</th>\n",
       "      <td>0</td>\n",
       "      <td>12472</td>\n",
       "      <td>0.153306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6083</th>\n",
       "      <td>0</td>\n",
       "      <td>13729</td>\n",
       "      <td>0.135068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4357</th>\n",
       "      <td>12</td>\n",
       "      <td>9854</td>\n",
       "      <td>0.128499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3908</th>\n",
       "      <td>16</td>\n",
       "      <td>8857</td>\n",
       "      <td>0.117475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5899</th>\n",
       "      <td>16</td>\n",
       "      <td>13307</td>\n",
       "      <td>0.107577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0</td>\n",
       "      <td>3963</td>\n",
       "      <td>0.107169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>15</td>\n",
       "      <td>5293</td>\n",
       "      <td>0.106955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4574</th>\n",
       "      <td>0</td>\n",
       "      <td>10329</td>\n",
       "      <td>0.105131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5729</th>\n",
       "      <td>0</td>\n",
       "      <td>12906</td>\n",
       "      <td>0.102904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>13</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.099704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3456</th>\n",
       "      <td>0</td>\n",
       "      <td>7848</td>\n",
       "      <td>0.096836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>15</td>\n",
       "      <td>5231</td>\n",
       "      <td>0.096128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5243</th>\n",
       "      <td>0</td>\n",
       "      <td>11817</td>\n",
       "      <td>0.094043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>0</td>\n",
       "      <td>2680</td>\n",
       "      <td>0.088105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6342</th>\n",
       "      <td>0</td>\n",
       "      <td>14314</td>\n",
       "      <td>0.087585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7204</th>\n",
       "      <td>0</td>\n",
       "      <td>16288</td>\n",
       "      <td>0.085219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>11</td>\n",
       "      <td>1465</td>\n",
       "      <td>0.084066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3420</th>\n",
       "      <td>13</td>\n",
       "      <td>7759</td>\n",
       "      <td>0.078877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>214</td>\n",
       "      <td>0.077576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3905</th>\n",
       "      <td>13</td>\n",
       "      <td>8857</td>\n",
       "      <td>0.076438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7049</th>\n",
       "      <td>0</td>\n",
       "      <td>15914</td>\n",
       "      <td>0.075622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5111</th>\n",
       "      <td>8</td>\n",
       "      <td>11487</td>\n",
       "      <td>0.074468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883</th>\n",
       "      <td>0</td>\n",
       "      <td>8813</td>\n",
       "      <td>0.073120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>15</td>\n",
       "      <td>10105</td>\n",
       "      <td>0.072549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3887</th>\n",
       "      <td>0</td>\n",
       "      <td>8827</td>\n",
       "      <td>0.070415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5513</th>\n",
       "      <td>8</td>\n",
       "      <td>12438</td>\n",
       "      <td>0.070313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7086</th>\n",
       "      <td>0</td>\n",
       "      <td>16012</td>\n",
       "      <td>0.069892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5330</th>\n",
       "      <td>14</td>\n",
       "      <td>12014</td>\n",
       "      <td>0.069250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4403</th>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>0.068381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>0</td>\n",
       "      <td>6069</td>\n",
       "      <td>0.067990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7137</th>\n",
       "      <td>0</td>\n",
       "      <td>16112</td>\n",
       "      <td>0.066227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6908</th>\n",
       "      <td>0</td>\n",
       "      <td>15586</td>\n",
       "      <td>0.065670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>0</td>\n",
       "      <td>15607</td>\n",
       "      <td>0.065225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6506</th>\n",
       "      <td>0</td>\n",
       "      <td>14712</td>\n",
       "      <td>0.064719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0.064448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>13</td>\n",
       "      <td>4087</td>\n",
       "      <td>0.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5803</th>\n",
       "      <td>0</td>\n",
       "      <td>13073</td>\n",
       "      <td>0.063466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490</th>\n",
       "      <td>0</td>\n",
       "      <td>5698</td>\n",
       "      <td>0.062696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>0</td>\n",
       "      <td>8358</td>\n",
       "      <td>0.062310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2719</th>\n",
       "      <td>0</td>\n",
       "      <td>6185</td>\n",
       "      <td>0.061677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>8</td>\n",
       "      <td>6266</td>\n",
       "      <td>0.061367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>0</td>\n",
       "      <td>1832</td>\n",
       "      <td>0.061306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.061259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      position  feature  attribution\n",
       "7135        16    16100     0.892943\n",
       "4059         0     9213     0.333110\n",
       "6267        14    14157     0.276517\n",
       "4356         8     9854     0.254655\n",
       "3056        11     6981     0.202731\n",
       "5841         0    13188     0.170647\n",
       "3863        12     8764     0.154779\n",
       "5529         0    12472     0.153306\n",
       "6083         0    13729     0.135068\n",
       "4357        12     9854     0.128499\n",
       "3908        16     8857     0.117475\n",
       "5899        16    13307     0.107577\n",
       "1717         0     3963     0.107169\n",
       "2296        15     5293     0.106955\n",
       "4574         0    10329     0.105131\n",
       "5729         0    12906     0.102904\n",
       "892         13     2024     0.099704\n",
       "3456         0     7848     0.096836\n",
       "2268        15     5231     0.096128\n",
       "5243         0    11817     0.094043\n",
       "1179         0     2680     0.088105\n",
       "6342         0    14314     0.087585\n",
       "7204         0    16288     0.085219\n",
       "642         11     1465     0.084066\n",
       "3420        13     7759     0.078877\n",
       "94           0      214     0.077576\n",
       "3905        13     8857     0.076438\n",
       "7049         0    15914     0.075622\n",
       "5111         8    11487     0.074468\n",
       "3883         0     8813     0.073120\n",
       "4479        15    10105     0.072549\n",
       "3887         0     8827     0.070415\n",
       "5513         8    12438     0.070313\n",
       "7086         0    16012     0.069892\n",
       "5330        14    12014     0.069250\n",
       "4403         0     9924     0.068381\n",
       "2668         0     6069     0.067990\n",
       "7137         0    16112     0.066227\n",
       "6908         0    15586     0.065670\n",
       "6917         0    15607     0.065225\n",
       "6506         0    14712     0.064719\n",
       "18           0       43     0.064448\n",
       "1773        13     4087     0.064200\n",
       "5803         0    13073     0.063466\n",
       "2490         0     5698     0.062696\n",
       "3675         0     8358     0.062310\n",
       "2719         0     6185     0.061677\n",
       "2750         8     6266     0.061367\n",
       "801          0     1832     0.061306\n",
       "3            0        4     0.061259"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_sparse_feature_to_long_df(sparse_tensor: torch.Tensor) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a sparse tensor to a long format pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(sparse_tensor.detach().cpu().numpy())\n",
    "    df_long = df.melt(ignore_index=False, var_name='column', value_name='value')\n",
    "    df_long.columns = [\"feature\", \"attribution\"]\n",
    "    df_long_nonzero = df_long[df_long['attribution'] != 0]\n",
    "    df_long_nonzero = df_long_nonzero.reset_index().rename(columns={'index': 'position'})\n",
    "    return df_long_nonzero\n",
    "\n",
    "df_long_nonzero = convert_sparse_feature_to_long_df(feature_attribution_df.sae_feature_attributions[sae.cfg.hook_name][0])\n",
    "df_long_nonzero.sort_values(\"attribution\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "734bf30c-297f-4af8-8b1d-db62b34c3f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos 13, Feature 2024 had a total attribution of 0.10\n",
      "Pos 13, Feature 7759 had a total attribution of 0.08\n",
      "Pos 13, Feature 8857 had a total attribution of 0.08\n",
      "Pos 13, Feature 4087 had a total attribution of 0.06\n",
      "Pos 13, Feature 6061 had a total attribution of 0.03\n",
      "Pos 13, Feature 12684 had a total attribution of 0.02\n",
      "Pos 13, Feature 5555 had a total attribution of 0.02\n",
      "Pos 13, Feature 12672 had a total attribution of 0.02\n",
      "Pos 13, Feature 5001 had a total attribution of 0.02\n",
      "Pos 13, Feature 8305 had a total attribution of 0.01\n",
      "Pos 13, Feature 9392 had a total attribution of 0.01\n",
      "Pos 13, Feature 5601 had a total attribution of 0.01\n",
      "Pos 13, Feature 4646 had a total attribution of 0.01\n",
      "Pos 13, Feature 15075 had a total attribution of 0.01\n",
      "Pos 13, Feature 527 had a total attribution of 0.01\n",
      "Pos 13, Feature 9561 had a total attribution of 0.01\n",
      "Pos 13, Feature 324 had a total attribution of 0.01\n",
      "Pos 13, Feature 4781 had a total attribution of 0.01\n",
      "Pos 13, Feature 10350 had a total attribution of 0.01\n",
      "Pos 13, Feature 14993 had a total attribution of 0.01\n"
     ]
    }
   ],
   "source": [
    "for pos in [13]: #range(6, 17):\n",
    "    for i, v in df_long_nonzero.query(f\"position=={pos}\").groupby(\"feature\").attribution.sum().sort_values(ascending=False).head(20).items():\n",
    "        print(f\"Pos {pos}, Feature {i} had a total attribution of {v:.2f}\")\n",
    "        # html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"8-gemmascope-res-16k\", feature_idx=int(i))\n",
    "        # display(IFrame(html, width=1200, height=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1a3df-9e72-4695-aaf5-aa18055106cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e9f17-bd48-44c3-beb4-f44ff913d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer = 8\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=\"gemma-scope-2b-pt-res\",  # <- Release name \n",
    "        sae_id=f\"layer_8/width_16k/average_l0_71\",  # <- SAE id (not always a hook point!)\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a6d35-e4f6-49d2-a725-6f9a22ac700c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff15d66-ac43-4261-bee7-dc4c91d9093f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80224aa4-6752-4756-a3e8-4b65707b7b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206b3adb-c520-4d9f-b092-8b26079cab0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93b080-ad47-47c7-93c0-255eb8c47514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e4c756-cc5d-4d5b-98af-f714c220f542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938e265-4813-4c5e-bea8-5ff7cc5fe209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc7f80-efce-47cc-a726-239ae38a8606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a908ee-e61f-4c69-ac04-938f2438a5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-finetuning]",
   "language": "python",
   "name": "conda-env-.conda-finetuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
