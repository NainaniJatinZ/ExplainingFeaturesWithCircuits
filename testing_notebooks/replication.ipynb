{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ecc51b-681b-42ab-98a8-20968a8450d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jnainani_umass_edu/ExplainingFeaturesWithCircuits\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jnainani_umass_edu/ExplainingFeaturesWithCircuits/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f3f113a-16b5-4646-82b0-fe02e49894a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b480d1904d4d72b6f9cd5c51d49d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n",
      "First pair ('What is the output of 64 plus 71 ? ', 'What is the output of 64 and 71 ? ')\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "# from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.ActivationCache import ActivationCache\n",
    "from transformer_lens.HookedTransformer import HookedTransformer\n",
    "import itertools\n",
    "from functools import partial\n",
    "from typing import Callable, Optional, Sequence, Tuple, Union, overload\n",
    "import einops\n",
    "import pandas as pd\n",
    "import torch\n",
    "from jaxtyping import Float, Int\n",
    "from tqdm.auto import tqdm\n",
    "from typing_extensions import Literal\n",
    "import types\n",
    "import json\n",
    "from transformer_lens.utils import Slice, SliceInput\n",
    "import functools\n",
    "from neel_plotly import line, imshow, scatter\n",
    "import f_utils\n",
    "import argparse  # Import argparse to handle command-line arguments\n",
    "\n",
    "# Set up an argument parser to take the example number as input\n",
    "# parser = argparse.ArgumentParser(description=\"number of example to run\")\n",
    "# parser.add_argument(\"--example_number\", type=int, required=True, help=\"The example number to use.\")\n",
    "# args = parser.parse_args()\n",
    "example_number = 0 #args.example_number\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"<hf token here>\"\n",
    "model = HookedSAETransformer.from_pretrained(\"google/gemma-2-2b\", device = device)\n",
    "\n",
    "\n",
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res\", # <- Release name \n",
    "    sae_id = \"layer_8/width_16k/average_l0_71\", # <- SAE id (not always a hook point!)\n",
    "    device = device\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "N = 100  # Number of pairs to generate\n",
    "dataset = f_utils.generate_dataset(N, example_number)\n",
    "print(\"First pair\", dataset[0])\n",
    "clean_pr = []\n",
    "corr_pr = []\n",
    "for i, (clean, corrupted) in enumerate(dataset):\n",
    "    clean_pr.append(clean)\n",
    "    corr_pr.append(corrupted)\n",
    "    \n",
    "# clean_tokens = model.to_tokens(clean_pr)\n",
    "# corrupted_tokens = model.to_tokens(corr_pr)\n",
    "\n",
    "# _, clean_cache = model.run_with_cache(clean_tokens)\n",
    "\n",
    "def equal_feature_metric(cache):\n",
    "    sae_in = cache[sae.cfg.hook_name]\n",
    "    feature_acts = sae.encode(sae_in)\n",
    "    # print(feature_acts.shape)\n",
    "    feature_acts = feature_acts.squeeze()\n",
    "    return feature_acts[:, :, 15191][-2:].sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d7d274-bc39-4062-b9be-67c9cbe4f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_cache_with_extra_hook(\n",
    "    self,\n",
    "    *model_args: Any,\n",
    "    current_activation_name: str,\n",
    "    current_hook: Any,\n",
    "    names_filter: NamesFilter = None,\n",
    "    device: DeviceType = None,\n",
    "    remove_batch_dim: bool = False,\n",
    "    incl_bwd: bool = False,\n",
    "    reset_hooks_end: bool = True,\n",
    "    clear_contexts: bool = False,\n",
    "    pos_slice: Optional[Union[Slice, SliceInput]] = None,\n",
    "    **model_kwargs: Any,\n",
    "):\n",
    "\n",
    "    pos_slice = Slice.unwrap(pos_slice)\n",
    "\n",
    "    # Get the caching hooks\n",
    "    cache_dict, fwd, bwd = self.get_caching_hooks(\n",
    "        names_filter,\n",
    "        incl_bwd,\n",
    "        device,\n",
    "        remove_batch_dim=remove_batch_dim,\n",
    "        pos_slice=pos_slice,\n",
    "    )\n",
    "\n",
    "    # Add the extra forward hook\n",
    "    fwd_hooks = [(current_activation_name, current_hook)] + fwd\n",
    "\n",
    "    # Run the model with the hooks\n",
    "    with self.hooks(\n",
    "        fwd_hooks=fwd_hooks,\n",
    "        bwd_hooks=bwd,\n",
    "        reset_hooks_end=reset_hooks_end,\n",
    "        clear_contexts=clear_contexts,\n",
    "    ):\n",
    "        model_out = self(*model_args, **model_kwargs)\n",
    "        if incl_bwd:\n",
    "            model_out.backward()\n",
    "\n",
    "    return model_out, cache_dict\n",
    "\n",
    "def generic_activation_patch(\n",
    "    model: HookedTransformer,\n",
    "    corrupted_tokens: Int[torch.Tensor, \"batch pos\"],\n",
    "    clean_cache: ActivationCache,\n",
    "    patching_metric: Callable[[Float[torch.Tensor, \"batch pos d_vocab\"]], Float[torch.Tensor, \"\"]],\n",
    "    patch_setter: Callable[\n",
    "        [CorruptedActivation, Sequence[int], ActivationCache], PatchedActivation\n",
    "    ],\n",
    "    activation_name: str,\n",
    "    index_axis_names: Optional[Sequence[AxisNames]] = None,\n",
    "    index_df: Optional[pd.DataFrame] = None,\n",
    "    return_index_df: bool = False,\n",
    ") -> Union[torch.Tensor, Tuple[torch.Tensor, pd.DataFrame]]:\n",
    "\n",
    "\n",
    "    if index_df is None:\n",
    "        assert index_axis_names is not None\n",
    "\n",
    "        # Get the max range for all possible axes\n",
    "        max_axis_range = {\n",
    "            \"layer\": model.cfg.n_layers,\n",
    "            \"pos\": corrupted_tokens.shape[-1],\n",
    "            \"head_index\": model.cfg.n_heads,\n",
    "        }\n",
    "        max_axis_range[\"src_pos\"] = max_axis_range[\"pos\"]\n",
    "        max_axis_range[\"dest_pos\"] = max_axis_range[\"pos\"]\n",
    "        max_axis_range[\"head\"] = max_axis_range[\"head_index\"]\n",
    "\n",
    "        # Get the max range for each axis we iterate over\n",
    "        index_axis_max_range = [max_axis_range[axis_name] for axis_name in index_axis_names]\n",
    "\n",
    "        # Get the dataframe where each row is a tuple of indices\n",
    "        index_df = transformer_lens.patching.make_df_from_ranges(index_axis_max_range, index_axis_names)\n",
    "\n",
    "        flattened_output = False\n",
    "    else:\n",
    "        # A dataframe of indices was provided. Verify that we did not *also* receive index_axis_names\n",
    "        assert index_axis_names is None\n",
    "        index_axis_max_range = index_df.max().to_list()\n",
    "\n",
    "        flattened_output = True\n",
    "\n",
    "    # Create an empty tensor to show the patched metric for each patch\n",
    "    if flattened_output:\n",
    "        patched_metric_output = torch.zeros(len(index_df), device=model.cfg.device)\n",
    "    else:\n",
    "        patched_metric_output = torch.zeros(index_axis_max_range, device=model.cfg.device)\n",
    "\n",
    "    # A generic patching hook - for each index, it applies the patch_setter appropriately to patch the activation\n",
    "    def patching_hook(corrupted_activation, hook, index, clean_activation):\n",
    "        return patch_setter(corrupted_activation, index, clean_activation)\n",
    "\n",
    "    # Iterate over every list of indices, and make the appropriate patch!\n",
    "    for c, index_row in enumerate(tqdm((list(index_df.iterrows())))):\n",
    "        index = index_row[1].to_list()\n",
    "\n",
    "        # The current activation name is just the activation name plus the layer (assumed to be the first element of the input)\n",
    "        current_activation_name = utils.get_act_name(activation_name, layer=index[0])\n",
    "\n",
    "        # The hook function cannot receive additional inputs, so we use partial to include the specific index and the corresponding clean activation\n",
    "        current_hook = partial(\n",
    "            patching_hook,\n",
    "            index=index,\n",
    "            clean_activation=clean_cache[current_activation_name],\n",
    "        )\n",
    "        \n",
    "#         incl_bwd = False\n",
    "#         cache_dict, fwd, bwd = model.get_caching_hooks(\n",
    "#             incl_bwd=incl_bwd,\n",
    "#             device=device,\n",
    "#             names_filter=None\n",
    "#         )\n",
    "        \n",
    "#         fwd_hooks = [(current_activation_name, current_hook)] + fwd\n",
    "        # Run the model with the patching hook and get the logits!\n",
    "        # patched_logits, patched_cache = \"\", \"\"\n",
    "        \n",
    "        patched_logits, patched_cache = model.run_with_cache_with_extra_hook(\n",
    "            corrupted_tokens, \n",
    "            current_activation_name=current_activation_name, \n",
    "            current_hook= current_hook\n",
    "        )\n",
    "        # print(patched_cache.keys())\n",
    "        # print(patched_logits.shape)\n",
    "\n",
    "        # Calculate the patching metric and store\n",
    "        if flattened_output:\n",
    "            patched_metric_output[c] = patching_metric(patched_cache).item()\n",
    "        else:\n",
    "            patched_metric_output[tuple(index)] = patching_metric(patched_cache).item()\n",
    "\n",
    "    if return_index_df:\n",
    "        return patched_metric_output, index_df\n",
    "    else:\n",
    "        return patched_metric_output\n",
    "    \n",
    "def layer_pos_head_vector_patch_setter(\n",
    "    corrupted_activation,\n",
    "    index,\n",
    "    clean_activation,\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies the activation patch where index = [layer, pos, head_index]\n",
    "\n",
    "    Implicitly assumes that the activation axis order is [batch, pos, head_index, ...], which is true of all attention head vector activations (q, k, v, z, result) but *not* of attention patterns.\n",
    "    \"\"\"\n",
    "    assert len(index) == 3\n",
    "    layer, pos, head_index = index\n",
    "    corrupted_activation[:, pos, head_index] = clean_activation[:, pos, head_index]\n",
    "    return corrupted_activation\n",
    "\n",
    "get_act_patch_attn_head_out_by_pos = partial(\n",
    "    generic_activation_patch,\n",
    "    patch_setter=layer_pos_head_vector_patch_setter,\n",
    "    activation_name=\"z\",\n",
    "    index_axis_names=(\"layer\", \"pos\", \"head\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def layer_pos_patch_setter(corrupted_activation, index, clean_activation):\n",
    "    \"\"\"\n",
    "    Applies the activation patch where index = [layer, pos]\n",
    "\n",
    "    Implicitly assumes that the activation axis order is [batch, pos, ...], which is true of everything that is not an attention pattern shaped tensor.\n",
    "    \"\"\"\n",
    "    assert len(index) == 2\n",
    "    layer, pos = index\n",
    "    corrupted_activation[:, pos, ...] = clean_activation[:, pos, ...]\n",
    "    return corrupted_activation\n",
    "    \n",
    "get_act_patch_resid_pre = partial(\n",
    "    generic_activation_patch,\n",
    "    patch_setter=layer_pos_patch_setter,\n",
    "    activation_name=\"resid_pre\",\n",
    "    index_axis_names=(\"layer\", \"pos\"),\n",
    ")\n",
    "\n",
    "def layer_head_vector_patch_setter(\n",
    "    corrupted_activation,\n",
    "    index,\n",
    "    clean_activation,\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies the activation patch where index = [layer,  head_index]\n",
    "\n",
    "    Implicitly assumes that the activation axis order is [batch, pos, head_index, ...], which is true of all attention head vector activations (q, k, v, z, result) but *not* of attention patterns.\n",
    "    \"\"\"\n",
    "    assert len(index) == 2\n",
    "    layer, head_index = index\n",
    "    corrupted_activation[:, :, head_index] = clean_activation[:, :, head_index]\n",
    "\n",
    "    return corrupted_activation\n",
    "\n",
    "get_act_patch_attn_head_out_all_pos = partial(\n",
    "    generic_activation_patch,\n",
    "    patch_setter=layer_head_vector_patch_setter,\n",
    "    activation_name=\"z\",\n",
    "    index_axis_names=(\"layer\", \"head\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "861a7f9c-3ada-4db5-98b1-d9e301b63f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the new method to the model instance\n",
    "model.run_with_cache_with_extra_hook = types.MethodType(run_with_cache_with_extra_hook, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74627ca9-f1cb-4e27-a3db-283e195f4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = model.to_tokens(clean_pr)\n",
    "corrupted_tokens = model.to_tokens(corr_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76729183-793f-4e2b-99a7-f75a47751381",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, clean_cache = model.run_with_cache(clean_tokens)\n",
    "_, corrupted_cache = model.run_with_cache(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64e9a925-55db-4577-9804-f3d4fa927a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa223c433b449a6a72cbb2a35bd5c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resid_pre_act_patch_results = get_act_patch_resid_pre(model, corrupted_tokens, clean_cache, equal_feature_metric)\n",
    "fig = imshow(\n",
    "    resid_pre_act_patch_results, \n",
    "    yaxis=\"Layer\", \n",
    "    xaxis=\"Position\", \n",
    "    x=[f\"{tok} {i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))],\n",
    "    title=\"resid_pre Activation Patching\",\n",
    "    return_fig=True  # This ensures the figure object is returned\n",
    ")\n",
    "\n",
    "fig.write_image(f\"results/example{example_number}_1/resid_pre_activation_patching.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56de8fff-ddca-4483-be0a-d23dd72e2305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56bafa642844c679adfaf102e6c1f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn_head_out_all_pos_act_patch_results = get_act_patch_attn_head_out_all_pos(model, corrupted_tokens, clean_cache, equal_feature_metric)\n",
    "fig = imshow(attn_head_out_all_pos_act_patch_results,  \n",
    "       yaxis=\"Layer\", \n",
    "       xaxis=\"Head\", \n",
    "       title=\"attn_head_out Activation Patching (All Pos)\", \n",
    "        return_fig=True)\n",
    "\n",
    "fig.write_image(f\"results/example{example_number}_1/attn_head_out Activation Patching All Pos.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acbc311a-1292-40fa-be73-d3775a513cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da42f0d5b6284ffd9b2b6996a61eff72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DO_SLOW_RUNS = True\n",
    "ALL_HEAD_LABELS = [f\"L{i}H{j}\" for i in range(model.cfg.n_layers) for j in range(model.cfg.n_heads)]\n",
    "if DO_SLOW_RUNS:\n",
    "    attn_head_out_act_patch_results = f_utils.get_act_patch_attn_head_out_by_pos(model, corrupted_tokens, clean_cache, equal_feature_metric)\n",
    "    attn_head_out_act_patch_results = einops.rearrange(attn_head_out_act_patch_results, \"layer pos head -> (layer head) pos\")\n",
    "    fig = imshow(attn_head_out_act_patch_results, \n",
    "        yaxis=\"Head Label\", \n",
    "        xaxis=\"Pos\", \n",
    "        x=[f\"{tok} {i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))],\n",
    "        y=ALL_HEAD_LABELS,\n",
    "        title=\"attn_head_out Activation Patching By Pos\", \n",
    "        return_fig=True)\n",
    "    fig.write_image(f\"results/example{example_number}_1/attn_head_out_act_patch_results.png\")\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "007c6d4e-a463-43c2-8f4b-ec526b03ab76",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Assuming attn_head_out_act_patch_results is your tensor\n",
    "sliced_results = attn_head_out_act_patch_results[:72, :]\n",
    "# Adjust the y-axis labels for the first 72 elements\n",
    "sliced_y_labels = ALL_HEAD_LABELS[:72]\n",
    "\n",
    "# Adjust the x-axis labels for the last 7 positions\n",
    "sliced_x_labels = [f\"{tok} {i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))] #[-7:]\n",
    "\n",
    "fig = imshow(\n",
    "    sliced_results, \n",
    "    yaxis=\"Head Label\", \n",
    "    xaxis=\"Pos\", \n",
    "    x=sliced_x_labels,\n",
    "    y=sliced_y_labels,\n",
    "    title=\"attn_head_out Activation Patching By Pos\", \n",
    "    width=1000,  # Increase the width of the figure\n",
    "    height=1200,  # Increase the height of the figure\n",
    "    return_fig=True\n",
    ")\n",
    "\n",
    "# Optionally, you can adjust the tickfont size for better readability\n",
    "fig.update_layout(\n",
    "    yaxis=dict(tickfont=dict(size=10)),  # Adjust the size as needed\n",
    "    xaxis=dict(tickfont=dict(size=10))   # Adjust the size as needed\n",
    ")\n",
    "\n",
    "# Save the figure\n",
    "fig.write_image(f\"results/example{example_number}_1/attn_head_out_act_patch_results_sliced.png\")\n",
    "\n",
    "\n",
    "# Assuming sliced_results is your sliced tensor\n",
    "mean_value = sliced_results.mean().item()\n",
    "std_dev = sliced_results.std().item()\n",
    "\n",
    "# Calculate the threshold for one standard deviation away from the mean\n",
    "lower_threshold = mean_value - std_dev\n",
    "upper_threshold = mean_value + std_dev\n",
    "\n",
    "# Identify the indices where the values are one standard deviation away from the mean\n",
    "indices = (sliced_results < lower_threshold) | (sliced_results > upper_threshold)\n",
    "y_indices, x_indices = torch.where(indices)\n",
    "\n",
    "# Extract the corresponding y labels, x labels, and values\n",
    "tuples_list = [\n",
    "    (sliced_y_labels[y_idx], sliced_x_labels[x_idx], sliced_results[y_idx, x_idx].item())\n",
    "    for y_idx, x_idx in zip(y_indices, x_indices)\n",
    "]\n",
    "\n",
    "# Convert the first element of each tuple in the list\n",
    "converted_tuples = [f_utils.convert_to_tuple(item[0]) for item in tuples_list] #[(convert_to_tuple(item[0]), item[1], item[2]) for item in tuples_list]\n",
    "\n",
    "output_path = f\"results/example{example_number}/converted_tuples.json\"\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(converted_tuples, f)\n",
    "\n",
    "f_utils.save_relevant_attention_patterns(clean_cache, converted_tuples, example_number, model.to_str_tokens(clean_tokens[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3f141-8505-4b82-912f-a7ca2cf6688f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-finetuning]",
   "language": "python",
   "name": "conda-env-.conda-finetuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
