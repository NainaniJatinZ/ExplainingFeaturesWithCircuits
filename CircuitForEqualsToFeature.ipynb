{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cdfa9f9-f155-41c9-96c9-39ae86660659",
   "metadata": {},
   "source": [
    "# Imports and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aafd3c-0b36-410d-a5a1-f4f35ecd175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.ActivationCache import ActivationCache\n",
    "from transformer_lens.HookedTransformer import HookedTransformer\n",
    "from neel_plotly import line, imshow, scatter\n",
    "import itertools\n",
    "from functools import partial\n",
    "from typing import Callable, Optional, Sequence, Tuple, Union, overload\n",
    "\n",
    "import einops\n",
    "import pandas as pd\n",
    "import torch\n",
    "from jaxtyping import Float, Int\n",
    "from tqdm.auto import tqdm\n",
    "from typing_extensions import Literal\n",
    "\n",
    "import types\n",
    "from transformer_lens.utils import Slice, SliceInput\n",
    "\n",
    "import functools\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "\n",
    "# Imports for displaying vis in Colab / notebook\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# For the most part I'll try to import functions and classes near where they are used\n",
    "# to make it clear where they come from.\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "# from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb881d37-8dfa-48de-930d-eb51a07a6063",
   "metadata": {},
   "source": [
    "# Loading gemma 2 2b and sae I'm interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e339e-3caa-4250-a709-7ad31203df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"<hf token here>\"\n",
    "model = HookedSAETransformer.from_pretrained(\"google/gemma-2-2b\", device = device)\n",
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res\", # <- Release name \n",
    "    sae_id = \"layer_8/width_16k/average_l0_71\", # <- SAE id (not always a hook point!)\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e9530-2f5f-40cf-b2e3-bc42512afe94",
   "metadata": {},
   "source": [
    "# Investigating and exploring the Abstract feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6e548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "def get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"8-gemmascope-res-16k\", feature_idx=0):\n",
    "    return html_template.format(sae_release, sae_id, feature_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b1546-d8b4-4767-bfd7-d407bbd00552",
   "metadata": {},
   "source": [
    "## I discovered the feature when investigating arithmetic equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb50ee-f77c-4be6-98c2-1751367736b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c1777d7-73cd-4a34-881f-5c86c9d16fbf",
   "metadata": {},
   "source": [
    "## If you just look at the top activating logits, its just \"=\" tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac634bc-552f-429a-b5d5-6abe6cf0b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"8-gemmascope-res-16k\", feature_idx=int(15191))\n",
    "display(IFrame(html, width=1200, height=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b662dfab-7310-4a34-aad6-e0a0c35af9e6",
   "metadata": {},
   "source": [
    "## But that's not it, when I played with it, I found that it activates on equations that dont have the = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde9078-c479-4a36-be30-0e69fa4d0848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "400eafe6-dc5d-418e-a87f-ca25f8701559",
   "metadata": {},
   "source": [
    "## Specifically, it needs both operators and operands in the equation to activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0bf81d-e210-4eec-aa40-1b9bb60fa8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2de0611f-df67-4a95-a2fa-c1be091b2be7",
   "metadata": {},
   "source": [
    "# Generating Clean and corrupted samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277285b-cc5e-42eb-83ef-054cd664542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_example_pair():\n",
    "    # Generate two random numbers between 1 and 3 digits\n",
    "    num1 = random.randint(10, 99)\n",
    "    num2 = random.randint(10, 99)\n",
    "    \n",
    "    # Create clean and corrupted examples\n",
    "    clean_example = f'What is the output of {num1} plus {num2} ? '\n",
    "    corrupted_example = f'What is the output of {num1} and {num2} ? '\n",
    "    \n",
    "    return clean_example, corrupted_example\n",
    "\n",
    "def generate_dataset(N):\n",
    "    dataset = []\n",
    "    for _ in range(N):\n",
    "        clean, corrupted = generate_example_pair()\n",
    "        dataset.append((clean, corrupted))\n",
    "    return dataset\n",
    "\n",
    "# Example usage\n",
    "N = 100  # Number of pairs to generate\n",
    "dataset = generate_dataset(N)\n",
    "\n",
    "# Print the dataset\n",
    "for i, (clean, corrupted) in enumerate(dataset):\n",
    "    print(f\"Pair {i+1}:\")\n",
    "    print(f\"  Clean:     {clean}\")\n",
    "    print(f\"  Corrupted: {corrupted}\")\n",
    "    print()\n",
    "    if i>10:\n",
    "        break\n",
    "        \n",
    "clean_pr = []\n",
    "corr_pr = []\n",
    "for i, (clean, corrupted) in enumerate(dataset):\n",
    "    clean_pr.append(clean)\n",
    "    corr_pr.append(corrupted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df5a37-a5ee-4f34-82c4-82a4b97e53e4",
   "metadata": {},
   "source": [
    "## Testing feature activation difference between clean and corrupted prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e213b-5d80-4062-9069-b3a52e714023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_till_feature(prompt):\n",
    "    _, cache = model.run_with_cache(\n",
    "            prompt, \n",
    "            stop_at_layer=sae.cfg.hook_layer + 1, \n",
    "            names_filter=[sae.cfg.hook_name])\n",
    "    sae_in = cache[sae.cfg.hook_name]\n",
    "    feature_acts = sae.encode(sae_in).squeeze()\n",
    "    return feature_acts[:, 15191][-2:].sum()\n",
    "\n",
    "for i, (clean, corrupted) in enumerate(dataset):\n",
    "    print(\"Clean: \", run_model_till_feature(clean))\n",
    "    print(\"Corrupted: \", run_model_till_feature(corrupted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4dda93-8be3-4108-81a7-bd7fa226c73a",
   "metadata": {},
   "source": [
    "# Run with hooks and cache\n",
    "\n",
    "For normal patching experiments, the metric is something like the logit difference. In this case, the metric is the activation of a feature. \n",
    "\n",
    "As the goal is to find a circuit for a feature, I only care about the components before that feature.\n",
    "\n",
    "I edited the run_with_cache method to allow addition of an extra hook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f668b63-8272-4edc-a53e-4b05abe96f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def run_with_cache_with_extra_hook(\n",
    "    self,\n",
    "    *model_args: Any,\n",
    "    current_activation_name: str,\n",
    "    current_hook: Any,\n",
    "    names_filter: NamesFilter = None,\n",
    "    device: DeviceType = None,\n",
    "    remove_batch_dim: bool = False,\n",
    "    incl_bwd: bool = False,\n",
    "    reset_hooks_end: bool = True,\n",
    "    clear_contexts: bool = False,\n",
    "    pos_slice: Optional[Union[Slice, SliceInput]] = None,\n",
    "    **model_kwargs: Any,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the model and returns the model output and a Cache object.\n",
    "    \n",
    "    Adds an extra forward hook (current_activation_name, current_hook) to the hooks.\n",
    "\n",
    "    Args:\n",
    "        *model_args: Positional arguments for the model.\n",
    "        current_activation_name: The name of the activation to hook.\n",
    "        current_hook: The hook function to use.\n",
    "        names_filter (NamesFilter, optional): A filter for which activations to cache.\n",
    "        device (str or torch.Device, optional): The device to cache activations on.\n",
    "        remove_batch_dim (bool, optional): If True, removes the batch dimension when caching.\n",
    "        incl_bwd (bool, optional): If True, caches gradients as well.\n",
    "        reset_hooks_end (bool, optional): If True, removes all hooks added by this function.\n",
    "        clear_contexts (bool, optional): If True, clears hook contexts whenever hooks are reset.\n",
    "        pos_slice: The slice to apply to the cache output. Defaults to None.\n",
    "        **model_kwargs: Keyword arguments for the model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the model output and a Cache object.\n",
    "    \"\"\"\n",
    "\n",
    "    pos_slice = Slice.unwrap(pos_slice)\n",
    "\n",
    "    # Get the caching hooks\n",
    "    cache_dict, fwd, bwd = self.get_caching_hooks(\n",
    "        names_filter,\n",
    "        incl_bwd,\n",
    "        device,\n",
    "        remove_batch_dim=remove_batch_dim,\n",
    "        pos_slice=pos_slice,\n",
    "    )\n",
    "\n",
    "    # Add the extra forward hook\n",
    "    fwd_hooks = [(current_activation_name, current_hook)] + fwd\n",
    "\n",
    "    # Run the model with the hooks\n",
    "    with self.hooks(\n",
    "        fwd_hooks=fwd_hooks,\n",
    "        bwd_hooks=bwd,\n",
    "        reset_hooks_end=reset_hooks_end,\n",
    "        clear_contexts=clear_contexts,\n",
    "    ):\n",
    "        model_out = self(*model_args, **model_kwargs)\n",
    "        if incl_bwd:\n",
    "            model_out.backward()\n",
    "\n",
    "    return model_out, cache_dict\n",
    "\n",
    "\n",
    "\n",
    "# Attach the new method to the model instance\n",
    "model.run_with_cache_with_extra_hook = types.MethodType(run_with_cache_with_extra_hook, model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generic_activation_patch(\n",
    "    model: HookedTransformer,\n",
    "    corrupted_tokens: Int[torch.Tensor, \"batch pos\"],\n",
    "    clean_cache: ActivationCache,\n",
    "    patching_metric: Callable[[Float[torch.Tensor, \"batch pos d_vocab\"]], Float[torch.Tensor, \"\"]],\n",
    "    patch_setter: Callable[\n",
    "        [CorruptedActivation, Sequence[int], ActivationCache], PatchedActivation\n",
    "    ],\n",
    "    activation_name: str,\n",
    "    index_axis_names: Optional[Sequence[AxisNames]] = None,\n",
    "    index_df: Optional[pd.DataFrame] = None,\n",
    "    return_index_df: bool = False,\n",
    ") -> Union[torch.Tensor, Tuple[torch.Tensor, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    A generic function to do activation patching, will be specialised to specific use cases.\n",
    "\n",
    "    Activation patching is about studying the counterfactual effect of a specific activation between a clean run and a corrupted run. The idea is have two inputs, clean and corrupted, which have two different outputs, and differ in some key detail. Eg \"The Eiffel Tower is in\" vs \"The Colosseum is in\". Then to take a cached set of activations from the \"clean\" run, and a set of corrupted.\n",
    "\n",
    "    Internally, the key function comes from three things: A list of tuples of indices (eg (layer, position, head_index)), a index_to_act_name function which identifies the right activation for each index, a patch_setter function which takes the corrupted activation, the index and the clean cache, and a metric for how well the patched model has recovered.\n",
    "\n",
    "    The indices can either be given explicitly as a pandas dataframe, or by listing the relevant axis names and having them inferred from the tokens and the model config. It is assumed that the first column is always layer.\n",
    "\n",
    "    This function then iterates over every tuple of indices, does the relevant patch, and stores it\n",
    "\n",
    "    Args:\n",
    "        model: The relevant model\n",
    "        corrupted_tokens: The input tokens for the corrupted run\n",
    "        clean_cache: The cached activations from the clean run\n",
    "        patching_metric: A function from the model's output logits to some metric (eg loss, logit diff, etc)\n",
    "        patch_setter: A function which acts on (corrupted_activation, index, clean_cache) to edit the activation and patch in the relevant chunk of the clean activation\n",
    "        activation_name: The name of the activation being patched\n",
    "        index_axis_names: The names of the axes to (fully) iterate over, implicitly fills in index_df\n",
    "        index_df: The dataframe of indices, columns are axis names and each row is a tuple of indices. Will be inferred from index_axis_names if not given. When this is input, the output will be a flattened tensor with an element per row of index_df\n",
    "        return_index_df: A Boolean flag for whether to return the dataframe of indices too\n",
    "\n",
    "    Returns:\n",
    "        patched_output: The tensor of the patching metric for each patch. By default it has one dimension for each index dimension, via index_df set explicitly it is flattened with one element per row.\n",
    "        index_df *optional*: The dataframe of indices\n",
    "    \"\"\"\n",
    "\n",
    "    if index_df is None:\n",
    "        assert index_axis_names is not None\n",
    "\n",
    "        # Get the max range for all possible axes\n",
    "        max_axis_range = {\n",
    "            \"layer\": model.cfg.n_layers,\n",
    "            \"pos\": corrupted_tokens.shape[-1],\n",
    "            \"head_index\": model.cfg.n_heads,\n",
    "        }\n",
    "        max_axis_range[\"src_pos\"] = max_axis_range[\"pos\"]\n",
    "        max_axis_range[\"dest_pos\"] = max_axis_range[\"pos\"]\n",
    "        max_axis_range[\"head\"] = max_axis_range[\"head_index\"]\n",
    "\n",
    "        # Get the max range for each axis we iterate over\n",
    "        index_axis_max_range = [max_axis_range[axis_name] for axis_name in index_axis_names]\n",
    "\n",
    "        # Get the dataframe where each row is a tuple of indices\n",
    "        index_df = transformer_lens.patching.make_df_from_ranges(index_axis_max_range, index_axis_names)\n",
    "\n",
    "        flattened_output = False\n",
    "    else:\n",
    "        # A dataframe of indices was provided. Verify that we did not *also* receive index_axis_names\n",
    "        assert index_axis_names is None\n",
    "        index_axis_max_range = index_df.max().to_list()\n",
    "\n",
    "        flattened_output = True\n",
    "\n",
    "    # Create an empty tensor to show the patched metric for each patch\n",
    "    if flattened_output:\n",
    "        patched_metric_output = torch.zeros(len(index_df), device=model.cfg.device)\n",
    "    else:\n",
    "        patched_metric_output = torch.zeros(index_axis_max_range, device=model.cfg.device)\n",
    "\n",
    "    # A generic patching hook - for each index, it applies the patch_setter appropriately to patch the activation\n",
    "    def patching_hook(corrupted_activation, hook, index, clean_activation):\n",
    "        return patch_setter(corrupted_activation, index, clean_activation)\n",
    "\n",
    "    # Iterate over every list of indices, and make the appropriate patch!\n",
    "    for c, index_row in enumerate(tqdm((list(index_df.iterrows())))):\n",
    "        index = index_row[1].to_list()\n",
    "\n",
    "        # The current activation name is just the activation name plus the layer (assumed to be the first element of the input)\n",
    "        current_activation_name = utils.get_act_name(activation_name, layer=index[0])\n",
    "\n",
    "        # The hook function cannot receive additional inputs, so we use partial to include the specific index and the corresponding clean activation\n",
    "        current_hook = partial(\n",
    "            patching_hook,\n",
    "            index=index,\n",
    "            clean_activation=clean_cache[current_activation_name],\n",
    "        )\n",
    "        \n",
    "#         incl_bwd = False\n",
    "#         cache_dict, fwd, bwd = model.get_caching_hooks(\n",
    "#             incl_bwd=incl_bwd,\n",
    "#             device=device,\n",
    "#             names_filter=None\n",
    "#         )\n",
    "        \n",
    "#         fwd_hooks = [(current_activation_name, current_hook)] + fwd\n",
    "        # Run the model with the patching hook and get the logits!\n",
    "        # patched_logits, patched_cache = \"\", \"\"\n",
    "        \n",
    "        patched_logits, patched_cache = model.run_with_cache_with_extra_hook(\n",
    "            corrupted_tokens, \n",
    "            current_activation_name=current_activation_name, \n",
    "            current_hook= current_hook\n",
    "        )\n",
    "        # print(patched_cache.keys())\n",
    "        # print(patched_logits.shape)\n",
    "\n",
    "        # Calculate the patching metric and store\n",
    "        if flattened_output:\n",
    "            patched_metric_output[c] = patching_metric(patched_cache).item()\n",
    "        else:\n",
    "            patched_metric_output[tuple(index)] = patching_metric(patched_cache).item()\n",
    "\n",
    "    if return_index_df:\n",
    "        return patched_metric_output, index_df\n",
    "    else:\n",
    "        return patched_metric_output\n",
    "\n",
    "def layer_pos_patch_setter(corrupted_activation, index, clean_activation):\n",
    "    \"\"\"\n",
    "    Applies the activation patch where index = [layer, pos]\n",
    "\n",
    "    Implicitly assumes that the activation axis order is [batch, pos, ...], which is true of everything that is not an attention pattern shaped tensor.\n",
    "    \"\"\"\n",
    "    assert len(index) == 2\n",
    "    layer, pos = index\n",
    "    corrupted_activation[:, pos, ...] = clean_activation[:, pos, ...]\n",
    "    return corrupted_activation\n",
    "    \n",
    "get_act_patch_resid_pre = partial(\n",
    "    generic_activation_patch,\n",
    "    patch_setter=layer_pos_patch_setter,\n",
    "    activation_name=\"resid_pre\",\n",
    "    index_axis_names=(\"layer\", \"pos\"),\n",
    ")\n",
    "\n",
    "def layer_head_vector_patch_setter(\n",
    "    corrupted_activation,\n",
    "    index,\n",
    "    clean_activation,\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies the activation patch where index = [layer,  head_index]\n",
    "\n",
    "    Implicitly assumes that the activation axis order is [batch, pos, head_index, ...], which is true of all attention head vector activations (q, k, v, z, result) but *not* of attention patterns.\n",
    "    \"\"\"\n",
    "    assert len(index) == 2\n",
    "    layer, head_index = index\n",
    "    corrupted_activation[:, :, head_index] = clean_activation[:, :, head_index]\n",
    "\n",
    "    return corrupted_activation\n",
    "\n",
    "get_act_patch_attn_head_out_all_pos = partial(\n",
    "    generic_activation_patch,\n",
    "    patch_setter=layer_head_vector_patch_setter,\n",
    "    activation_name=\"z\",\n",
    "    index_axis_names=(\"layer\", \"head\"),\n",
    ")\n",
    "\n",
    "def layer_pos_head_vector_patch_setter(\n",
    "    corrupted_activation,\n",
    "    index,\n",
    "    clean_activation,\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies the activation patch where index = [layer, pos, head_index]\n",
    "\n",
    "    Implicitly assumes that the activation axis order is [batch, pos, head_index, ...], which is true of all attention head vector activations (q, k, v, z, result) but *not* of attention patterns.\n",
    "    \"\"\"\n",
    "    assert len(index) == 3\n",
    "    layer, pos, head_index = index\n",
    "    corrupted_activation[:, pos, head_index] = clean_activation[:, pos, head_index]\n",
    "    return corrupted_activation\n",
    "\n",
    "get_act_patch_attn_head_out_by_pos = partial(\n",
    "    generic_activation_patch,\n",
    "    patch_setter=layer_pos_head_vector_patch_setter,\n",
    "    activation_name=\"z\",\n",
    "    index_axis_names=(\"layer\", \"pos\", \"head\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d0e53-34d0-43ff-ae43-7f5497cc5fb0",
   "metadata": {},
   "source": [
    "# METRIC FOR PATCHING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce113e89-b250-4f5f-ac5a-549da0ffe93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_feature_metric(cache):\n",
    "    sae_in = cache[sae.cfg.hook_name]\n",
    "    feature_acts = sae.encode(sae_in)\n",
    "    # print(feature_acts.shape)\n",
    "    feature_acts = feature_acts.squeeze()\n",
    "    return feature_acts[:, :, 15191][-2:].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd433019-11d0-4bcb-bc56-eecda95337dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = model.to_tokens(clean_pr)\n",
    "corrupted_tokens = model.to_tokens(corr_pr)\n",
    "\n",
    "_, clean_cache = model.run_with_cache(clean_tokens)\n",
    "_, corrupted_cache = model.run_with_cache(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbd8e1-4719-4bc7-acfb-99a2f98e3723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb980fbc-3dec-4885-b842-0243b14ef1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resid_pre_act_patch_results = get_act_patch_resid_pre(model, corrupted_tokens, clean_cache, equal_feature_metric)\n",
    "\n",
    "fig = imshow(\n",
    "    resid_pre_act_patch_results, \n",
    "    yaxis=\"Layer\", \n",
    "    xaxis=\"Position\", \n",
    "    x=[f\"{tok} {i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))],\n",
    "    title=\"resid_pre Activation Patching\",\n",
    "    return_fig=True  # This ensures the figure object is returned\n",
    ")\n",
    "\n",
    "fig.write_image(\"results/v1/resid_pre_activation_patching0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58bed3f-76ff-40da-88c8-d1a567566b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_head_out_all_pos_act_patch_results = get_act_patch_attn_head_out_all_pos(model, corrupted_tokens, clean_cache, equal_feature_metric)\n",
    "fig = imshow(attn_head_out_all_pos_act_patch_results,  \n",
    "       yaxis=\"Layer\", \n",
    "       xaxis=\"Head\", \n",
    "       title=\"attn_head_out Activation Patching (All Pos)\", \n",
    "        return_fig=True)\n",
    "\n",
    "fig.write_image(\"results/v1/attn_head_out Activation Patching All Pos0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460eceda-c014-41c2-956f-8fa7356c0cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_SLOW_RUNS = True\n",
    "ALL_HEAD_LABELS = [f\"L{i}H{j}\" for i in range(model.cfg.n_layers) for j in range(model.cfg.n_heads)]\n",
    "if DO_SLOW_RUNS:\n",
    "    attn_head_out_act_patch_results = get_act_patch_attn_head_out_by_pos(model, corrupted_tokens, clean_cache, equal_feature_metric)\n",
    "    attn_head_out_act_patch_results = einops.rearrange(attn_head_out_act_patch_results, \"layer pos head -> (layer head) pos\")\n",
    "    fig = imshow(attn_head_out_act_patch_results, \n",
    "        yaxis=\"Head Label\", \n",
    "        xaxis=\"Pos\", \n",
    "        x=[f\"{tok} {i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))],\n",
    "        y=ALL_HEAD_LABELS,\n",
    "        title=\"attn_head_out Activation Patching By Pos\", \n",
    "        return_fig=True)\n",
    "    fig.write_image(\"results/v1/attn_head_out_act_patch_results0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b92d412-e66a-4158-a0fc-ce521741762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming attn_head_out_act_patch_results is your tensor\n",
    "sliced_results = attn_head_out_act_patch_results[:72, -7:]\n",
    "# Adjust the y-axis labels for the first 72 elements\n",
    "sliced_y_labels = ALL_HEAD_LABELS[:72]\n",
    "\n",
    "# Adjust the x-axis labels for the last 7 positions\n",
    "sliced_x_labels = [f\"{tok} {i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))][-7:]\n",
    "\n",
    "fig = imshow(\n",
    "    sliced_results, \n",
    "    yaxis=\"Head Label\", \n",
    "    xaxis=\"Pos\", \n",
    "    x=sliced_x_labels,\n",
    "    y=sliced_y_labels,\n",
    "    title=\"attn_head_out Activation Patching By Pos\", \n",
    "    width=1000,  # Increase the width of the figure\n",
    "    height=1200,  # Increase the height of the figure\n",
    "    return_fig=True\n",
    ")\n",
    "\n",
    "# Optionally, you can adjust the tickfont size for better readability\n",
    "fig.update_layout(\n",
    "    yaxis=dict(tickfont=dict(size=10)),  # Adjust the size as needed\n",
    "    xaxis=dict(tickfont=dict(size=10))   # Adjust the size as needed\n",
    ")\n",
    "\n",
    "# Save the figure\n",
    "fig.write_image(\"results/v1/attn_head_out_act_patch_results_sliced0.png\")\n",
    "\n",
    "import torch\n",
    "\n",
    "# Assuming sliced_results is your sliced tensor\n",
    "mean_value = sliced_results.mean().item()\n",
    "std_dev = sliced_results.std().item()\n",
    "\n",
    "# Calculate the threshold for one standard deviation away from the mean\n",
    "lower_threshold = mean_value - std_dev\n",
    "upper_threshold = mean_value + std_dev\n",
    "\n",
    "# Identify the indices where the values are one standard deviation away from the mean\n",
    "indices = (sliced_results < lower_threshold) | (sliced_results > upper_threshold)\n",
    "y_indices, x_indices = torch.where(indices)\n",
    "\n",
    "# Extract the corresponding y labels, x labels, and values\n",
    "tuples_list = [\n",
    "    (sliced_y_labels[y_idx], sliced_x_labels[x_idx], sliced_results[y_idx, x_idx].item())\n",
    "    for y_idx, x_idx in zip(y_indices, x_indices)\n",
    "]\n",
    "\n",
    "# Display the tuples\n",
    "print(tuples_list)\n",
    "\n",
    "\n",
    "# Function to convert L2H1 format to (2, 1)\n",
    "def convert_to_tuple(layer_head_str):\n",
    "    layer = int(layer_head_str[1])\n",
    "    head = int(layer_head_str[3])\n",
    "    return (layer, head)\n",
    "\n",
    "# Convert the first element of each tuple in the list\n",
    "converted_tuples = [convert_to_tuple(item[0]) for item in tuples_list] #[(convert_to_tuple(item[0]), item[1], item[2]) for item in tuples_list]\n",
    "import json\n",
    "# Display the result\n",
    "print(\"tuples\", converted_tuples)\n",
    "output_path = f\"results/v1/converted_tuples0.json\"\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(converted_tuples, f)\n",
    "\n",
    "\n",
    "def save_relevant_attention_patterns(clean_cache, layer_head_tuples):\n",
    "    for layer_ind, head_ind in layer_head_tuples:\n",
    "        temp_att_pattern = clean_cache[f'blocks.{layer_ind}.attn.hook_pattern'][1, head_ind, :, :]\n",
    "        attention_pattern = temp_att_pattern.detach().cpu().numpy()\n",
    "        # Define the x and y labels, assuming they correspond to tokens\n",
    "        tokens = model.to_str_tokens(clean_tokens[0])\n",
    "        labels = [f\"{tok} {i}\" for i, tok in enumerate(tokens)]\n",
    "\n",
    "        # Generate the heatmap\n",
    "        fig = px.imshow(\n",
    "            attention_pattern,\n",
    "            labels=dict(x=\"Head Position\", y=\"Head Position\", color=\"Attention\"),\n",
    "            x=labels,\n",
    "            y=labels,\n",
    "            title=f\"Attention Pattern in Layer {layer_ind}, Head {head_ind}\",\n",
    "            color_continuous_scale=\"Blues\"\n",
    "        )\n",
    "        # Display the figure\n",
    "        fig.write_image(f\"results/v1/heads/L{layer_ind}H{head_ind}_atten_pattern0.png\")\n",
    "        \n",
    "save_relevant_attention_patterns(clean_cache, converted_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec1befa-7371-4a97-a48b-22d6fb360ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd2beb-d469-400a-98f0-a02962a3e5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "824b9ba1-c701-4bcb-a7c5-dfe143eefc64",
   "metadata": {},
   "source": [
    "https://www.neuronpedia.org/list/cltetkb9g000111bn9cun5dk6\n",
    "\n",
    "\n",
    "https://www.neuronpedia.org/list/clthkc9p20001ueje4kc0jeoa\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e237f85-8a37-466c-9d25-e8e8323ad606",
   "metadata": {},
   "source": [
    "# THE ORIGINAL (failed) GOAL: Finding feature circuits for features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed0975",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, :, 15191]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4671e17",
   "metadata": {},
   "source": [
    "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "        0.0000, 0.0000, 0.0000, 0.0000, 6.1795, 0.0000, 0.0000, 7.0834],\n",
    "       device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8388c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (clean, corrupted) in enumerate(dataset):\n",
    "    _, clean_cache = model.run_with_cache_with_saes(clean, saes=saes)\n",
    "    print(f\"Pair {i+1}:\")\n",
    "    print(f\"  Clean:     {clean}\")\n",
    "    print(clean_cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, :, 15191][13])\n",
    "    _, corr_cache = model.run_with_cache_with_saes(corrupted, saes=saes)\n",
    "    print(f\"  Corrupted: {corrupted}\")\n",
    "    print(corr_cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, :, 15191][13])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb949968",
   "metadata": {},
   "source": [
    "Pair 1:\n",
    "  Clean:     What is the output of 96 plus 43 ? \n",
    "tensor(7.0159, device='cuda:0')\n",
    "  Corrupted: What is the output of 96 and 43 ? \n",
    "tensor(0., device='cuda:0')\n",
    "Pair 2:\n",
    "  Clean:     What is the output of 56 plus 77 ? \n",
    "tensor(5.5384, device='cuda:0')\n",
    "  Corrupted: What is the output of 56 and 77 ? \n",
    "tensor(0., device='cuda:0')\n",
    "Pair 3:\n",
    "  Clean:     What is the output of 93 plus 56 ? \n",
    "tensor(6.7896, device='cuda:0')\n",
    "  Corrupted: What is the output of 93 and 56 ? \n",
    "tensor(0., device='cuda:0')\n",
    "Pair 4:\n",
    "  Clean:     What is the output of 29 plus 37 ? \n",
    "tensor(5.6018, device='cuda:0')\n",
    "  Corrupted: What is the output of 29 and 37 ? \n",
    "tensor(0., device='cuda:0')\n",
    "Pair 5:\n",
    "  Clean:     What is the output of 29 plus 75 ? \n",
    "tensor(6.8103, device='cuda:0')\n",
    "  Corrupted: What is the output of 29 and 75 ? \n",
    "tensor(0., device='cuda:0')\n",
    "Pair 6:\n",
    "  Clean:     What is the output of 48 plus 18 ? \n",
    "tensor(8.9317, device='cuda:0')\n",
    "  Corrupted: What is the output of 48 and 18 ? \n",
    "tensor(2.8513, device='cuda:0')\n",
    "Pair 7:\n",
    "  Clean:     What is the output of 91 plus 12 ? \n",
    "tensor(6.7175, device='cuda:0')\n",
    "  Corrupted: What is the output of 91 and 12 ? \n",
    "tensor(0., device='cuda:0')\n",
    "Pair 8:\n",
    "  Clean:     What is the output of 33 plus 76 ? \n",
    "tensor(6.0717, device='cuda:0')\n",
    "  Corrupted: What is the output of 33 and 76 ? \n",
    "tensor(0., device='cuda:0')\n",
    "Pair 9:\n",
    "  Clean:     What is the output of 53 plus 83 ? \n",
    "tensor(6.7595, device='cuda:0')\n",
    "  Corrupted: What is the output of 53 and 83 ? \n",
    "tensor(0., device='cuda:0')\n",
    "Pair 10:\n",
    "  Clean:     What is the output of 39 plus 16 ? \n",
    "tensor(7.5431, device='cuda:0')\n",
    "  Corrupted: What is the output of 39 and 16 ? \n",
    "tensor(0., device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee2cc5-ae52-473a-999f-599b58f67d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD till 8 \n",
    "saes = []\n",
    "for layer in range(9):\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=\"gemma-scope-2b-pt-res\", \n",
    "        sae_id=closest_strings[layer],  \n",
    "        device=device\n",
    "    )\n",
    "    sae.use_error_term = True\n",
    "    saes.append(sae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf937f-1d0d-4bbe-a429-5e56c310f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deep_attr(obj: Any, path: str):\n",
    "    \"\"\"Helper function to get a nested attribute from a object.\n",
    "    In practice used to access HookedTransformer HookPoints (eg model.blocks[0].attn.hook_z)\n",
    "    Args:\n",
    "        obj: Any object. In practice, this is a HookedTransformer (or subclass)\n",
    "        path: str. The path to the attribute you want to access. (eg \"blocks.0.attn.hook_z\")\n",
    "\n",
    "    returns:\n",
    "        Any. The attribute at the end of the path\n",
    "    \"\"\"\n",
    "    parts = path.split(\".\")\n",
    "    # Navigate to the last component in the path\n",
    "    for part in parts:\n",
    "        if part.isdigit():  # This is a list index\n",
    "            obj = obj[int(part)]\n",
    "        else:  # This is an attribute\n",
    "            obj = getattr(obj, part)\n",
    "    return obj\n",
    "\n",
    "def replace_with_zeros(layer, feature_ind, clean_cache):\n",
    "    \"\"\"\n",
    "    Forward hook function to replace activations in the corrupted run\n",
    "    with zeros instead of the clean activations.\n",
    "    \n",
    "    Args:\n",
    "        layer (int): The layer number where the intervention should take place.\n",
    "        feature_ind (int): The feature index to replace.\n",
    "        clean_cache (ActivationCache): The cache from the clean run (not used in this case).\n",
    "    \"\"\"\n",
    "    def hook_fn(module, input, output):\n",
    "        # Create a tensor of zeros with the same shape as the specific output slice\n",
    "        zero_tensor = torch.zeros_like(output[0, :, feature_ind])\n",
    "        \n",
    "        # Replace the corresponding activations in the output with zeros\n",
    "        output[0, :, feature_ind] = zero_tensor\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    return hook_fn\n",
    "\n",
    "# Define the hook function\n",
    "def replace_with_clean_activations(layer, feature_ind, clean_cache):\n",
    "    \"\"\"\n",
    "    Forward hook function to replace activations in the corrupted run\n",
    "    with those from the clean run.\n",
    "    \n",
    "    Args:\n",
    "        layer (int): The layer number where the intervention should take place.\n",
    "        feature_ind (int): The feature index to replace.\n",
    "        clean_cache (ActivationCache): The cache from the clean run.\n",
    "    \"\"\"\n",
    "    def hook_fn(module, input, output):\n",
    "        # Extract the clean activations\n",
    "        clean_acts = clean_cache[f'blocks.{layer}.hook_resid_post.hook_sae_acts_post'][0, :, feature_ind]\n",
    "        # print(output[:, :, feature_ind])\n",
    "        # print(clean_acts)\n",
    "        # Replace the corresponding activations in the output of the corrupted run\n",
    "        output[0, :, feature_ind] = clean_acts\n",
    "        # print(output[0, :, feature_ind])\n",
    "        return output\n",
    "    \n",
    "    return hook_fn\n",
    "\n",
    "def subtract_and_mean(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Subtract two tensors element-wise and return the mean of the resulting tensor.\n",
    "    \n",
    "    Args:\n",
    "        tensor1 (torch.Tensor): The first tensor.\n",
    "        tensor2 (torch.Tensor): The second tensor.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The mean of the element-wise subtraction of the two tensors.\n",
    "    \"\"\"\n",
    "    # Ensure that both tensors have the same shape\n",
    "    assert tensor1.shape == tensor2.shape, \"Tensors must have the same shape\"\n",
    "    \n",
    "    # Subtract the tensors element-wise\n",
    "    difference = tensor1 - tensor2\n",
    "    \n",
    "    # Calculate the mean of the difference\n",
    "    mean_difference = torch.mean(difference)\n",
    "    \n",
    "    return mean_difference\n",
    "\n",
    "\n",
    "\n",
    "def feature_acts(cache):\n",
    "    return cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, :, 15191][13]\n",
    "\n",
    "def overall_feature(cache):\n",
    "    return cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, :, 15191]\n",
    "\n",
    "def feature_difference(cache1, cache2):\n",
    "    return feature_acts(cache1) - feature_acts(cache2)\n",
    "\n",
    "def overall_feature_difference(cache1, cache2):\n",
    "    return subtract_and_mean(cache1['blocks.8.hook_resid_post.hook_sae_acts_post'][0, :, 15191], cache2['blocks.8.hook_resid_post.hook_sae_acts_post'][0, :, 15191])\n",
    "# # Specify the layer number and feature index\n",
    "# layer = 2  # Example: replace activations at layer 2\n",
    "# feature_ind = 5  # Example: replace activations at feature index 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "op_clean, clean_cache = model.run_with_cache_with_saes(clean, saes=saes)\n",
    "op_corr, corr_cache = model.run_with_cache_with_saes(corrupted, saes=saes)\n",
    "\n",
    "print(\"og logit\", op_clean.shape)\n",
    "print(\"Original feature diff: \", feature_difference(clean_cache, corr_cache))\n",
    "\n",
    "for key, val in operator_features.items():\n",
    "    layer = key\n",
    "    for ind in range(10000):\n",
    "        feature_ind = ind\n",
    "        hook = replace_with_clean_activations(layer, feature_ind, clean_cache)\n",
    "        # with model.saes(saes=saes, reset_saes_end=True):\n",
    "            # Now that SAEs are attached, access the hook point\n",
    "        hook_point = get_deep_attr(saes[layer], 'hook_sae_acts_post')\n",
    "        # print(hook_point)\n",
    "        # Register the hook\n",
    "        hook_handle = hook_point.register_forward_hook(hook)\n",
    "        # print(hook_handle)\n",
    "        # Run the corrupted prompt with the hook applied\n",
    "        inter_op, corr_cache_intervened = model.run_with_cache_with_saes(corrupted, saes=saes)\n",
    "        # print(\"intervened logit\", inter_op[0, -1, :10])\n",
    "        # Remove the hook after the forward pass\n",
    "        hook_handle.remove()\n",
    "        \n",
    "        # print(subtract_and_mean(op_corr, inter_op))\n",
    "        if overall_feature_difference(corr_cache_intervened, corr_cache) != 0: \n",
    "            print(\"UPPPPPPPP\")\n",
    "        # print(overall_feature_difference(corr_cache_intervened, corr_cache))\n",
    "            \n",
    "            \n",
    "        # hook_point = get_deep_attr(model, f'blocks.{layer}.hook_resid_post.hook_sae_acts_post')\n",
    "        # print(hook_point)\n",
    "        # hook_handle = hook_point.register_forward_hook(hook)\n",
    "        # print(hook_handle)\n",
    "        # _, corr_cache_intervened = model.run_with_cache_with_saes(corrupted, saes=saes)\n",
    "        # hook_handle.remove()\n",
    "        # print(\"PLESAAE CHANGE ONCE JUST ONCE :\", feature_acts(corr_cache_intervened))\n",
    "        # print(\"PLESAAE CHANGE ONCE JUST ONCE :\", feature_difference(clean_cache, corr_cache_intervened))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b2625-822e-4837-a271-2e80099f7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corr -> clean \n",
    "\n",
    "_, clean_cache = model.run_with_cache_with_saes(clean, saes=saes)\n",
    "_, corr_cache = model.run_with_cache_with_saes(corrupted, saes=saes)\n",
    "\n",
    "print(\"Original feature difference: \", feature_difference(clean_cache, corr_cache))\n",
    "\n",
    "# Dictionary to store the differences\n",
    "differences = []\n",
    "\n",
    "for key, val in operator_features.items():\n",
    "    layer = key\n",
    "    print(\"layer: \", layer)\n",
    "    for feature_ind in range(1000):\n",
    "        hook = replace_with_clean_activations(layer, feature_ind, corr_cache)\n",
    "        \n",
    "        # Now that SAEs are attached, access the hook point\n",
    "        hook_point = get_deep_attr(saes[layer], 'hook_sae_acts_post')\n",
    "        \n",
    "        # Register the hook\n",
    "        hook_handle = hook_point.register_forward_hook(hook)\n",
    "        \n",
    "        # Run the corrupted prompt with the hook applied\n",
    "        _, clean_cache_intervened = model.run_with_cache_with_saes(clean, saes=saes)\n",
    "        \n",
    "        # Remove the hook after the forward pass\n",
    "        hook_handle.remove()\n",
    "        \n",
    "        # Calculate the overall feature difference\n",
    "        diff = overall_feature_difference(clean_cache_intervened, clean_cache)\n",
    "        \n",
    "        # Store the difference along with the corresponding layer and feature index\n",
    "        differences.append((layer, feature_ind, diff))\n",
    "\n",
    "# Sort the differences to get the top 10\n",
    "top_differences = sorted(differences, key=lambda x: x[2], reverse=True)[:10]\n",
    "\n",
    "# Print the top 10 features with their differences\n",
    "print(\"Top 10 features with the highest overall_feature_difference:\")\n",
    "for layer, feature_ind, diff in top_differences:\n",
    "    print(f\"Layer: {layer}, Feature Index: {feature_ind}, Overall Feature Difference: {diff}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3e693",
   "metadata": {},
   "source": [
    "Original feature difference:  tensor(7.5431, device='cuda:0')\n",
    "layer:  0\n",
    "layer:  1\n",
    "layer:  2\n",
    "layer:  3\n",
    "layer:  4\n",
    "layer:  5\n",
    "layer:  6\n",
    "layer:  7\n",
    "layer:  8\n",
    "Top 10 features with the highest overall_feature_difference:\n",
    "Layer: 1, Feature Index: 172, Overall Feature Difference: 5.880991693629767e-07\n",
    "Layer: 1, Feature Index: 202, Overall Feature Difference: 5.722046125811175e-07\n",
    "Layer: 5, Feature Index: 697, Overall Feature Difference: 5.722046125811175e-07\n",
    "Layer: 5, Feature Index: 112, Overall Feature Difference: 5.404154990173993e-07\n",
    "Layer: 0, Feature Index: 133, Overall Feature Difference: 5.245208853921213e-07\n",
    "Layer: 2, Feature Index: 616, Overall Feature Difference: 5.245208853921213e-07\n",
    "Layer: 1, Feature Index: 534, Overall Feature Difference: 5.086263286102621e-07\n",
    "Layer: 4, Feature Index: 646, Overall Feature Difference: 5.086263286102621e-07\n",
    "Layer: 2, Feature Index: 498, Overall Feature Difference: 4.92731771828403e-07\n",
    "Layer: 4, Feature Index: 920, Overall Feature Difference: 4.92731771828403e-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d0727-4566-474d-8aaa-5ea53d5a4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corr -> clean \n",
    "\n",
    "op_clean, clean_cache = model.run_with_cache_with_saes(clean, saes=saes)\n",
    "op_corr, corr_cache = model.run_with_cache_with_saes(corrupted, saes=saes)\n",
    "\n",
    "print(\"og logit\", op_clean.shape)\n",
    "print(\"Original feature diff: \", feature_difference(clean_cache, corr_cache))\n",
    "\n",
    "for key, val in operator_features.items():\n",
    "    layer = key\n",
    "    for feature_ind in val:\n",
    "        feature_ind = feature_ind.item()\n",
    "        hook = replace_with_zeros(layer, feature_ind, corr_cache)\n",
    "        # with model.saes(saes=saes, reset_saes_end=True):\n",
    "            # Now that SAEs are attached, access the hook point\n",
    "        hook_point = get_deep_attr(saes[layer], 'hook_sae_acts_post')\n",
    "        # print(hook_point)\n",
    "        # Register the hook\n",
    "        hook_handle = hook_point.register_forward_hook(hook)\n",
    "        # print(hook_handle)\n",
    "        # Run the corrupted prompt with the hook applied\n",
    "        inter_op, clean_cache_intervened = model.run_with_cache_with_saes(clean, saes=saes)\n",
    "        # print(\"intervened logit\", inter_op[0, -1, :10])\n",
    "        # Remove the hook after the forward pass\n",
    "        hook_handle.remove()\n",
    "        \n",
    "        # print(subtract_and_mean(op_corr, inter_op))\n",
    "        # print(\"PLESAAE CHANGE ONCE JUST ONCE :\", feature_difference(clean_cache_intervened, corr_cache))\n",
    "        if overall_feature_difference(clean_cache_intervened, clean_cache) != 0: \n",
    "            print(\"UPPPPPPPP\")\n",
    "            print(overall_feature_difference(clean_cache_intervened, clean_cache))\n",
    "            # print(\"Og :\", overall_feature(clean_cache))\n",
    "            # print(\"Intervened :\", overall_feature(clean_cache_intervened))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8da701",
   "metadata": {},
   "source": [
    "og logit torch.Size([1, 15, 256000])\n",
    "Original feature diff:  tensor(7.5431, device='cuda:0')\n",
    "UPPPPPPPP\n",
    "tensor(5.8810e-07, device='cuda:0')\n",
    "UPPPPPPPP\n",
    "tensor(2.3842e-07, device='cuda:0')\n",
    "UPPPPPPPP\n",
    "tensor(4.6094e-07, device='cuda:0')\n",
    "UPPPPPPPP\n",
    "tensor(6.8347e-07, device='cuda:0')\n",
    "UPPPPPPPP\n",
    "tensor(3.0200e-07, device='cuda:0')\n",
    "UPPPPPPPP\n",
    "tensor(-1.5895e-08, device='cuda:0')\n",
    "UPPPPPPPP\n",
    "tensor(1.5895e-07, device='cuda:0')\n",
    "UPPPPPPPP\n",
    "tensor(7.9473e-08, device='cuda:0')\n",
    "UPPPPPPPP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c507e",
   "metadata": {},
   "source": [
    "# Finding the feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6412f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res\", # <- Release name \n",
    "    sae_id = \"layer_8/width_16k/average_l0_71\", # <- SAE id (not always a hook point!)\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e880ef26-5992-4b87-95d9-f8c4e8faeb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "\n",
    "prompt = \"What is the output of 53 plus 34 ? It is \"\n",
    "answer = '8'\n",
    "# Show that the model can confidently predict the next token.\n",
    "test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d4937-4424-4a51-97f4-a83bec37020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"What’s the output of 54 plus 32? It is \", \n",
    "\"What result is 54 plus 32? It is \",\n",
    "\"What does 54 plus 32 equal? It is \",\n",
    "\"What’s the sum of 54 and 32? It is \",\n",
    "\"What do you get from 54 plus 32? It is \",\n",
    "\"What does 54 plus 32 give? It is \",\n",
    "\"What is 54 plus 32 equal to? It is \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52442711-3a66-4c6a-856b-13befd748231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "# Dictionary to accumulate the sums of values for each index\n",
    "vals_dict = defaultdict(float)\n",
    "# Dictionary to count occurrences of each index\n",
    "count_dict = defaultdict(int)\n",
    "\n",
    "# Run through each prompt and accumulate the values for each index\n",
    "for pr in prompts:\n",
    "    _, cache = model.run_with_cache_with_saes(pr, saes=[sae])\n",
    "    vals, inds = torch.topk(cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, -1, :], 15)\n",
    "    \n",
    "    for val, ind in zip(vals, inds):\n",
    "        vals_dict[ind.item()] += val.item()\n",
    "        count_dict[ind.item()] += 1\n",
    "\n",
    "# Calculate the average value for each index\n",
    "avg_vals_dict = {ind: vals_dict[ind] / count_dict[ind] for ind in vals_dict}\n",
    "\n",
    "# Sort the indices by their average values in descending order\n",
    "sorted_avg_vals = sorted(avg_vals_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the top 10 indices with the highest average values\n",
    "top_10_inds = sorted_avg_vals[:10]\n",
    "\n",
    "# Print the top 10 indices and their corresponding average values\n",
    "print(\"Top 10 feature indices and their average values:\")\n",
    "for ind, avg_val in top_10_inds:\n",
    "    print(f\"Index: {ind}, Average Value: {avg_val}\")\n",
    "    html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"8-gemmascope-res-16k\", feature_idx=ind)\n",
    "    display(IFrame(html, width=1200, height=300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45af129a-9cb1-46b9-a001-3fe1314623fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hooked SAE Transformer will enable us to get the feature activations from the SAE\n",
    "_, cache = model.run_with_cache_with_saes(prompt, saes=[sae])\n",
    "\n",
    "print([(k, v.shape) for k,v in cache.items() if \"sae\" in k])\n",
    "\n",
    "# note there were 11 tokens in our prompt, the residual stream dimension is 768, and the number of SAE features is 768\n",
    "\n",
    "from IPython.display import IFrame\n",
    "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "def get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"8-gemmascope-res-16k\", feature_idx=0):\n",
    "    return html_template.format(sae_release, sae_id, feature_idx)\n",
    "\n",
    "# Create the line plot\n",
    "fig = px.line(\n",
    "    cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, -1, :].cpu().numpy(),\n",
    "    title=\"Feature activations at the final token position\",\n",
    "    labels={\"index\": \"Feature\", \"value\": \"Activation\"},\n",
    ")\n",
    "\n",
    "# Show the plot (optional, for interactive display)\n",
    "fig.show()\n",
    "\n",
    "# Save the figure to a file\n",
    "fig.write_image(\"feature_activations.png\")  # Save as a PNG image\n",
    "\n",
    "# let's print the top 5 features and how much they fired\n",
    "vals, inds = torch.topk(cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, -1, :], 15)\n",
    "for val, ind in zip(vals, inds):\n",
    "    print(f\"Feature {ind} fired {val:.2f}\")\n",
    "    # html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"8-gemmascope-res-16k\", feature_idx=ind)\n",
    "    # display(IFrame(html, width=1200, height=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb5c05a",
   "metadata": {},
   "source": [
    "Pos\n",
    "Feature 16100 fired 27.27\n",
    "Feature 2024 fired 12.52\n",
    "Feature 10744 fired 10.61\n",
    "Feature 8857 fired 9.67\n",
    "Feature 13307 fired 8.00\n",
    "Feature 5326 fired 7.58\n",
    "Feature 8025 fired 7.53\n",
    "Feature 14739 fired 6.78\n",
    "Feature 15191 fired 6.25\n",
    "Feature 15600 fired 6.15\n",
    "Feature 11484 fired 5.83\n",
    "Feature 6179 fired 5.61\n",
    "Feature 2121 fired 5.11\n",
    "Feature 5927 fired 5.10\n",
    "Feature 3173 fired 4.64\n",
    "\n",
    "neg \n",
    "Feature 16100 fired 26.37\n",
    "Feature 2024 fired 12.97\n",
    "Feature 13307 fired 10.09\n",
    "Feature 14739 fired 9.11\n",
    "Feature 8857 fired 8.37\n",
    "Feature 10744 fired 8.32\n",
    "Feature 15191 fired 7.78\n",
    "Feature 5326 fired 7.65\n",
    "Feature 8025 fired 6.88\n",
    "Feature 2121 fired 5.81\n",
    "Feature 15600 fired 5.77\n",
    "Feature 302 fired 5.42\n",
    "Feature 4150 fired 5.32\n",
    "Feature 6179 fired 5.24\n",
    "Feature 324 fired 5.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb1c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the biggest features in terms of absolute difference\n",
    "prompt = [\"What is 54 plus 32 ? \\nIt is \", \"What is 54 plus 32 ? \\nWhat is \"]\n",
    "_, cache = model.run_with_cache_with_saes(prompt, saes=[sae])\n",
    "print([(k, v.shape) for k,v in cache.items() if \"sae\" in k])\n",
    "\n",
    "feature_activation_df = pd.DataFrame(cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, -1, :].cpu().numpy(),\n",
    "                                     index = [f\"feature_{i}\" for i in range(sae.cfg.d_sae)],\n",
    ")\n",
    "feature_activation_df.columns = [\"int\"]\n",
    "feature_activation_df[\"string\"] = cache['blocks.8.hook_resid_post.hook_sae_acts_post'][1, -1, :].cpu().numpy()\n",
    "feature_activation_df[\"diff\"]= feature_activation_df[\"int\"] - feature_activation_df[\"string\"]\n",
    "\n",
    "fig = px.line(\n",
    "    feature_activation_df,\n",
    "    title=\"Feature activations for the prompt\",\n",
    "    labels={\"index\": \"Feature\", \"value\": \"Activation\"},\n",
    ")\n",
    "\n",
    "# hide the x-ticks\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "fig.show()\n",
    "fig.write_image(\"diff_feature_activations.png\") \n",
    "\n",
    "diff = cache['blocks.8.hook_resid_post.hook_sae_acts_post'][1, -1, :].cpu() - cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, -1, :].cpu()\n",
    "vals, inds = torch.topk(torch.abs(diff), 15)\n",
    "for val, ind in zip(vals, inds):\n",
    "    print(f\"Feature {ind} had a difference of {val:.2f}\")\n",
    "    html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"8-gemmascope-res-16k\", feature_idx=ind)\n",
    "    display(IFrame(html, width=1200, height=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a37c66",
   "metadata": {},
   "source": [
    "# Finding other abstract features attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e01d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_variation = {'What is the output of 53 plus 32 ? A: ': 13,\n",
    "                   'How much is 12 plus 89 ? A: ': 12, \n",
    "                   'What is 31 plus 16? It is ': 12, \n",
    "                   'What is the sum of 59 and the product of 82 and 33? It is ': 21,\n",
    "                   'What is the difference between 19 and the product of 55 and 10 ?' : 21, \n",
    "                   '13 + 45 * 42 = ': 12} \n",
    "for pr in equal_variation.keys():\n",
    "    tokens = model.to_str_tokens(pr)\n",
    "    print(tokens)\n",
    "    for ind, token in enumerate(tokens):\n",
    "        # print(ind, token)\n",
    "        if '?' in token or '=' in token:\n",
    "            print(ind, token)\n",
    "            equal_variation[pr] = ind\n",
    "    # print(model.tokenizer.encode(pr))\n",
    "            \n",
    "import torch.nn.functional as F\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res\", # <- Release name \n",
    "    sae_id = \"layer_8/width_16k/average_l0_71\", # <- SAE id (not always a hook point!)\n",
    "    device = device\n",
    ")\n",
    "pr = 'What is the output of 53 plus 32 ? A: '\n",
    "\n",
    "# for ind in range(1, 17):\n",
    "ind = 13\n",
    "_, cache = model.run_with_cache_with_saes(pr, saes=[sae])\n",
    "# print*F.softmax(x, dim=1)\n",
    "x = cache['blocks.8.hook_resid_post.hook_sae_acts_post']\n",
    "topk_values, topk_indices = torch.topk(x[0, ind, :], 20)\n",
    "gathered_values = x[0, :, topk_indices]\n",
    "softmaxed_values = F.softmax(gathered_values, dim=0)\n",
    "print(softmaxed_values)\n",
    "# vals, inds = torch.topk(cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, 13, :], 50)\n",
    "# inds\n",
    "\n",
    "# layer = 8\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=\"gemma-scope-2b-pt-res\",  # <- Release name \n",
    "        sae_id=f\"layer_8/width_16k/average_l0_71\",  # <- SAE id (not always a hook point!)\n",
    "        device=device\n",
    "    )\n",
    "vals_dict = defaultdict(float)\n",
    "count_dict = defaultdict(int)\n",
    "for pr, ind in equal_variation.items():\n",
    "    _, cache = model.run_with_cache_with_saes(pr, saes=[sae])\n",
    "    vals, inds = torch.topk(cache[f'blocks.8.hook_resid_post.hook_sae_acts_post'][0, ind, :], 50)\n",
    "    \n",
    "    for val, ind in zip(vals, inds):\n",
    "        vals_dict[ind.item()] += val.item()\n",
    "        count_dict[ind.item()] += 1\n",
    "print(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc58ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store top 10 features for each layer\n",
    "top_features_per_layer = {}\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    # Load the SAE for the current layer\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=\"gemma-scope-2b-pt-res-canonical\",  # <- Release name \n",
    "        sae_id=f\"layer_{layer}/width_16k/canonical\",  # <- SAE id (not always a hook point!)\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Dictionary to accumulate the sums of values for each index\n",
    "    vals_dict = defaultdict(float)\n",
    "    # Dictionary to count occurrences of each index\n",
    "    count_dict = defaultdict(int)\n",
    "\n",
    "    # Run through each prompt and accumulate the values for each index\n",
    "    for pr, ind in equal_variation.items():\n",
    "        _, cache = model.run_with_cache_with_saes(pr, saes=[sae])\n",
    "        vals, inds = torch.topk(cache[f'blocks.{layer}.hook_resid_post.hook_sae_acts_post'][0, ind, :], 15)\n",
    "\n",
    "        for val, ind in zip(vals, inds):\n",
    "            vals_dict[ind.item()] += val.item()\n",
    "            count_dict[ind.item()] += 1\n",
    "\n",
    "    # Calculate the average value for each index\n",
    "    avg_vals_dict = {ind: vals_dict[ind] / count_dict[ind] for ind in vals_dict}\n",
    "\n",
    "    # Sort the indices by their average values in descending order\n",
    "    sorted_avg_vals = sorted(avg_vals_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top 10 indices with the highest average values\n",
    "    top_10_inds = sorted_avg_vals[:5]\n",
    "\n",
    "    # Store the top 10 features for this layer\n",
    "    top_features_per_layer[layer] = top_10_inds\n",
    "\n",
    "    # Optionally, print the top 10 indices and their corresponding average values\n",
    "    print(f\"Top 10 feature indices for layer {layer} and their average values:\")\n",
    "    for ind, avg_val in top_10_inds:\n",
    "        print(f\"Index: {ind}, Average Value: {avg_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07afdaca",
   "metadata": {},
   "source": [
    "Top 10 feature indices for layer 0 and their average values:\n",
    "Index: 11566, Average Value: 46.200286102294925\n",
    "Index: 6210, Average Value: 34.4622917175293\n",
    "Index: 5737, Average Value: 26.257030487060547\n",
    "Index: 443, Average Value: 25.61747932434082\n",
    "Index: 8418, Average Value: 25.00115203857422\n",
    "Top 10 feature indices for layer 1 and their average values:\n",
    "Index: 14339, Average Value: 52.61330922444662\n",
    "Index: 6210, Average Value: 40.77787780761719\n",
    "Index: 9727, Average Value: 32.45850467681885\n",
    "Index: 1112, Average Value: 23.716838836669922\n",
    "Index: 1508, Average Value: 21.153118133544922\n",
    "Top 10 feature indices for layer 2 and their average values:\n",
    "Index: 9433, Average Value: 36.74750518798828\n",
    "Index: 15046, Average Value: 29.509745279947918\n",
    "Index: 1473, Average Value: 25.7916259765625\n",
    "Index: 324, Average Value: 24.814545822143554\n",
    "Index: 12653, Average Value: 22.311517906188964\n",
    "Top 10 feature indices for layer 3 and their average values:\n",
    "Index: 4793, Average Value: 38.174590301513675\n",
    "Index: 10909, Average Value: 32.39120864868164\n",
    "Index: 10538, Average Value: 25.340720494588215\n",
    "Index: 7465, Average Value: 21.02108383178711\n",
    "Index: 6210, Average Value: 17.363277435302734\n",
    "Top 10 feature indices for layer 4 and their average values:\n",
    "Index: 14260, Average Value: 41.60628128051758\n",
    "Index: 11465, Average Value: 30.917710876464845\n",
    "Index: 9433, Average Value: 29.14781951904297\n",
    "Index: 2983, Average Value: 21.2492618560791\n",
    "Index: 14307, Average Value: 17.716243743896484\n",
    "Top 10 feature indices for layer 5 and their average values:\n",
    "Index: 14260, Average Value: 32.957908630371094\n",
    "Index: 4793, Average Value: 29.707582092285158\n",
    "Index: 2841, Average Value: 23.889563242594402\n",
    "Index: 4463, Average Value: 20.049205780029297\n",
    "Index: 5971, Average Value: 19.761066436767578\n",
    "Top 10 feature indices for layer 6 and their average values:\n",
    "Index: 14525, Average Value: 28.608153978983562\n",
    "Index: 13401, Average Value: 27.663322067260744\n",
    "Index: 13506, Average Value: 21.753177642822266\n",
    "Index: 4315, Average Value: 20.728521982828777\n",
    "Index: 1227, Average Value: 17.006886800130207\n",
    "Top 10 feature indices for layer 7 and their average values:\n",
    "Index: 3186, Average Value: 38.38584518432617\n",
    "Index: 15356, Average Value: 28.452518463134766\n",
    "Index: 6751, Average Value: 27.4490966796875\n",
    "Index: 9234, Average Value: 25.660961151123047\n",
    "Index: 9182, Average Value: 23.671207427978516\n",
    "Top 10 feature indices for layer 8 and their average values:\n",
    "Index: 14801, Average Value: 41.51571083068848\n",
    "Index: 7357, Average Value: 33.20413398742676\n",
    "Index: 3178, Average Value: 31.21910858154297\n",
    "Index: 4781, Average Value: 26.748559188842773\n",
    "Index: 8217, Average Value: 23.01729393005371\n",
    "Top 10 feature indices for layer 9 and their average values:\n",
    "Index: 3517, Average Value: 36.59803263346354\n",
    "Index: 785, Average Value: 26.311880111694336\n",
    "Index: 13664, Average Value: 24.884449005126953\n",
    "Index: 13125, Average Value: 20.077545166015625\n",
    "Index: 13084, Average Value: 19.83129119873047\n",
    "Top 10 feature indices for layer 10 and their average values:\n",
    "Index: 11772, Average Value: 43.767255783081055\n",
    "Index: 13146, Average Value: 33.37391662597656\n",
    "Index: 12541, Average Value: 31.429155985514324\n",
    "Index: 8915, Average Value: 27.96062660217285\n",
    "Index: 2710, Average Value: 27.930782318115234\n",
    "Top 10 feature indices for layer 11 and their average values:\n",
    "Index: 12930, Average Value: 39.17667706807455\n",
    "Index: 892, Average Value: 39.01293754577637\n",
    "Index: 7330, Average Value: 31.008285522460938\n",
    "Index: 14117, Average Value: 30.257923762003582\n",
    "Index: 3300, Average Value: 23.18971824645996\n",
    "Top 10 feature indices for layer 12 and their average values:\n",
    "Index: 10708, Average Value: 47.934686024983726\n",
    "Index: 9467, Average Value: 29.514047622680664\n",
    "Index: 6558, Average Value: 28.683330917358397\n",
    "Index: 15437, Average Value: 28.182409286499023\n",
    "Index: 4514, Average Value: 21.887222290039062\n",
    "Top 10 feature indices for layer 13 and their average values:\n",
    "Index: 3883, Average Value: 50.04939206441244\n",
    "Index: 5472, Average Value: 45.397453943888344\n",
    "Index: 9467, Average Value: 42.304283142089844\n",
    "Index: 12931, Average Value: 38.15758209228515\n",
    "Index: 12969, Average Value: 29.625812530517578\n",
    "Top 10 feature indices for layer 14 and their average values:\n",
    "Index: 15603, Average Value: 58.318609873453774\n",
    "Index: 2884, Average Value: 42.05490417480469\n",
    "Index: 15559, Average Value: 39.046241760253906\n",
    "Index: 3211, Average Value: 32.775944519042966\n",
    "Index: 15471, Average Value: 29.37887954711914\n",
    "Top 10 feature indices for layer 15 and their average values:\n",
    "Index: 4300, Average Value: 51.74925676981608\n",
    "Index: 15892, Average Value: 42.46276702880859\n",
    "Index: 772, Average Value: 34.68086624145508\n",
    "Index: 13987, Average Value: 33.935614013671874\n",
    "Index: 14211, Average Value: 32.66618982950846\n",
    "Top 10 feature indices for layer 16 and their average values:\n",
    "Index: 7645, Average Value: 53.15576171875\n",
    "Index: 6094, Average Value: 52.22842343648275\n",
    "Index: 1281, Average Value: 51.58769098917643\n",
    "Index: 318, Average Value: 48.4905039469401\n",
    "Index: 16333, Average Value: 42.81694030761719\n",
    "Top 10 feature indices for layer 17 and their average values:\n",
    "Index: 9962, Average Value: 81.1238276163737\n",
    "Index: 6396, Average Value: 65.58076477050781\n",
    "Index: 8675, Average Value: 46.98172505696615\n",
    "Index: 14016, Average Value: 43.80245780944824\n",
    "Index: 2844, Average Value: 42.20041688283285\n",
    "Top 10 feature indices for layer 18 and their average values:\n",
    "Index: 12865, Average Value: 88.5863431294759\n",
    "Index: 11871, Average Value: 81.81711196899414\n",
    "Index: 2464, Average Value: 62.58462905883789\n",
    "Index: 623, Average Value: 48.85605239868164\n",
    "Index: 8486, Average Value: 41.510746765136716\n",
    "Top 10 feature indices for layer 19 and their average values:\n",
    "Index: 619, Average Value: 65.64395141601562\n",
    "Index: 9837, Average Value: 56.29365158081055\n",
    "Index: 15530, Average Value: 48.74580383300781\n",
    "Index: 13266, Average Value: 47.22393544514974\n",
    "Index: 6897, Average Value: 42.807013511657715\n",
    "Top 10 feature indices for layer 20 and their average values:\n",
    "Index: 3013, Average Value: 88.45135752360027\n",
    "Index: 13363, Average Value: 64.82990264892578\n",
    "Index: 13260, Average Value: 56.37356376647949\n",
    "Index: 9268, Average Value: 50.00233395894369\n",
    "Index: 10032, Average Value: 47.21528625488281\n",
    "Top 10 feature indices for layer 21 and their average values:\n",
    "Index: 13608, Average Value: 96.21867370605469\n",
    "Index: 10140, Average Value: 79.54266357421875\n",
    "Index: 3461, Average Value: 65.96608924865723\n",
    "Index: 12089, Average Value: 56.30696487426758\n",
    "Index: 5099, Average Value: 53.99234619140625\n",
    "Top 10 feature indices for layer 22 and their average values:\n",
    "Index: 8273, Average Value: 78.92124303181966\n",
    "Index: 5079, Average Value: 78.50505065917969\n",
    "Index: 8483, Average Value: 71.00369644165039\n",
    "Index: 261, Average Value: 69.07780456542969\n",
    "Index: 1072, Average Value: 68.29635492960612\n",
    "Top 10 feature indices for layer 23 and their average values:\n",
    "Index: 1541, Average Value: 181.09188588460287\n",
    "Index: 5148, Average Value: 88.39195251464844\n",
    "Index: 6895, Average Value: 81.28666687011719\n",
    "Index: 9039, Average Value: 72.08507792154948\n",
    "Index: 12660, Average Value: 65.19308471679688\n",
    "Top 10 feature indices for layer 24 and their average values:\n",
    "Index: 13752, Average Value: 127.97515869140625\n",
    "Index: 8648, Average Value: 87.73385620117188\n",
    "Index: 11687, Average Value: 80.15138626098633\n",
    "Index: 3074, Average Value: 75.44028854370117\n",
    "Index: 15615, Average Value: 75.26011505126954\n",
    "Top 10 feature indices for layer 25 and their average values:\n",
    "Index: 13749, Average Value: 202.93736012776694\n",
    "Index: 1689, Average Value: 182.919921875\n",
    "Index: 3017, Average Value: 147.8154312133789\n",
    "Index: 5553, Average Value: 135.17332458496094\n",
    "Index: 10550, Average Value: 127.17289733886719\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8caf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the dictionary and print the indices and corresponding layer\n",
    "for layer, top_features in top_features_per_layer.items():\n",
    "    print(f\"Layer {layer}:\")\n",
    "    if layer!=8:\n",
    "        continue\n",
    "    for ind, avg_val in top_features:\n",
    "        print(f\"  Index: {ind}, Average Value: {avg_val}\")\n",
    "        html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=f\"{layer}-gemmascope-res-16k\", feature_idx=ind)\n",
    "        display(IFrame(html, width=1200, height=300))\n",
    "    # if layer>20:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res\", # <- Release name \n",
    "    sae_id = \"layer_8/width_16k/average_l0_71\", # <- SAE id (not always a hook point!)\n",
    "    device = device\n",
    ")\n",
    "pr = 'What is the output of 53 plus 32 ? A: '\n",
    "\n",
    "# for ind in range(1, 17):\n",
    "ind = 13\n",
    "_, cache = model.run_with_cache_with_saes(pr, saes=[sae])\n",
    "x = cache['blocks.8.hook_resid_post.hook_sae_acts_post']\n",
    "topk_values, topk_indices = torch.topk(x[0, ind, :], 50)\n",
    "gathered_values = x[0, :, topk_indices]\n",
    "print(gathered_values.shape)\n",
    "# softmaxed_values = F.softmax(gathered_values, dim=0)\n",
    "# print(softmaxed_values)\n",
    "# vals, inds = torch.topk(cache['blocks.8.hook_resid_post.hook_sae_acts_post'][0, 13, :], 50)\n",
    "# inds\n",
    "softmaxed_x = F.softmax(gathered_values, dim=0)\n",
    "top3_indices_after_softmax = torch.topk(softmaxed_x, 2, dim=0).indices\n",
    "max_indices = torch.argmax(softmaxed_x, dim=0)\n",
    "\n",
    "for i in range(topk_indices.size(0)):\n",
    "    if 13 in top3_indices_after_softmax[:, i]:\n",
    "        print(topk_indices[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ccfbe1",
   "metadata": {},
   "source": [
    "# Attribution attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35a37c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from typing import Any, Literal, NamedTuple, Callable\n",
    "\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Use the functional API with inplace=False\n",
    "# feature_acts = self.hook_sae_acts_post(F.relu(hidden_pre, inplace=False))\n",
    "\n",
    "class SaeReconstructionCache(NamedTuple):\n",
    "    sae_in: torch.Tensor\n",
    "    feature_acts: torch.Tensor\n",
    "    sae_out: torch.Tensor\n",
    "    sae_error: torch.Tensor\n",
    "\n",
    "\n",
    "def track_grad(tensor: torch.Tensor) -> None:\n",
    "    \"\"\"wrapper around requires_grad and retain_grad\"\"\"\n",
    "    tensor.requires_grad_(True)\n",
    "    tensor.retain_grad()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ApplySaesAndRunOutput:\n",
    "    model_output: torch.Tensor\n",
    "    model_activations: dict[str, torch.Tensor]\n",
    "    sae_activations: dict[str, SaeReconstructionCache]\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Helper to zero grad all tensors in this object.\"\"\"\n",
    "        self.model_output.grad = None\n",
    "        for act in self.model_activations.values():\n",
    "            act.grad = None\n",
    "        for cache in self.sae_activations.values():\n",
    "            cache.sae_in.grad = None\n",
    "            cache.feature_acts.grad = None\n",
    "            cache.sae_out.grad = None\n",
    "            cache.sae_error.grad = None\n",
    "\n",
    "\n",
    "def apply_saes_and_run(\n",
    "    model: HookedTransformer,\n",
    "    saes: dict[str, SAE],\n",
    "    input: Any,\n",
    "    include_error_term: bool = True,\n",
    "    track_model_hooks: list[str] | None = None,\n",
    "    return_type: Literal[\"logits\", \"loss\"] = \"logits\",\n",
    "    track_grads: bool = False,\n",
    ") -> ApplySaesAndRunOutput:\n",
    "    \"\"\"\n",
    "    Apply the SAEs to the model at the specific hook points, and run the model.\n",
    "    By default, this will include a SAE error term which guarantees that the SAE\n",
    "    will not affect model output. This function is designed to work correctly with\n",
    "    backprop as well, so it can be used for gradient-based feature attribution.\n",
    "\n",
    "    Args:\n",
    "        model: the model to run\n",
    "        saes: the SAEs to apply\n",
    "        input: the input to the model\n",
    "        include_error_term: whether to include the SAE error term to ensure the SAE doesn't affect model output. Default True\n",
    "        track_model_hooks: a list of hook points to record the activations and gradients. Default None\n",
    "        return_type: this is passed to the model.run_with_hooks function. Default \"logits\"\n",
    "        track_grads: whether to track gradients. Default False\n",
    "    \"\"\"\n",
    "\n",
    "    fwd_hooks = []\n",
    "    bwd_hooks = []\n",
    "\n",
    "    sae_activations: dict[str, SaeReconstructionCache] = {}\n",
    "    model_activations: dict[str, torch.Tensor] = {}\n",
    "\n",
    "    # this hook just track the SAE input, output, features, and error. If `track_grads=True`, it also ensures\n",
    "    # that requires_grad is set to True and retain_grad is called for intermediate values.\n",
    "    def reconstruction_hook(sae_in: torch.Tensor, hook: HookPoint, hook_point: str):  # noqa: ARG001\n",
    "        sae = saes[hook_point]\n",
    "#         x = sae_in.to(sae.dtype)\n",
    "#         x = sae.reshape_fn_in(x)\n",
    "#         x = sae.hook_sae_input(x)\n",
    "#         x = sae.run_time_activation_norm_fn_in(x)\n",
    "\n",
    "#         sae_in = x - (sae.b_dec * sae.cfg.apply_b_dec_to_input)\n",
    "\n",
    "#         # # \"... d_in, d_in d_sae -> ... d_sae\",\n",
    "#         hidden_pre = sae.hook_sae_acts_pre(sae_in @ sae.W_enc + sae.b_enc)\n",
    "#         feature_acts = sae.hook_sae_acts_post(F.relu(hidden_pre, inplace=False))\n",
    "        feature_acts = sae.encode(sae_in)\n",
    "        \n",
    "        \n",
    "        sae_out = sae.decode(feature_acts)\n",
    "        sae_error = (sae_in - sae_out).detach().clone()\n",
    "        if track_grads:\n",
    "            track_grad(sae_error)\n",
    "            track_grad(sae_out)\n",
    "            track_grad(feature_acts)\n",
    "            track_grad(sae_in)\n",
    "        sae_activations[hook_point] = SaeReconstructionCache(\n",
    "            sae_in=sae_in,\n",
    "            feature_acts=feature_acts,\n",
    "            sae_out=sae_out,\n",
    "            sae_error=sae_error,\n",
    "        )\n",
    "\n",
    "        if include_error_term:\n",
    "            return sae_out + sae_error\n",
    "        return sae_out\n",
    "\n",
    "    def sae_bwd_hook(output_grads: torch.Tensor, hook: HookPoint):  # noqa: ARG001\n",
    "        # this just passes the output grads to the input, so the SAE gets the same grads despite the error term hackery\n",
    "        return (output_grads,)\n",
    "\n",
    "    # this hook just records model activations, and ensures that intermediate activations have gradient tracking turned on if needed\n",
    "    def tracking_hook(hook_input: torch.Tensor, hook: HookPoint, hook_point: str):  # noqa: ARG001\n",
    "        model_activations[hook_point] = hook_input\n",
    "        if track_grads:\n",
    "            track_grad(hook_input)\n",
    "        return hook_input\n",
    "\n",
    "    for hook_point in saes.keys():\n",
    "        fwd_hooks.append(\n",
    "            (hook_point, partial(reconstruction_hook, hook_point=hook_point))\n",
    "        )\n",
    "        bwd_hooks.append((hook_point, sae_bwd_hook))\n",
    "    for hook_point in track_model_hooks or []:\n",
    "        fwd_hooks.append((hook_point, partial(tracking_hook, hook_point=hook_point)))\n",
    "\n",
    "    # now, just run the model while applying the hooks\n",
    "    with model.hooks(fwd_hooks=fwd_hooks, bwd_hooks=bwd_hooks):\n",
    "        model_output = model(input, return_type=return_type)\n",
    "\n",
    "    return ApplySaesAndRunOutput(\n",
    "        model_output=model_output,\n",
    "        model_activations=model_activations,\n",
    "        sae_activations=sae_activations,\n",
    "    )\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from typing import Any, Literal, NamedTuple\n",
    "\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "EPS = 1e-8\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "@dataclass\n",
    "class AttributionGrads:\n",
    "    metric: torch.Tensor\n",
    "    model_output: torch.Tensor\n",
    "    model_activations: dict[str, torch.Tensor]\n",
    "    sae_activations: dict[str, SaeReconstructionCache]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Attribution:\n",
    "    model_attributions: dict[str, torch.Tensor]\n",
    "    model_activations: dict[str, torch.Tensor]\n",
    "    model_grads: dict[str, torch.Tensor]\n",
    "    sae_feature_attributions: dict[str, torch.Tensor]\n",
    "    sae_feature_activations: dict[str, torch.Tensor]\n",
    "    sae_feature_grads: dict[str, torch.Tensor]\n",
    "    sae_errors_attribution_proportion: dict[str, float]\n",
    "\n",
    "\n",
    "def calculate_attribution_grads(\n",
    "    model: HookedSAETransformer,\n",
    "    prompt: str,\n",
    "    metric_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "    track_hook_points: list[str] | None = None,\n",
    "    include_saes: dict[str, SAE] | None = None,\n",
    "    return_logits: bool = True,\n",
    "    include_error_term: bool = True,\n",
    ") -> AttributionGrads:\n",
    "    \"\"\"\n",
    "    Wrapper around apply_saes_and_run that calculates gradients wrt to the metric_fn.\n",
    "    Tracks grads for both SAE feature and model neurons, and returns them in a structured format.\n",
    "    \"\"\"\n",
    "    output = apply_saes_and_run(\n",
    "        model,\n",
    "        saes=include_saes or {},\n",
    "        input=prompt,\n",
    "        return_type=\"logits\" if return_logits else \"loss\",\n",
    "        track_model_hooks=track_hook_points,\n",
    "        include_error_term=include_error_term,\n",
    "        track_grads=True,\n",
    "    )\n",
    "    metric = metric_fn(output.model_output)\n",
    "    output.zero_grad()\n",
    "    metric.backward()\n",
    "    return AttributionGrads(\n",
    "        metric=metric,\n",
    "        model_output=output.model_output,\n",
    "        model_activations=output.model_activations,\n",
    "        sae_activations=output.sae_activations,\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_feature_attribution(\n",
    "    model: HookedSAETransformer,\n",
    "    input: Any,\n",
    "    metric_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "    track_hook_points: list[str] | None = None,\n",
    "    include_saes: dict[str, SAE] | None = None,\n",
    "    return_logits: bool = True,\n",
    "    include_error_term: bool = True,\n",
    ") -> Attribution:\n",
    "    \"\"\"\n",
    "    Calculate feature attribution for SAE features and model neurons following\n",
    "    the procedure in https://transformer-circuits.pub/2024/march-update/index.html#feature-heads.\n",
    "    This include the SAE error term by default, so inserting the SAE into the calculation is\n",
    "    guaranteed to not affect the model output. This can be disabled by setting `include_error_term=False`.\n",
    "\n",
    "    Args:\n",
    "        model: The model to calculate feature attribution for.\n",
    "        input: The input to the model.\n",
    "        metric_fn: A function that takes the model output and returns a scalar metric.\n",
    "        track_hook_points: A list of model hook points to track activations for, if desired\n",
    "        include_saes: A dictionary of SAEs to include in the calculation. The key is the hook point to apply the SAE to.\n",
    "        return_logits: Whether to return the model logits or loss. This is passed to TLens, so should match whatever the metric_fn expects (probably logits)\n",
    "        include_error_term: Whether to include the SAE error term in the calculation. This is recommended, as it ensures that the SAE will not affecting the model output.\n",
    "    \"\"\"\n",
    "    # first, calculate gradients wrt to the metric_fn.\n",
    "    # these will be multiplied with the activation values to get the attributions\n",
    "    outputs_with_grads = calculate_attribution_grads(\n",
    "        model,\n",
    "        input,\n",
    "        metric_fn,\n",
    "        track_hook_points,\n",
    "        include_saes=include_saes,\n",
    "        return_logits=return_logits,\n",
    "        include_error_term=include_error_term,\n",
    "    )\n",
    "    model_attributions = {}\n",
    "    model_activations = {}\n",
    "    model_grads = {}\n",
    "    sae_feature_attributions = {}\n",
    "    sae_feature_activations = {}\n",
    "    sae_feature_grads = {}\n",
    "    sae_error_proportions = {}\n",
    "    # this code is long, but all it's doing is multiplying the grads by the activations\n",
    "    # and recording grads, acts, and attributions in dictionaries to return to the user\n",
    "    with torch.no_grad():\n",
    "        for name, act in outputs_with_grads.model_activations.items():\n",
    "            assert act.grad is not None\n",
    "            raw_activation = act.detach().clone()\n",
    "            model_attributions[name] = (act.grad * raw_activation).detach().clone()\n",
    "            model_activations[name] = raw_activation\n",
    "            model_grads[name] = act.grad.detach().clone()\n",
    "        for name, act in outputs_with_grads.sae_activations.items():\n",
    "            assert act.feature_acts.grad is not None\n",
    "            assert act.sae_out.grad is not None\n",
    "            raw_activation = act.feature_acts.detach().clone()\n",
    "            sae_feature_attributions[name] = (\n",
    "                (act.feature_acts.grad * raw_activation).detach().clone()\n",
    "            )\n",
    "            sae_feature_activations[name] = raw_activation\n",
    "            sae_feature_grads[name] = act.feature_acts.grad.detach().clone()\n",
    "            if include_error_term:\n",
    "                assert act.sae_error.grad is not None\n",
    "                error_grad_norm = act.sae_error.grad.norm().item()\n",
    "            else:\n",
    "                error_grad_norm = 0\n",
    "            sae_out_norm = act.sae_out.grad.norm().item()\n",
    "            sae_error_proportions[name] = error_grad_norm / (\n",
    "                sae_out_norm + error_grad_norm + EPS\n",
    "            )\n",
    "        return Attribution(\n",
    "            model_attributions=model_attributions,\n",
    "            model_activations=model_activations,\n",
    "            model_grads=model_grads,\n",
    "            sae_feature_attributions=sae_feature_attributions,\n",
    "            sae_feature_activations=sae_feature_activations,\n",
    "            sae_feature_grads=sae_feature_grads,\n",
    "            sae_errors_attribution_proportion=sae_error_proportions,\n",
    "        )\n",
    "        \n",
    "        \n",
    "# prompt = \" Tiger Woods plays the sport of\"\n",
    "# pos_token = model.tokenizer.encode(\" golf\")[0]\n",
    "prompt = \"What is the output of 54 plus 32 ? It is \"\n",
    "pos_token = [model.tokenizer.encode(\"8\")[1]]\n",
    "neg_token = [model.tokenizer.encode(\"1\")[1]]\n",
    "# def metric_fn(logits: torch.tensor, pos_token: torch.tensor =pos_token, neg_token: torch.Tensor=neg_token) -> torch.Tensor:\n",
    "#     return logits[0,-1,pos_token] - logits[0,-1,neg_token]\n",
    "\n",
    "def metric_fn(logits: torch.Tensor, pos_token: torch.Tensor = pos_token, neg_token: torch.Tensor = neg_token) -> torch.Tensor:\n",
    "    return (logits[0, -1, pos_token] - logits[0, -1, neg_token]).sum()\n",
    "\n",
    "\n",
    "feature_attribution_df = calculate_feature_attribution(\n",
    "    input = prompt,\n",
    "    model = model,\n",
    "    metric_fn = metric_fn,\n",
    "    include_saes={sae.cfg.hook_name: sae},\n",
    "    include_error_term=True,\n",
    "    return_logits=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bdb631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_feature_to_long_df(sparse_tensor: torch.Tensor) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a sparse tensor to a long format pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(sparse_tensor.detach().cpu().numpy())\n",
    "    df_long = df.melt(ignore_index=False, var_name='column', value_name='value')\n",
    "    df_long.columns = [\"feature\", \"attribution\"]\n",
    "    df_long_nonzero = df_long[df_long['attribution'] != 0]\n",
    "    df_long_nonzero = df_long_nonzero.reset_index().rename(columns={'index': 'position'})\n",
    "    return df_long_nonzero\n",
    "\n",
    "df_long_nonzero = convert_sparse_feature_to_long_df(feature_attribution_df.sae_feature_attributions[sae.cfg.hook_name][0])\n",
    "df_long_nonzero.sort_values(\"attribution\", ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d49e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in df_long_nonzero.query(\"position==12\").groupby(\"feature\").attribution.sum().sort_values(ascending=False).head(5).items():\n",
    "    print(f\"Feature {i} had a total attribution of {v:.2f}\")\n",
    "    html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"8-gemmascope-res-16k\", feature_idx=int(i))\n",
    "    display(IFrame(html, width=1200, height=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596ef91a",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe97f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "sae_keys = list(df.loc['gemma-scope-2b-pt-res']['saes_map'].keys())\n",
    "# Dictionary to store the closest string for each layer\n",
    "closest_strings = {}\n",
    "\n",
    "# Regular expression to extract the layer number and l0 value\n",
    "pattern = re.compile(r'layer_(\\d+)/width_16k/average_l0_(\\d+)')\n",
    "\n",
    "# Organize strings by layer\n",
    "layer_dict = defaultdict(list)\n",
    "\n",
    "for s in sae_keys:\n",
    "    match = pattern.search(s)\n",
    "    if match:\n",
    "        layer = int(match.group(1))\n",
    "        l0_value = int(match.group(2))\n",
    "        layer_dict[layer].append((s, l0_value))\n",
    "\n",
    "# Find the string with l0 value closest to 100 for each layer\n",
    "for layer, items in layer_dict.items():\n",
    "    closest_string = min(items, key=lambda x: abs(x[1] - 100))\n",
    "    closest_strings[layer] = closest_string[0]\n",
    "\n",
    "# Output the closest string for each layer\n",
    "for layer in sorted(closest_strings):\n",
    "    print(f\"Layer {layer}: {closest_strings[layer]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd490c0",
   "metadata": {},
   "source": [
    "Layer 0: layer_0/width_16k/average_l0_105\n",
    "Layer 1: layer_1/width_16k/average_l0_102\n",
    "Layer 2: layer_2/width_16k/average_l0_141\n",
    "Layer 3: layer_3/width_16k/average_l0_59\n",
    "Layer 4: layer_4/width_16k/average_l0_124\n",
    "Layer 5: layer_5/width_16k/average_l0_68\n",
    "Layer 6: layer_6/width_16k/average_l0_70\n",
    "Layer 7: layer_7/width_16k/average_l0_69\n",
    "Layer 8: layer_8/width_16k/average_l0_71\n",
    "Layer 9: layer_9/width_16k/average_l0_73\n",
    "Layer 10: layer_10/width_16k/average_l0_77\n",
    "Layer 11: layer_11/width_16k/average_l0_80\n",
    "Layer 12: layer_12/width_16k/average_l0_82\n",
    "Layer 13: layer_13/width_16k/average_l0_84\n",
    "Layer 14: layer_14/width_16k/average_l0_84\n",
    "Layer 15: layer_15/width_16k/average_l0_78\n",
    "Layer 16: layer_16/width_16k/average_l0_78\n",
    "Layer 17: layer_17/width_16k/average_l0_77\n",
    "Layer 18: layer_18/width_16k/average_l0_74\n",
    "Layer 19: layer_19/width_16k/average_l0_73\n",
    "Layer 20: layer_20/width_16k/average_l0_71\n",
    "Layer 21: layer_21/width_16k/average_l0_70\n",
    "Layer 22: layer_22/width_16k/average_l0_72\n",
    "Layer 23: layer_23/width_16k/average_l0_75\n",
    "Layer 24: layer_24/width_16k/average_l0_73\n",
    "Layer 25: layer_25/width_16k/average_l0_116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cfa39d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac2115e4",
   "metadata": {},
   "source": [
    "tensor(4909, device='cuda:0')\n",
    "tensor(7759, device='cuda:0')\n",
    "tensor(2003, device='cuda:0')\n",
    "tensor(4781, device='cuda:0')\n",
    "tensor(4646, device='cuda:0')\n",
    "tensor(8109, device='cuda:0')\n",
    "tensor(2165, device='cuda:0')\n",
    "tensor(10524, device='cuda:0')\n",
    "tensor(14888, device='cuda:0')\n",
    "tensor(15075, device='cuda:0')\n",
    "tensor(778, device='cuda:0')\n",
    "tensor(15191, device='cuda:0')\n",
    "tensor(2707, device='cuda:0')\n",
    "tensor(10585, device='cuda:0')\n",
    "tensor(2121, device='cuda:0')\n",
    "tensor(9261, device='cuda:0')\n",
    "tensor(4978, device='cuda:0')\n",
    "tensor(8262, device='cuda:0')\n",
    "tensor(1083, device='cuda:0')\n",
    "tensor(571, device='cuda:0')\n",
    "tensor(9188, device='cuda:0')\n",
    "tensor(9561, device='cuda:0')\n",
    "tensor(7876, device='cuda:0')\n",
    "tensor(14993, device='cuda:0')\n",
    "tensor(2288, device='cuda:0')\n",
    "tensor(11746, device='cuda:0')\n",
    "tensor(5864, device='cuda:0')\n",
    "tensor(11973, device='cuda:0')\n",
    "tensor(498, device='cuda:0')\n",
    "tensor(4788, device='cuda:0')\n",
    "tensor(5821, device='cuda:0')\n",
    "tensor(10971, device='cuda:0')\n",
    "tensor(10825, device='cuda:0')\n",
    "tensor(5326, device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43aad94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12f45655",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1505e89f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-finetuning]",
   "language": "python",
   "name": "conda-env-.conda-finetuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
